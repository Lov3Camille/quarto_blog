[{"content":"My blog turns 1 today! Yay! For this occasion, I\u0026rsquo;ve reimplemented my blog with Quarto. Also, this lets me use all of Quarto\u0026rsquo;s cool new tricks.\nFor many of you, this move will change nothing. But those of you that use my RSS feed will need to update their feed. Once the new blog is online, you can get the new RSS feed via the RSS icon in the navbar.\nAlternatively, you can take this chance and subscribe to my new(ish) email newsletter. It is a biweekly newsletter focusing on dataviz, statistics and Shiny. Of course, this way you won\u0026rsquo;t miss my new blog posts. You can sign up here.\n","description":"Just a short announcement before I move","id":0,"section":"post","tags":[""],"title":"Announcement: This blog moves to Quarto","uri":"https://albert-rapp.de/post/2022-07-14-this-blog-moves-its-rss/"},{"content":"Today\u0026rsquo;s a short blog post. It\u0026rsquo;s mainly for sharing a cool trick I just learned.\nHere\u0026rsquo;s a simple template to incorporate your Shiny app into an HTML file. For instance, you can incorporate your shiny app into your blog post like I do here. Simply exchange the src argument by your Shiny app\u0026rsquo;s URL and then you\u0026rsquo;re good to go. Here, I use the app that I have shown you a couple of months ago.\n\u0026lt;iframe src=\u0026quot;https://rappa.shinyapps.io/interactive-ggplot\u0026quot; data-external=\u0026quot;1\u0026quot; width=\u0026quot;925px\u0026quot; height=\u0026quot;800px\u0026quot;\u0026gt; \u0026lt;/iframe\u0026gt;  From what I could tell, this is same code that knitr::include_app() drops. But including the iframe manually let\u0026rsquo;s you adjust the width and height of your frame. Beware that you will have to choose the dimensions large enough for your Shiny app.\nUPDATE: Originally, I had demonstrated the above code chunk here. But that causes unnecessary traffic on my shinyapps.io account, so I removed the demo.\n","description":"This is a one-minute blog post to share how to incorporate Shiny apps in blog posts.","id":1,"section":"post","tags":["shorts","shiny"],"title":"How to embed a Shiny app into your blog posts","uri":"https://albert-rapp.de/post/2022-05-09-embedding-a-shiny-app/"},{"content":"This week, I am oddly proud to announce that I have reached 1000 followers on Twitter. Check out the visualization that I have created for this joyous occasion.\nTo me, my rising follower count and the somewhat regular mails that I receive are a sign that people like to read my blog. And to thank you all for following my work, let me give you a quick intro to the packages rtweet and lubridate. These were instrumental in creating the above visual.\nWorking with rtweet At the end of February 2022, I wondered how my follower count evolves over time. Unfortunately, this is not something Twitter shows you by default. The Analytics page only shows me the change within my last 28 days. To overcome this drawback, I consulted the rtweet package which is a fabulous package that lets you interact with Twitter\u0026rsquo;s API through R.\nIn my case, I only do rudimentary work with it and track my followers over time. For this to work, I have set up an R script that runs every hour to download a list of my followers. Each hour, the list\u0026rsquo;s length tells me how many followers I have.\nIf you want to do the same, install the package first. Make sure to install the development version from GitHub, though.\n1  remotes::install_github(\u0026#34;rOpenSci/rtweet\u0026#34;)   Basic functionalities rtweet comes with a lot of convenient function. Most of these start with get_. For instance, there are\n get_followers(): This is the function to get a list of an account\u0026rsquo;s followers. get_timeline(): This gives you the a user\u0026rsquo;s timeline like tweets, replies and mentions. get_retweets(): This gives you the most recent retweets of a given tweet.  My aforementioned R script just runs get_followers() and computes the number of rows of the resulting tibble.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  library(rtweet) tib \u0026lt;- get_followers(\u0026#39;rappa753\u0026#39;) tib ## # A tibble: 1,018 x 1 ## user_id  ## \u0026lt;chr\u0026gt;  ## 1 778980355457941504  ## 2 561505786  ## 3 1265356152507940870 ## 4 1095327279536730117 ## 5 2508861812  ## 6 253373869  ## 7 2287065422  ## 8 1491615200  ## 9 1192011365243871232 ## 10 1522471850177093632 ## # ... with 1,008 more rows nrow(tib) ## [1] 1018   For my above visualization, I used get_timeline() to extract my five most popular tweets. Here, I ranked the popularity by the count of likes resp. \u0026ldquo;favorites\u0026rdquo; as rtweet likes to call it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  library(tidyverse) tl \u0026lt;- get_timeline(\u0026#39;rappa753\u0026#39;, n = 1000) tl_slice \u0026lt;- tl %\u0026gt;% slice_max(favorite_count, n = 5) %\u0026gt;% select(created_at, full_text, favorite_count, retweet_count) tl_slice ## # A tibble: 5 x 4 ## created_at full_text favorite_count retweet_count ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Sat Mar 05 20:56:33 +0000 2022 \u0026#34;The fun thing ab~ 508 83 ## 2 Sat Feb 19 17:32:23 +0000 2022 \u0026#34;Ever wanted to u~ 342 56 ## 3 Wed Mar 30 10:25:48 +0000 2022 \u0026#34;From standard ou~ 336 58 ## 4 Tue Apr 19 19:20:53 +0000 2022 \u0026#34;Last week, I hel~ 287 46 ## 5 Sat Nov 06 16:16:05 +0000 2021 \u0026#34;I tried to expla~ 273 38   Notice that I tweeted two of these before I started tracking my followers. Consequently, I ignored them for my visualization.\nUnfortunately, the rtweet package cannot do everything. For example, the snapshot functionality tweet_shot() does not work anymore. I think that\u0026rsquo;s because the Twitter API changed.\nThis bothered me during the 30 day chart challenge in April because I wanted to automatically extract great visualizations from Twitter. But as tweet_shot() was not working, I had to call Twitter\u0026rsquo;s API manually without rtweet. If you\u0026rsquo;re curious about how that works, check out my corresponding blog post. There, I\u0026rsquo;ve also explained how to set up a script that gets executed, say, every hour automatically.\nSetting up a Twitter app This pretty much explains how rtweet works. In general, it is really easy to use. And if you only want to use it only occasionally, there is not much more to it.\nHowever, if you want to use the package more often - as in calling the API every hour - then you need to set up a Twitter app. You can read up on how that works in the \u0026ldquo;Preparations\u0026rdquo; section of the above blog post. Once you\u0026rsquo;ve got that down, your rtweet calls can be authenticated through your Twitter app like so.\n1 2 3 4  auth \u0026lt;- rtweet::rtweet_app( bearer_token = keyring::key_get(\u0026#39;twitter-bearer-token\u0026#39;) ) auth_as(auth)   Here, I have used the keyring package to hide the bearer token of my Twitter app. If that doesn\u0026rsquo;t mean anything to you, let me once again refer to the above blog post. The important thing is that after these lines ran your rtweet calls get funneled through your own Twitter app.\nWorking with lubridate As you saw above, the timeline that we extracted and saved in rtweet contains time data. Here it is once again.\n1 2 3 4 5 6 7 8 9  tl_slice ## # A tibble: 5 x 4 ## created_at full_text favorite_count retweet_count ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Sat Mar 05 20:56:33 +0000 2022 \u0026#34;The fun thing ab~ 508 83 ## 2 Sat Feb 19 17:32:23 +0000 2022 \u0026#34;Ever wanted to u~ 342 56 ## 3 Wed Mar 30 10:25:48 +0000 2022 \u0026#34;From standard ou~ 336 58 ## 4 Tue Apr 19 19:20:53 +0000 2022 \u0026#34;Last week, I hel~ 287 46 ## 5 Sat Nov 06 16:16:05 +0000 2021 \u0026#34;I tried to expla~ 273 38   Sadly, working with times and dates is rarely pleasant. But we can make our life a bit easier by using the lubridate package which was made for that. To show you how it works, it is probably best to show you a couple of use cases.\nAll of these will be taken from what I had to deal with to create my celebratory visualization. But I simplified it to minimal examples for this blog post. More use cases can be found in the lubridate cheatsheet or the tidyverse cookbook ressource by Malte Grosser.\nParse dates and times First, I needed to convert the created_at column from character to datetime format. The easiest way to do that gets rid of +0000 in the character vector and then parses the vector into the right format via parse_date_time(). But there is a catch. Check out what happens if I try this on my computer.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  library(lubridate) tl_slice %\u0026gt;% mutate(created_at = parse_date_time( str_remove(created_at, \u0026#39;\\\\+0000\u0026#39;), # remove the +0000  orders = \u0026#39;a b d H:M:S Y\u0026#39; )) ## # A tibble: 5 x 4 ## created_at full_text favorite_count retweet_count ## \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 NA \u0026#34;The fun thing about getting better a~ 508 83 ## 2 NA \u0026#34;Ever wanted to use colors in #ggplot~ 342 56 ## 3 NA \u0026#34;From standard output to powerful sto~ 336 58 ## 4 NA \u0026#34;Last week, I held a short workshop t~ 287 46 ## 5 NA \u0026#34;I tried to explain the variance of t~ 273 38   See how all values in created_at are NA now? That\u0026rsquo;s a problem. And we will solve it very soon. But first, let me explain how the function call works.\nThe orders argument specifies how the vector created_at (without +0000) is to be understood. We clarify that created_at contains (in the order of appearance)\n abbreviated week day names (a) abbreviated month names (b) the day of the month as decimals (d) and so on  Where do these abbreviations a, b, d, etc. come from? They are defined in the help page of parse_date_time(). You can find them in the section \u0026ldquo;Details\u0026rdquo;. But why does the code not work? Why do we always get an NA? For once, my computer is truly the problem. Or rather, its settings.\nBy default, my computer is set to German. But even if RStudio or R\u0026rsquo;s error messages are set to English, my computer\u0026rsquo;s so-called \u0026ldquo;locale\u0026rdquo; may be still be set to German. That\u0026rsquo;s a problem because abbreviations like \u0026ldquo;Sat\u0026rdquo; and \u0026ldquo;Wed\u0026rdquo; refer to the English words \u0026ldquo;Saturday\u0026rdquo; and \u0026ldquo;Wednesday\u0026rdquo;. So, we need to make sure that parse_date_time() understands that it needs to use an English locale. Then, everything works out.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  parsed_highlights \u0026lt;- tl_slice %\u0026gt;% mutate(created_at = parse_date_time( str_remove(created_at, \u0026#39;\\\\+0000\u0026#39;), # remove the +0000  orders = \u0026#39;a b d H:M:S Y\u0026#39;, locale = \u0026#34;English\u0026#34; )) parsed_highlights ## # A tibble: 5 x 4 ## created_at full_text favorite_count retweet_count ## \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2022-03-05 20:56:33 \u0026#34;The fun thing about getting~ 508 83 ## 2 2022-02-19 17:32:23 \u0026#34;Ever wanted to use colors i~ 342 56 ## 3 2022-03-30 10:25:48 \u0026#34;From standard output to pow~ 336 58 ## 4 2022-04-19 19:20:53 \u0026#34;Last week, I held a short w~ 287 46 ## 5 2021-11-06 16:16:05 \u0026#34;I tried to explain the vari~ 273 38   We are now ready to send this data to ggplot. Since created_at is formatted in datetime now, ggplot will understand what it means when we map x = created_at.\n1 2 3  parsed_highlights %\u0026gt;% ggplot(aes(created_at, favorite_count)) + geom_line()   Using scale_x_date(time) and locale Did you see that the x-axis uses German abbreviations and doesn\u0026rsquo;t show what year we\u0026rsquo;re in? That\u0026rsquo;s not great. Let\u0026rsquo;s change that. As is always the case when we want to format the axes we will need a scale_*() function. Here, what we need is scale_x_datetime().\nBut this won\u0026rsquo;t solve our German locale problem. The easiest way to solve that tricky ordeal is to change the locale globally via Sys.setlocale(). Don\u0026rsquo;t worry, though. The locale will reset after restarting R. No permanent \u0026ldquo;damage\u0026rdquo; here.\n1 2 3 4 5 6 7 8 9 10  Sys.setlocale(\u0026#34;LC_ALL\u0026#34;,\u0026#34;English\u0026#34;) ## [1] \u0026#34;LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252\u0026#34; p \u0026lt;- parsed_highlights %\u0026gt;% ggplot(aes(created_at, favorite_count)) + geom_line() + scale_x_datetime( date_breaks = \u0026#39;2 months\u0026#39;, # break every two months date_labels = \u0026#39;%b %Y\u0026#39; ) p   Notice that we have once again used the same abbreviation as for parse_date_time(). This time, though, they have to be preceded by %. Don\u0026rsquo;t ask me why. It is just the way it is.\nCreate new dates Let us add a rectangle to our previous plot via an annotation. This is similar to what I needed to do when adding my \u0026ldquo;mysterious wonderland\u0026rdquo; to my plot.\nSince the x aesthetic is formatted to datetime, we have to specify dates for the xmin and xmax aesthetic of our annotation. Therefore, we need to create dates manually. In this case, make_datetime() is the way to go. If we\u0026rsquo;re dealing only with dates (without times), then make_date() is a useful pendant. Both functions are quite straightforward.\n1 2 3 4 5 6 7 8 9 10  p \u0026lt;- p + annotate( \u0026#39;rect\u0026#39;, xmin = make_datetime(year = 2021, month = 11, day = 6, hour = 12), xmax = make_datetime(year = 2021, month = 9), ymin = 200, ymax = 273, alpha = 0.5 ) p   Filter with intervals Maybe we want to highlight a part of our line. To do so, we could filter our data to check whether certain date ranges correspond to parts that we want to highlight. Usually when we want to check if a value x is within a certain set of objects we use x %in% objects.\nTo do the same with dates, we need to create an interval with interval() first. Then, we can use that in filter() in conjunction with %within% instead of %in%.\n1 2 3 4 5 6 7 8 9 10 11  my_interval \u0026lt;- interval( start = make_date(year = 2022, month = 2), end = make_date(year = 2022, month = 3, day = 10) ) p + geom_line( data = parsed_highlights %\u0026gt;% filter(created_at %within% my_interval), color = \u0026#39;red\u0026#39;, size = 2 )   Calculations with times Say that you want to highlight the first five days after a certain date. (That\u0026rsquo;s exactly what I did in my plot.) Then, you can simply add days(5) to this date. There are similar functions like minutes(), hours() and so on. Let me show you how that may look in a visualization.\n1 2 3 4 5 6 7 8 9 10  p + annotate( \u0026#39;rect\u0026#39;, xmin = parsed_highlights[[1, \u0026#39;created_at\u0026#39;]] - hours(24), xmax = parsed_highlights[[1, \u0026#39;created_at\u0026#39;]] + days(5), ymin = -Inf, ymax = Inf, alpha = 0.25, fill = \u0026#39;blue\u0026#39; )   Closing This was a short intro to lubridate and rtweet. Naturally, the evolution of my follower count contained a lot more steps. In the end, though, these steps were merely a collection of\n techniques you know from my two previous storytelling with ggplot posts (see here and here) plus data wrangling using times and dates with the functions that I just showed you.  Once again, thank you all for your support. And if you liked this post and don\u0026rsquo;t follow my work yet, then consider following me on Twitter and/or subscribing to my RSS feed. See you next time!\n","description":"We take a quick look at the rtweet and lubridate package. These help you to analyse your Twitter timeline. And they helped me to visualize my follower count as I reached 1000 followers this week.","id":2,"section":"post","tags":["API","visualization"],"title":"Analyze your Twitter timeline with {rtweet} and {lubridate}","uri":"https://albert-rapp.de/post/2022-05-06-track-twitter-follower/"},{"content":"A standard ggplot output can rarely convey a powerful message. For effective data visualization you need to customize your plot. A couple of weeks ago, I showed you how.\nIn this blog post, I will rebuild another great data viz from scratch. If you have read my original blog post, then you won\u0026rsquo;t have to learn many new tricks. Most of the techniques that I use can be found there. This is also why I save explanations only for the parts that are new. This should keep this blog post a bit shorter. You\u0026rsquo;re welcome.\nNevertheless, in today\u0026rsquo;s installment of my ggplot2 series I will teach you something truly special. I will teach you how to create\u0026hellip;*drum roll*\u0026hellip;rounded rectangles. Sounds exciting, doesn\u0026rsquo;t it? Well, maybe not. But it looks great. Check out what we\u0026rsquo;ll build today.\nThis plot comes to you via another excellent entry of the storytelling with data (SWD) blog. To draw rectangles with rounded rectangles we can leverage the ggchicklet package. Though, for some mysterious reason, the geom_* that we need is hidden in that package. Therefore, we will have to dig it out. That\u0026rsquo;s the easy way to do it. And honestly, this is probably also the practical way to do it.\nHowever, every now and then I want to do things the hard way. So, my dear reader, this is why I will also show you how to go from rectangles to rounded rectangles the hard way. But only after showing you the easy way first, of course. Only then, in the second part of this blog post, will I take the sadistically-inclined among you on a tour to the world of grobs.\nGrobs, you say? Is that an instrument? No, Patrick, it is an graphical object. Under the hood, we can transform a ggplot to a list of graphical objects. And with a few hacks, we can adjust that list. This way, the list will contain not rectGrobs but roundrectGrobs. Then, we can put everything back together, close the hood and enjoy our round rectangles. Now, enough intro, let\u0026rsquo;s go.\nBasic plot First, let us recreate the \u0026ldquo;bad\u0026rdquo; plot that the above SWD blog post remodels. In the end, we will work on the remodeled data viz too. As always, though, there is something to be learnt from creating an ugly plot. So, here\u0026rsquo;s the beast that we will build.\n Source: Storytelling with data (SWD) blog  Read data I didn\u0026rsquo;t find the underlying data and had to guess the values from the plot. Thus, I probably didn\u0026rsquo;t get the values exactly right. But for our purposes this should suffice. If you want, you can download the European csv-file that I created here.\n1 2 3  library(tidyverse) # Use read_csv2 because it\u0026#39;s an European file dat \u0026lt;- read_csv2(\u0026#39;ratios.csv\u0026#39;)   Compute averages Let me point out that taking the average of the ratios may not necessarily give an appropriate result (in a statistical kind of sense). But, once again, this should not bother us as we only want to learn how to plot.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  avgs \u0026lt;- dat %\u0026gt;% pivot_longer( cols = -1, names_to = \u0026#39;type\u0026#39;, values_to = \u0026#39;ratio\u0026#39; ) %\u0026gt;% group_by(type) %\u0026gt;% summarise(ratio = mean(ratio)) %\u0026gt;% mutate(location = \u0026#39;REGION AVERAGE\u0026#39;) avgs ## # A tibble: 3 x 3 ## type ratio location  ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  ## 1 inventory_turnover 9.78 REGION AVERAGE ## 2 store_lower 7.11 REGION AVERAGE ## 3 store_upper 12.1 REGION AVERAGE ### Combine with data  dat_longer \u0026lt;- dat %\u0026gt;% pivot_longer( cols = -1, names_to = \u0026#39;type\u0026#39;, values_to = \u0026#39;ratio\u0026#39; ) dat_longer_with_avgs \u0026lt;- dat_longer %\u0026gt;% bind_rows(avgs)   Create bars 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  ## Colors we will use throughout this blog post color_palette \u0026lt;- thematic::okabe_ito(8) # Make sure that bars are in the same order as in the data set dat_factored \u0026lt;- dat_longer %\u0026gt;% mutate(location = factor(location, levels = dat$location)) p \u0026lt;- dat_factored %\u0026gt;% ggplot(aes(location, ratio)) + geom_col( data = filter(dat_factored, type == \u0026#39;inventory_turnover\u0026#39;), fill = color_palette[2] ) + theme_minimal() p   Turn labels and get rid of axis text 1 2 3 4 5 6  p \u0026lt;- p + labs(x = element_blank(), y = element_blank()) + theme( axis.text.x = element_text(angle = 50, hjust = 1) ) p   Remove expansion to get x-labels closer to the bars 1 2  p \u0026lt;- p + coord_cartesian(ylim = c(0, 30), expand = F) p   Remove other grid lines 1 2 3 4 5 6 7  p \u0026lt;- p + theme( panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), panel.grid.major.y = element_line(colour = \u0026#39;black\u0026#39;, size = 0.75) ) p   Format y-axis 1 2 3 4 5 6  p \u0026lt;- p + scale_y_continuous( breaks = seq(0, 30, 5), labels = scales::label_comma(accuracy = 0.1) ) p   Add points 1 2 3 4 5 6 7 8 9 10 11 12  p \u0026lt;- p + geom_point( data = filter(dat_factored, type == \u0026#39;store_lower\u0026#39;), col = color_palette[1], size = 3 ) + geom_point( data = filter(dat_factored, type == \u0026#39;store_upper\u0026#39;), col = color_palette[3], size = 3 ) p   Add average lines 1 2 3 4 5 6 7 8 9 10 11 12  p \u0026lt;- p + geom_hline( yintercept = avgs[[3, \u0026#39;ratio\u0026#39;]], size = 2.5, col = color_palette[3] ) + geom_hline( yintercept = avgs[[2, \u0026#39;ratio\u0026#39;]], size = 2.5, col = color_palette[1] ) p   Add text labels 1 2 3 4 5 6 7  p + geom_text( data = filter(dat_factored, type == \u0026#39;inventory_turnover\u0026#39;), aes(label = scales::comma(ratio, accuarcy = 0.1)), nudge_y = 0.8, size = 2.5 )   Improved plot Now, let us begin building the improved plot. First, let us get the long labels onto the y-axis and use regular rectangles before we worry about the rounded rectangles.\nFlip axes and use rectangles to show upper and lower bounds. Unfortunately, geom_rect() does not work as intended.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  dat_with_avgs \u0026lt;- dat_longer_with_avgs %\u0026gt;% pivot_wider( names_from = \u0026#39;type\u0026#39;, values_from = \u0026#39;ratio\u0026#39; ) dat_with_avgs %\u0026gt;% ggplot() + geom_rect( aes( xmin = store_lower, xmax = store_upper, ymin = location, ymax = location ) )   Instead, let us create a new numeric column containing a location\u0026rsquo;s rank based on its inventory_turnover. This is done with row_number(). While we\u0026rsquo;re at it, let us create a new tibble that also contains information on the colors each geom will use. Then, we can map to these new columns in ggplot and make sure that the values are used as is by setting scale_*_identity(). This is one convenient way to control the aesthetics of each geom without functional programming. With the image from above in mind, we know that our final plot will need\n different col, fill and size values in geom_point() different fill and alpha values in geom_rect()  Here\u0026rsquo;s what this tibble looks like.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  bar_height \u0026lt;- 0.4 no_highlight_col \u0026lt;- \u0026#39;grey70\u0026#39; average_highlight_col \u0026lt;- \u0026#39;grey40\u0026#39; below_highlight \u0026lt;- color_palette[2] sorted_dat \u0026lt;- dat_with_avgs %\u0026gt;% mutate(num = row_number(inventory_turnover)) %\u0026gt;% # Sort so that everything is in order of rank # Important for text labels later on arrange(desc(num)) %\u0026gt;% mutate( rect_color = case_when( inventory_turnover \u0026lt; store_lower ~ below_highlight, location == \u0026#39;REGION AVERAGE\u0026#39; ~ average_highlight_col, T ~ no_highlight_col ), rect_alpha = if_else( inventory_turnover \u0026lt; store_lower, 0.5, 1 ), point_color = if_else( inventory_turnover \u0026lt; store_lower, below_highlight, \u0026#39;black\u0026#39; ), point_fill = if_else( inventory_turnover \u0026lt; store_lower, below_highlight, \u0026#39;white\u0026#39; ), point_size = if_else( inventory_turnover \u0026lt; store_lower, 3, 2 ) ) sorted_dat ## # A tibble: 24 x 10 ## location inventory_turno~ store_upper store_lower num rect_color rect_alpha ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Castle ~ 14.7 24.3 20 24 #009E73 0.5 ## 2 Wellsvi~ 13.6 7.6 2.5 23 grey70 1  ## 3 Basin C~ 12.7 8 4.5 22 grey70 1  ## 4 Atlantis 12.3 12.8 7.8 21 grey70 1  ## 5 Neverla~ 12.1 18 13.4 20 #009E73 0.5 ## 6 Bluffin~ 11.7 4.2 3 19 grey70 1  ## 7 Bikini ~ 11.6 12.5 7.8 18 grey70 1  ## 8 Metropo~ 11.3 24 11.3 17 grey70 1  ## 9 Hill Va~ 11 22 7.5 16 grey70 1  ## 10 Venusvi~ 10.4 15.5 12.3 15 #009E73 0.5 ## # ... with 14 more rows, and 3 more variables: point_color \u0026lt;chr\u0026gt;, ## # point_fill \u0026lt;chr\u0026gt;, point_size \u0026lt;dbl\u0026gt;   Now, we can create our plot. Notice that I set shape = 21 in geom_point() to use both the fill and col aesthetic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  sorted_dat %\u0026gt;% ggplot() + geom_rect( aes( xmin = store_lower, xmax = store_upper, ymin = num - bar_height, ymax = num + bar_height, fill = rect_color, alpha = rect_alpha ), ) + geom_point( aes( x = inventory_turnover, y = num, fill = point_fill, col = point_color, size = point_size ), shape = 21, stroke = 1 ) + scale_fill_identity() + scale_color_identity() + scale_size_identity() + scale_alpha_identity() + theme_minimal()   Use ggchicklet for rounded rectangles The whole point of this blog post is to use rounded rectangles. So let\u0026rsquo;s do that. The ggchicklet package has a geom called geom_rrect(). It works just like geom_rect() but accepts another value r which is used to determine the radius of the rounded rectangles. Unfortunately, this geom is not an exported function of this package. This means that if you write ggchicklet:: (e.g.Â in RStudio) and press TAB you won\u0026rsquo;t see geom_rrect(). Thus, you have to access the internal function via ::: (three colons).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  p \u0026lt;- sorted_dat %\u0026gt;% ggplot() + ggchicklet:::geom_rrect( aes( xmin = store_lower, xmax = store_upper, ymin = num - bar_height, ymax = num + bar_height, fill = rect_color, alpha = rect_alpha ), # Use relative npc unit (values between 0 and 1) # This ensures that radius is not too large for your canvas r = unit(0.5, \u0026#39;npc\u0026#39;) ) + geom_point( aes( x = inventory_turnover, y = num, fill = point_fill, col = point_color, size = point_size ), shape = 21, stroke = 1 ) + scale_fill_identity() + scale_color_identity() + scale_size_identity() + scale_alpha_identity() + theme_minimal() p   Remove grid lines, move axis and add some text elements We will set the y-axis labels manually later on. Otherwise, we cannot change its colors one-by-one. For now, let\u0026rsquo;s get rid of superfluous grid lines, move the x-axis and add a title.\nNotice that I draw the axis line manually with a segment annotation. This seems weird, I know. Unfortunately, it cannot be helped because I still need room for the y-axis labels. And if I do not plot the axis line manually, then the axis line will start all the way to the left. Make sure that you set clip = 'off' in coord_cartesian() for the annotation to be displayed.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  title_lab \u0026lt;- \u0026#39;Review stores with turnover ratios that are below their\\nforecasted range\u0026#39; title_size \u0026lt;- 14 axis_label_size \u0026lt;- 8 text_size \u0026lt;- 18 p \u0026lt;- p + scale_x_continuous( breaks = seq(0, 25, 5), position = \u0026#39;top\u0026#39; ) + coord_cartesian( xlim = c(-5, 25), ylim = c(0.75, 24.75), expand = F, clip = \u0026#39;off\u0026#39; ) + annotate( \u0026#39;segment\u0026#39;, x = 0, xend = 25, y = 24.75, yend = 24.75, col = no_highlight_col, size = 0.25 ) + labs( x = \u0026#39;INVENTORY TURNOVER RATIO\u0026#39;, y = element_blank(), title = title_lab ) + theme( text = element_text( size = text_size, color = average_highlight_col ), plot.title.position = \u0026#39;plot\u0026#39;, panel.grid = element_blank(), axis.title.x = element_text( size = axis_label_size, hjust = 0.21, color = no_highlight_col ), axis.text.x = element_text( size = axis_label_size, color = no_highlight_col ), axis.ticks.x = element_line(color = no_highlight_col, size = 0.25), axis.text.y = element_blank(), axis.line.x = element_blank() ) p   Add y-axis labels 1 2 3 4 5 6 7 8 9 10 11 12  y_axis_text_size \u0026lt;- 3 p + geom_text( aes( x = 0, y = num, label = location, col = no_highlight_col, hjust = 1, size = y_axis_text_size ) )   Highlight words Let us turn to text highlights. For that we will need ggtext. This will let us use geom_richtext() instead of geom_text(). Notice that I have note saved the last geom_text() modification in p. Otherwise, we would get two overlapping layers of text. You can highlight single words as demonstrated in my blog post about effective use of colors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  library(ggtext) sorted_dat_with_new_labels \u0026lt;- sorted_dat %\u0026gt;% mutate(location_label = case_when( inventory_turnover \u0026lt; store_lower ~ glue::glue( \u0026#39;\u0026lt;span style = \u0026#34;color:{below_highlight}\u0026#34;\u0026gt;**{location}**\u0026lt;/span\u0026gt;\u0026#39; ), location == \u0026#39;REGION AVERAGE\u0026#39; ~ glue::glue( \u0026#39;\u0026lt;span style = \u0026#34;color:{average_highlight_col}\u0026#34;\u0026gt;**{location}**\u0026lt;/span\u0026gt;\u0026#39; ), T ~ location )) p \u0026lt;- p + geom_richtext( data = sorted_dat_with_new_labels, aes( x = 0, y = num, label = location_label, col = no_highlight_col, hjust = 1, size = y_axis_text_size ), label.colour = NA, fill = NA ) p   Fantastic! Next, we only have to highlight words in our call to action. Make sure that plot.title in theme() is an element_markdown().\n1 2 3 4 5 6 7 8 9 10  title_lab_adjusted \u0026lt;- glue::glue( \u0026#34;Review stores with **turnover ratios** that are \u0026lt;span style = \u0026#39;color:{below_highlight}\u0026#39;\u0026gt;below their\u0026lt;/span\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style = \u0026#39;color:#7fceb9;\u0026#39;\u0026gt;**forecasted range**\u0026lt;/span\u0026gt;\u0026#34; ) p + labs(title = title_lab_adjusted) + theme( plot.title = element_markdown(), panel.background = element_rect(color = NA, fill = \u0026#39;white\u0026#39;) )   There you go. This concludes the easy way to draw rounded rectangles with ggplot2 and ggchicklet. Now, I am well aware that this is probably the moment when many readers will drop out. So, let me do some shameless self-promotion before everyone\u0026rsquo;s gone.\nIf you enjoyed this post, follow me on Twitter and/or subscribe to my RSS feed. For reaching out to me, feel free to hit the comments or send me a mail. I am always happy to see people commenting on my work.\nRounded rectangles with grobs Alright, this is where the hacking begins. In this last part of the blog post, I will show you to how transform rectangles to rounded rectangles. In principle, you could then create our SWD plot using geom_rect() and transform the rectangles afterwards. No additional package needed.\nSimple example with one bar Let me demonstrate a quick hack when there is only one bar in the plot. Unfortunately, this does not work with more than one bar. Still, this should get you acquainted with grobs. First, create a simple dummy plot.\n1 2 3 4 5 6 7  library(tidyverse) p \u0026lt;- mpg %\u0026gt;% filter(year == 2008) %\u0026gt;% ggplot(aes(year)) + geom_bar() + theme_minimal() p   Next, we can turn this plot into a so-called TableGrob. From what I understand, it is a highly nested list that contains all the graphical objects (grobs) that are part of our plot p.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  l \u0026lt;- ggplotGrob(p) l ## TableGrob (12 x 9) \u0026#34;layout\u0026#34;: 18 grobs ## z cells name grob ## 1 0 ( 1-12, 1- 9) background zeroGrob[plot.background..zeroGrob.845] ## 2 5 ( 6- 6, 4- 4) spacer zeroGrob[NULL] ## 3 7 ( 7- 7, 4- 4) axis-l absoluteGrob[GRID.absoluteGrob.834] ## 4 3 ( 8- 8, 4- 4) spacer zeroGrob[NULL] ## 5 6 ( 6- 6, 5- 5) axis-t zeroGrob[NULL] ## 6 1 ( 7- 7, 5- 5) panel gTree[panel-1.gTree.828] ## 7 9 ( 8- 8, 5- 5) axis-b absoluteGrob[GRID.absoluteGrob.831] ## 8 4 ( 6- 6, 6- 6) spacer zeroGrob[NULL] ## 9 8 ( 7- 7, 6- 6) axis-r zeroGrob[NULL] ## 10 2 ( 8- 8, 6- 6) spacer zeroGrob[NULL] ## 11 10 ( 5- 5, 5- 5) xlab-t zeroGrob[NULL] ## 12 11 ( 9- 9, 5- 5) xlab-b titleGrob[axis.title.x.bottom..titleGrob.837] ## 13 12 ( 7- 7, 3- 3) ylab-l titleGrob[axis.title.y.left..titleGrob.840] ## 14 13 ( 7- 7, 7- 7) ylab-r zeroGrob[NULL] ## 15 14 ( 4- 4, 5- 5) subtitle zeroGrob[plot.subtitle..zeroGrob.842] ## 16 15 ( 3- 3, 5- 5) title zeroGrob[plot.title..zeroGrob.841] ## 17 16 (10-10, 5- 5) caption zeroGrob[plot.caption..zeroGrob.844] ## 18 17 ( 2- 2, 2- 2) tag zeroGrob[plot.tag..zeroGrob.843]   In this case, calling l gave us an overview of plot parts. We will want to change stuff in the panel. Thus, let us extract the grobs from the sixth list entry of l. As we have seen in the table, this will give us a gTree. That\u0026rsquo;s another nested list. And it contains an interesting sublist called children. That\u0026rsquo;s where the grobs of this gTree are stored.\n1 2 3  grobs \u0026lt;- l$grobs[[6]] grobs$children ## (gTree[grill.gTree.826], zeroGrob[NULL], rect[geom_rect.rect.814], zeroGrob[NULL], zeroGrob[panel.border..zeroGrob.815])   Here, the rect grob is what we want to access. Thus, let us take a look what we can find there.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # str() helps us to unmask the complicated list structure grobs$children[[3]] %\u0026gt;% str() ## List of 10 ## $ x : \u0026#39;simpleUnit\u0026#39; num 0.0455native ## ..- attr(*, \u0026#34;unit\u0026#34;)= int 4 ## $ y : \u0026#39;simpleUnit\u0026#39; num 0.955native ## ..- attr(*, \u0026#34;unit\u0026#34;)= int 4 ## $ width : \u0026#39;simpleUnit\u0026#39; num 0.909native ## ..- attr(*, \u0026#34;unit\u0026#34;)= int 4 ## $ height: \u0026#39;simpleUnit\u0026#39; num 0.909native ## ..- attr(*, \u0026#34;unit\u0026#34;)= int 4 ## $ just : chr [1:2] \u0026#34;left\u0026#34; \u0026#34;top\u0026#34; ## $ hjust : NULL ## $ vjust : NULL ## $ name : chr \u0026#34;geom_rect.rect.814\u0026#34; ## $ gp :List of 6 ## ..$ col : logi NA ## ..$ fill : chr \u0026#34;#595959FF\u0026#34; ## ..$ lwd : num 1.42 ## ..$ lty : num 1 ## ..$ linejoin: chr \u0026#34;mitre\u0026#34; ## ..$ lineend : chr \u0026#34;butt\u0026#34; ## ..- attr(*, \u0026#34;class\u0026#34;)= chr \u0026#34;gpar\u0026#34; ## $ vp : NULL ## - attr(*, \u0026#34;class\u0026#34;)= chr [1:3] \u0026#34;rect\u0026#34; \u0026#34;grob\u0026#34; \u0026#34;gDesc\u0026#34;   This is a grob. It can be build using grid::rectGrob(). Basically, what you see here is a specification of everything from x and y position to graphical properties (gp) of this rectangular grob.\nThere is also a function grid::roundrectGrob(). As you may have guessed, it builds the rounded rectangle grobs that we so desperately crave. From grid::roundrectGrob()\u0026rsquo;s documentation, we know that we will have to specify another variable r to determine the radius of the rounded rectangles. So, here\u0026rsquo;s what we could do now.\n Extract x, y, gp and so on from grobs$children[[3]]. Add another argument r. Pass all of these arguments to grid::roundrectGrob() Exchange grobs$children[[3]] with our newly built roundrectGrob  This is what we will have to do at some point. But in this simple plot, I want to show you a different hack. Did you notice the class attributes of grobs$children[[3]]? Somewhere in there it says - attr(*, \u0026quot;class\u0026quot;)= chr [1:3] \u0026quot;rect\u0026quot; \u0026quot;grob\u0026quot; \u0026quot;gDesc\u0026quot;. And we can access and change that information through attr().\n1 2  attr(grobs$children[3][[1]], \u0026#39;class\u0026#39;) ## [1] \u0026#34;rect\u0026#34; \u0026#34;grob\u0026#34; \u0026#34;gDesc\u0026#34;   Now, a really basic hack is to\n change the class attribute from rect to roundrect. stick another argument r into the list put everything back together as if nothing happened  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  ## Change class attribute of grobs$children[3][[1]] from rect to roundrect current_attr \u0026lt;- attr(grobs$children[3][[1]], \u0026#39;class\u0026#39;) new_attr \u0026lt;- str_replace(current_attr, \u0026#39;rect\u0026#39;, \u0026#39;roundrect\u0026#39;) attr(grobs$children[3][[1]], \u0026#39;class\u0026#39;) \u0026lt;- new_attr # Add r argument for grid::roundrectGrob() # We need to add a \u0026#34;unit\u0026#34; # Here I use the relative unit npc grobs$children[3][[1]]$r \u0026lt;- unit(0.5, \u0026#39;npc\u0026#39;) # Copy original list and change grobs in place l_new \u0026lt;- l l_new$grobs[[6]] \u0026lt;- grobs # Draw grobs via grid::grid.draw() grid::grid.newpage() grid::grid.draw(l_new)   Dealing with multiple bars The previous hack works if we plot only one bar. However, if there are multiple x arguments, then grid::roundrectGrob() will error. It seems like that function is not vectorized. So, we will build the rounded rectangles ourselves with functional programming. First let\u0026rsquo;s take a look at the plot that we want to modify.\n1 2 3 4 5  p \u0026lt;- mpg %\u0026gt;% ggplot(aes(class, fill = class)) + geom_bar() + theme_minimal() p   Now, let\u0026rsquo;s find out what arguments grid::roundrectGrob() accepts and extract as many of these from grobs$children[3] as possible.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  l \u0026lt;- ggplotGrob(p) grobs \u0026lt;- l$grobs[[6]] # What arguments does roundrectGrob need? arg_names \u0026lt;- args(grid::roundrectGrob) %\u0026gt;% as.list() %\u0026gt;% names() # Somehow last one is NULL arg_names \u0026lt;- arg_names[-length(arg_names)] arg_names ## [1] \u0026#34;x\u0026#34; \u0026#34;y\u0026#34; \u0026#34;width\u0026#34; \u0026#34;height\u0026#34;  ## [5] \u0026#34;default.units\u0026#34; \u0026#34;r\u0026#34; \u0026#34;just\u0026#34; \u0026#34;name\u0026#34;  ## [9] \u0026#34;gp\u0026#34; \u0026#34;vp\u0026#34; # Extract the arguments roundrectGrob needs from grobs$children[3] extracted_args \u0026lt;- map(arg_names, ~pluck(grobs$children[3], 1, .)) names(extracted_args) \u0026lt;- arg_names extracted_args %\u0026gt;% str() ## List of 10 ## $ x : \u0026#39;simpleUnit\u0026#39; num [1:7] 0.0208native 0.16native 0.299native 0.438native 0.576native ... ## ..- attr(*, \u0026#34;unit\u0026#34;)= int 4 ## $ y : \u0026#39;simpleUnit\u0026#39; num [1:7] 0.119native 0.735native 0.647native 0.207native 0.529native ... ## ..- attr(*, \u0026#34;unit\u0026#34;)= int 4 ## $ width : \u0026#39;simpleUnit\u0026#39; num [1:7] 0.125native 0.125native 0.125native 0.125native 0.125native 0.125native 0.125native ## ..- attr(*, \u0026#34;unit\u0026#34;)= int 4 ## $ height : \u0026#39;simpleUnit\u0026#39; num [1:7] 0.0733native 0.689native 0.601native 0.161native 0.484native ... ## ..- attr(*, \u0026#34;unit\u0026#34;)= int 4 ## $ default.units: NULL ## $ r : NULL ## $ just : chr [1:2] \u0026#34;left\u0026#34; \u0026#34;top\u0026#34; ## $ name : chr \u0026#34;geom_rect.rect.925\u0026#34; ## $ gp :List of 6 ## ..$ col : logi [1:7] NA NA NA NA NA NA ... ## ..$ fill : chr [1:7] \u0026#34;#F8766D\u0026#34; \u0026#34;#C49A00\u0026#34; \u0026#34;#53B400\u0026#34; \u0026#34;#00C094\u0026#34; ... ## ..$ lwd : num [1:7] 1.42 1.42 1.42 1.42 1.42 ... ## ..$ lty : num [1:7] 1 1 1 1 1 1 1 ## ..$ linejoin: chr \u0026#34;mitre\u0026#34; ## ..$ lineend : chr \u0026#34;butt\u0026#34; ## ..- attr(*, \u0026#34;class\u0026#34;)= chr \u0026#34;gpar\u0026#34; ## $ vp : NULL   As you can can see, in our vector extracted_args the components x, y and so on are vectors of length 7 (since we have 7 bars in p). As I said before, this works because it is a rectGrob. But, with a roundrectGrob this would cause errors.\nNext, let us make sure that we know how many rectangles we need to change. Also, we will need to specify the radius r, and the graphical parameters gp should always have the same amount of arguments.\n1 2 3 4 5 6 7 8 9  ## How many rectangles are there? n_rects \u0026lt;- extracted_args$x %\u0026gt;% length() ## Add radius r extracted_args$r \u0026lt;- unit(rep(0.25, n_rects), \u0026#39;npc\u0026#39;) ## Make sure that all list components in gp have equally many values extracted_args$gp$linejoin \u0026lt;- rep(extracted_args$gp$linejoin, n_rects) extracted_args$gp$lineend \u0026lt;- rep(extracted_args$gp$lineend, n_rects)   Now comes the tedious part. We have to split up extracted_args into multiple nested lists. Unfortunately, the purrr package does not provide a function that works the way we want. That\u0026rsquo;s because we need many custom steps here. For instance, for the columns x and y we have to always extract a single value out of extracted_args. But with the columns just and name we need to extract the whole vector. Also, we have to adjust the names to ensure that they are unique.\nIn this blog post, we will get the tedious stuff out of the way with the following helper functions. Feel free to ignore them, if you only care about the general idea.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  ## Write function that does splitting for each rectangle ## Found no suitable purrr function that works in my case extract_value \u0026lt;- function(list, arg, rect) { x \u0026lt;- list[[arg]] # name and just need do be treated different # In all cases just pick the i-th entry of list[[arg]] if (!(arg %in% c(\u0026#39;name\u0026#39;, \u0026#39;just\u0026#39;))) return(x[rect]) ## There is only one name, so extract that and modify id if (arg == \u0026#39;name\u0026#39;) { return(paste0(x, rect)) } # \u0026#39;just\u0026#39; is two part vector and should always be the same if (arg == \u0026#39;just\u0026#39;) return(x) } split_my_list \u0026lt;- function(list, n_rects) { combinations \u0026lt;- tibble( rect = 1:n_rects, arg = list(names(list)) ) %\u0026gt;% unnest(cols = c(arg)) flattened_list \u0026lt;- combinations %\u0026gt;% pmap(~extract_value(list, ..2, ..1)) names(flattened_list) \u0026lt;- combinations$arg split(flattened_list, combinations$rect) }   Finally, we can split extracted_args into sub-lists. Each of these is then used to call grid::roundrectGrob() with do.call(). Then, we have to replace the same part in our list grobs as we did before. However, since we have multiple grobs now that need to be put into a single location. Therefore, we have to bundle the grobs into one object. This is done via grid::grobTree() and do.call().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  list_of_arglists \u0026lt;- split_my_list(extracted_args, n_rects) list_of_grobs \u0026lt;- map(list_of_arglists, ~do.call(grid::roundrectGrob, .)) # Build new list of grobs by replacing one part in old list grobs_new \u0026lt;- grobs # save one list argument into children[3] grobs_new$children[3] \u0026lt;- do.call(grid::grobTree, list_of_grobs) %\u0026gt;% list() l_new \u0026lt;- l l_new$grobs[[6]] \u0026lt;- grobs_new # Draw Plot grid::grid.newpage() grid::grid.draw(l_new)   Conclusion Woooow! Marvel at our glorious rounded rectangles! Thanks to our excellent programming skills we made it through the grob jungle. In practice, it is probably easier to use geom_chicklet(). But still, this was a somewhat fun exercise and helped to demystify grobs (at least to some extend).\nThat\u0026rsquo;s it for today. If you\u0026rsquo;ve made it this far, then you already know that you should follow me on Twitter and/or subscribe to my RSS feed. So, I expect you to be here next time. There\u0026rsquo;s no way out anymore. So long!\n","description":"We build rebuild a 'Storytelling with Data' plot which uses rounded rectangles. I'll show you an easy and a hard way to make rectangles round.","id":3,"section":"post","tags":[],"title":"Storytelling in ggplot using rounded rectangles","uri":"https://albert-rapp.de/post/2022-05-01-use-grobs-to-get-rounded-corners/"},{"content":"Last week, I gave a short workshop teaching R to Economics students without prior programming experience. On Twitter, I shared six lessons that I wish I had learnt before. This blog post is a more detailed account of my experience. Additionally, I incorporated many excellent suggestions from others on Twitter into this blog post.\nDo less The first and most important lesson is that I have to plan to do less. This is probably the trickiest part for me. Usually, I like to create an ambitious curriculum for my students. For example, in my semester-long YARDS course for mathematicians, I introduce many topics like\n data viz and wrangling with ggplot2 and dplyr, if-conditions and for-loops (yes, I think everyone should be familiar with for-loops), model building with tidymodels, functional programming with purrr, creating and sharing content with Rmarkdown and interactive web apps with shiny.  In that course, students already came with at least a basic understanding of programming. Most of the time, I only had to lay out a few code examples. Then, I quickly made students work on assignments. As students already knew how to read code and debug, they knew how to get themselves unstuck. This was nice as it enabled my ambitious curriculum. Though, I never fully appreciated how much easier this made teaching.\nScaling down for the workshop Of course, I knew that I needed to scale down in a 4x3h workshop. And I thought I did that. But I was in for a surprise because there was a crucial issue:\n Things that are obvious to someone with programming experience are not obvious to non-programmers at all.\n Let\u0026rsquo;s go through an example. Imagine students already had seen the following fictitious code chunks.\n1  filter(dat, year \u0026gt; 1995)   1 2  ggplot(data = dat) + geom_point(mapping = aes(x = x, y = y))   In one exercise, the students were supposed to plot not the whole data set dat but a filtered version of it. So, I thought that - having seen the code chunks and heard explanations - students would \u0026ldquo;intuitively\u0026rdquo; know to combine the chunks like so\n1 2  ggplot(data = filter(dat, year \u0026gt; 1994)) + geom_point(mapping = aes(x = x, y = y))   But this was not the case. Students were so unfamiliar with code that they were hesitant to \u0026ldquo;stick together\u0026rdquo; the two building blocks. Unfortunately, I did not anticipate this. And without any programming experience, students will get stuck in even more unexpected places. Therefore, be sure to have time for detours. And the only way I see how that\u0026rsquo;s possible is to plan to do less.\nTeach through typing So, providing students with building blocks was not enough. At least in their learning stage it wasn\u0026rsquo;t. Students were just too unfamiliar with code. Fortunately, it turns out that it does not take a huge effort to familiarize students with code. Think: What\u0026rsquo;s the fastest way to learn to write code? Well, write a lot of code.\nAnd that\u0026rsquo;s exactly what I tried with them. Instead of doing demos where only I typed code, I made students type along. And I know this sounds silly. But to my surprise, it helped. It helped a lot!\nIn the end, students had the same building block as if I had done the demo alone. Yet, students were more willing to experiment with the code they have written themselves. In my book, that\u0026rsquo;s a huge step on anyone\u0026rsquo;s programming journey.\nMake time for typing On Twitter, Trader Vix disagreed with this typing along approach. It is a fair point to say that not everyone can type \u0026ldquo;sufficiently\u0026rdquo; fast. Subsequently, some students may fall behind. In this case, I argued that even more students struggled without this typing approach. This way though, students were not falling behind. Instead, they didn\u0026rsquo;t know how to get started.\nOf course, you will have to make time for students to type. Once again: Plan to do less as students will be slower than you are. Also, make sure to ask students if they can execute the code. This can even have additional benefits. In one case, the students couldn\u0026rsquo;t execute the code we typed. From their description, I could tell that they forgot quotes as in x == 'text'. Thus, typing together can also generate teachable moments.\nStart with ggplot Some say vector manipulation should be the first thing to teach. I didn\u0026rsquo;t and this was one of the few things I wouldn\u0026rsquo;t change. My non-statistically inclined students seemed to find visual results more engaging than number/vector crunching. And once students can create graphics, the nice thing is that you can always refer back to that for motivation. Let me elaborate.\nShockingly, data wrangling does not have value in itself. I know that as mathematicians/statisticians/number crunchers it is hard to imagine that. However, incorporating a calculated value in a visualizations can generate insights. And many people find that more valuable. Take a look at two examples that use this idea. Both consider the ames data from {modeldata}.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  library(tidyverse) data(ames, package = \u0026#39;modeldata\u0026#39;) dat \u0026lt;- ames %\u0026gt;% janitor::clean_names() %\u0026gt;% select(lot_area, sale_price, neighborhood) dat ## # A tibble: 2,930 x 3 ## lot_area sale_price neighborhood ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt;  ## 1 31770 215000 North_Ames  ## 2 11622 105000 North_Ames  ## 3 14267 172000 North_Ames  ## 4 11160 244000 North_Ames  ## 5 13830 189900 Gilbert  ## 6 9978 195500 Gilbert  ## 7 4920 213500 Stone_Brook  ## 8 5005 191500 Stone_Brook  ## 9 5389 236500 Stone_Brook  ## 10 7500 189000 Gilbert  ## # ... with 2,920 more rows   Slice for highlighting Why would you want to extract specific rows from a data set? Here, we use it to highlight specific houses.\nHow do we do that? We extract the rows and throw that into another point layer.\nNow, come up with an artificial story why house 1, 5, 7 and 10 may be important. Bam! You now have a visual motivation for why slicing data is a neat technique to master.\n1 2 3 4 5 6  ggplot(dat, aes(lot_area, sale_price)) + geom_point(col = \u0026#39;grey80\u0026#39;) + geom_point(data = slice(dat, 1, 5, 7, 10), col = \u0026#39;red\u0026#39;, size = 2) + scale_x_log10() + scale_y_log10() + theme_minimal()   Summaries for context The same can be done to motivate vector calculations. Why is it good to extract a vector and compute its median? To put your other data into context. Let\u0026rsquo;s try that out for the sale prices in dat.\n1 2 3 4 5 6 7 8 9  x \u0026lt;- dat$sale_price med \u0026lt;- median(x) ggplot() + geom_jitter(aes(x = x, y = 1), alpha = 0.25) + geom_point(aes(x = med, y = 1), col = \u0026#39;red\u0026#39;, size = 5) + scale_x_log10() + labs(x = \u0026#39;Sale price\u0026#39;) + theme_minimal()   For a more elaborate case, you can do the same for each neighborhood.\n1 2 3 4 5 6 7 8 9  summaries \u0026lt;- dat %\u0026gt;% group_by(neighborhood) %\u0026gt;% summarise(med = median(sale_price)) ggplot(dat, aes(x = sale_price, y = neighborhood)) + geom_point(alpha = 0.2) + geom_point(data = summaries, aes(x = med), col = \u0026#39;red\u0026#39;, size = 3) + scale_x_log10() + theme_minimal()   Is ggplot too hard? So, why didn\u0026rsquo;t I use the plot() function instead of ggplot? That\u0026rsquo;s because most people (including myself) would agree that ggplot\u0026rsquo;s defaults are visually more pleasing. But the more pressing question is: Was ggplot too hard for the students? Let\u0026rsquo;s take a step back on this one.\nEvery new syntax is confusing Take a look at the following quote.\nWhen teaching, be careful not to mix up \\\"I learned this a long time\rago\\\" with \\\"This is\rsimple\\\"\\#rstats\r--- David Robinson (@drob)\rApril\r20, 2016\r\r\rIn the same spirit, let me say:\n When teaching, be careful not to mix up \u0026ldquo;I learned this only fairly recently\u0026rdquo; with \u0026ldquo;This is pretty advanced stuff and better not taught early on\u0026rdquo;.\n Personally, I have encountered ggplot and the tidyverse only late in my programming game. Therefore, I have refrained from teaching it to students for some time. Honestly though, this was also due to my own lack of wanting to learn new \u0026ldquo;complicated\u0026rdquo; syntax. As David Robinson states in his insightful \u0026ldquo;Teach the tidyverse to beginners\u0026rdquo; essay:\n\u0026quot;\u0026hellip;all programming syntax is confusing for non-programmers.\u0026quot;\nIn my case, I was not a non-programmer. But ggplot has its own ecosystem with its own syntax. Thus, it is easy to say that ggplot is way too hard for beginners. But the truth is:\n Every syntax is confusing to the uninitiated.\n So, did students cope with ggplot or not? Coming back to my students, this long-winded detour explains why they did surprisingly well with ggplot. Sure, the intricacies of aes() are probably not fully understood after this workshop. But having no prior experience in programming whatsoever, they were surprisingly willing to accept that aes() is just a necessary fact of life ggplot.\nAs a mathematician, I like to compare this to integral notation $ \\int_0^1 f(x)\\, \\mathrm{d}x $. In high school, I did not fully grasp the meaning of $ \\mathrm{d}x $. But I accepted its presence and treated it like a delimiter for what\u0026rsquo;s inside and outside the integral. Clearly, this is far from understanding the meaning of this notation. Yet, it enabled me to do what I needed to do.\nOverall, students would sometimes forget to put e.g.Â col = into the correct place. For most parts, though, the students accepted the syntax just the way it is and understood that within aes() we refer to the columns in data. And what may sound even more surprising:\nStudents fared better with ggplot() than with other functions like filter(), slice() or mutate().\nI suspect that is because ggplot is its own ecosystem and each plot follows the same template. But filter(), slice() and mutate() each work differently. One wants a conditional statement, one wants row indices and one wants a new column name plus its \u0026ldquo;formula\u0026rdquo;.\nStress to save variables before using them This was another surprising moments for me. Apparently, if you have never programmed, it is not obvious to save results. Therefore, make sure that students understand that calculations can only be used later if they are saved into a variable. But this variable-saving line has to be executed for the actual saving to happen. Often, students would have a code chunk like this\n1 2 3  dat \u0026lt;- filter(ames, Sale_Price \u0026gt; 200000) ggplot(data = dat) + geom_point(mapping = aes(x = Lot_Area, Sale_Price))   But then, they would only execute the second line. More often than not, there was no previous dat variable in the environment and students would get an error. This was confusing because the dat \u0026lt;- line is right there. Of course, that does not mean the line was executed. Unfortunately, you will likely have to mention this multiple times before it sticks.\nTeach named functions The symbols$, [ and ] all have their rightful place in R. However, students unanimously found using memorable function names like pull() or select() easier to learn. But remember: Do less. I tried to show students multiple ways to get a job done using e.g.Â $ or pull(). This only caused confusion. So, use only one approach.\nThough, in this specific case I would likely go with $ instead of pull(). Of course, pull() is nice but chances are that students will encounter $ at some point. Thus, see this as an investment into enabling students to read more code. For the same reason, Nikita Telkar suggested to teach the full name notation, e.g.Â dplyr::select().\nPersonally, I would not use full names all the time though. But one particular use case comes to mind. After experiencing a few errors due to not using capital letters in column names, the students were more than willing to use clean_names() from {janitor}. So, janitor::clean_names() may be a good showcase for the full name notation.\nUse pipes As mentioned, saving variables felt foreign to students at first. The same was true for nesting functions. Consequently, two step processes like selecting and filtering were hard. Here, pipes helped.\nThough I\u0026rsquo;ve had Math students complain that pipes feel wrong, for the Economics students it was just the right thing. In fact, the pipe often mimicked what students wanted to do anyway. Frequently, students knew that they want to e.g.Â filter data. So they typed filter(year \u0026gt; 1999). Of course, this misses the data. It seems like students were so caught up in what they wanted to do that they forgot to tell R what data to use. But chaining multiple function calls circumvented that (modulo at the beginning of the chain).\nAdditionally, let me mention that there were great contributions on Twitter by Deidre Toher and Fran Barton. Deidre Toher suggested reading pipes as \u0026lsquo;then\u0026rsquo; and Fran Barton pronounced conditions like x[x \u0026gt; 5] as \u0026lsquo;such that\u0026rsquo;. This kind of reading code aloud, could go a long way to make code feel more natural for beginners.\nOther reactions These were my six lessons. All of them were enhanced by a lot of great suggestions from Twitter. But not all suggestions fit into the previous text. So, let me mention a few more contributors.\nTidyverseSkeptic I don\u0026rsquo;t want to give the impression that I filtered out \u0026ldquo;negative\u0026rdquo; comments. So, let me point out that Prof.Â Norm Matloff chimed in and advocated against using pipes. For his elaborate reasons see the TidyverseSkeptic essay. That being said, I do not agree with many statements in his tweet or his essay (though I agree that for-loops should not be a reason to feel ashamed).\nHaving gotten this off my chest, I really do not want to comment more on the artificial fight between base-R and the tidyverse. To me, a lot of this dispute feels like a highly subjective back-and-forth between both sides anyway. I do like parts of both worlds and I don\u0026rsquo;t want to throw my own subjective two cents into the mix.\nMiscellaneous tips So, let me close this post on a lighter note with the remaining contributions.\n  Remington Moll suggested to use data sets that students care about. This could encompass letting students choose a data set from five prepared data sets. Of course, this could potentially take up a huge amount of preparation time. But maybe skimming a few data sets in advance could be enough for demos.\n  Fadel Megahed shared some of his own course material. In his slides, he uses timers and I like the idea. This way, in-class time management may become easier. I always struggle with sticking to the allotted time during a set of exercises.\n  Dr.Â Robert M Flight shared the datacarpentry lessons. I have only skimmed them but I\u0026rsquo;ve heard people praise these lessons multiple times already.\n  Closing This concludes my blog post. Thanks to everyone on Twitter for contributing. It pleased me to see that many people in the R community are passionate about teaching. If you liked this essay, then consider following me on Twitter and/or subscribing to my RSS feed. See you next time!\n","description":"I held a short workshop teaching R to Economics students. Here are six things that I wish I had known in advance.","id":4,"section":"post","tags":["opinion"],"title":"6 Lessons that I learned from teaching R to non-programmers","uri":"https://albert-rapp.de/post/2022-04-15-lessons-learned-from-teaching-nonprogrammers/"},{"content":"Intro It is mid-April and the #30daychartchallenge is well on its way. One glace at the hashtag\u0026rsquo;s Twitter feed suffices to realize that there are great contributions. That\u0026rsquo;s a perfect opportunity to collect data viz examples for future inspirations.\nIdeally, I can scroll through Twitter and with a few clicks incorporate these contributions straight into my Obsidian or any other Markdown-based note-taking system. Unfortunately, rtweet\u0026rsquo;s snapshot function does not seem to work anymore. So, let\u0026rsquo;s build something on our own that gets the job done. The full script can be found on GitHub gist. Here\u0026rsquo;s what we will need:\n Twitter app bearer token (to access Twitter\u0026rsquo;s API) - I\u0026rsquo;ll show you how to get that Elevated API access (just a few clicks once you have a bearer token) Dummy mail account to send tweets to  Overview Before we begin, let me summarize what kind of note-taking process I have in mind:\n  Stroll through Twitter and see great data viz on twitter.\n  Send tweet link and a few comments via mail to a dummy mail account\n  A scheduled process accesses the dummy mail account and scans for new mails from authorized senders.\n  If there is a new mail, R extracts tweet URL and uses Twitter\u0026rsquo;s API to download the tweet\u0026rsquo;s pictures and texts.\n  A template Markdown file is used to create a new note that contains the images and texts.\n  Markdown file is copied to your note-taking system within your file system.\n  Ideally, your Markdown template contains tags like #dataviz and #twitter so that your new note can be easily searched for.\n  Next time you look for inspiration, stroll through your collections or search for comments.\n  Preparations Ok, we know what we want to accomplish. Time to get the prelims done. First, we will need a Twitter developer account. Then, we have to mask sensitive information in our code. If you already have a twitter app resp. a bearer token and know the keyring package, feel free to skip this section.\nGet Twitter developer account Let\u0026rsquo;s create a developer account for Twitter. Unfortunately, there is no way to get such an account without providing Twitter with your phone number. Sadly, if this burden on your privacy is a problem for you, then you cannot proceed. Otherwise, create an account at developer.twitter.com.\nIn your developer portal, create a project. Within this project create an app. Along the way, you will get a bunch of keys, secrets, IDs and tokens. You will see them only once, so you will have to save them somewhere. I suggest saving them into a password manager like bitwarden.\nWhen you create your app or shortly after, you will need to set the authentication settings. I use OAuth 2.0. This requires\n type of app: Automated bot or app Callback URI / Redirect URI: http://127.0.0.1:1410 (DISCLAIMER: This is magic to me but the rtweet docs - or possibly some other doc (not entirely sure anymore)- taught me to set up an app that way) Website URL: Your Twitter link (in my case https://twitter.com/rappa753)  Next, you will likely need to upgrade your project to \u0026lsquo;elevated\u0026rsquo; status. This can be done for free on your project\u0026rsquo;s dashboard. From what I recall, you will have to fill out a form and tell Twitter what you want to do with your app. Just be honest and chances are that your request will immediately be granted. Just be yourself! What could possibly go wrong? Go get the girl elevated status (ahhh, what a perfect opportunity for a Taylor song).\n Click on detailed features to apply for higher access  How to embed your bearer token and other sensitive material in your code Use the keyring package to first save secrets via key_set and then extract them in your session via key_get(). This way, you won\u0026rsquo;t share your sensitive information by accident when you share your code (like I do). In this post, I do this for my bearer token, my dummy mail, my dummy mail\u0026rsquo;s password and for the allowed senders (that will be the mail where the tweets come from).\n1 2 3 4  bearer_token \u0026lt;- keyring::key_get(\u0026#39;twitter-bearer-token\u0026#39;) user_mail \u0026lt;- keyring::key_get(\u0026#39;dataviz-mail\u0026#39;) password_mail \u0026lt;- keyring::key_get(\u0026#39;dataviz-mail-password\u0026#39;) allowed_senders \u0026lt;- keyring::key_get(\u0026#39;allowed_senders\u0026#39;)   The allowed_senders limitation is a precaution so that we do not accidentally download some malicious spam mail from God knows who onto our computer. I am no security expert but this feels like a prudent thing to do. If one of you fellow readers knows more about this security business, feel kindly invited to reach out to me with better security strategies.\nWhat to do once we have a URL Let\u0026rsquo;s assume for the sake of this section that we already extracted a tweet URL from a mail. Here\u0026rsquo;s the URL that we will use. In fact, it\u0026rsquo;s Christian Gebhard\u0026rsquo;s tweet that inspired me to start this project. From the URL we can extract the tweet\u0026rsquo;s ID (the bunch of numbers after /status/). Also, we will need the URL of Twitter\u0026rsquo;s API.\n1 2 3 4 5  library(stringr) # for regex matching library(dplyr) # for binding rows and pipe tweet_url \u0026lt;- \u0026#39;https://twitter.com/c_gebhard/status/1510533315262042112\u0026#39; tweet_id \u0026lt;- tweet_url %\u0026gt;% str_match(\u0026#34;status/([0-9]+)\u0026#34;) %\u0026gt;% .[, 2] API_url \u0026lt;- \u0026#39;https://api.twitter.com/2/tweets\u0026#39;   Use GET() to access Twitter API Next, we use the GET() function from the httr package to interact with Twitter\u0026rsquo;s API.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  library(httr) # for API communication auth \u0026lt;- paste(\u0026#34;Bearer\u0026#34;, bearer_token) # API needs format \u0026#34;Bearer \u0026lt;my_token\u0026gt;\u0026#34; # Make request to API request \u0026lt;- GET( API_url, add_headers(Authorization = auth), query = list( ids = tweet_id, tweet.fields = \u0026#39;created_at\u0026#39;, # time stamp expansions = \u0026#39;attachments.media_keys,author_id\u0026#39;, # necessary expansion fields for img_url media.fields = \u0026#39;url\u0026#39; # img_url ) ) request ## Response [https://api.twitter.com/2/tweets?ids=1510533315262042112\u0026amp;tweet.fields=created_at\u0026amp;expansions=attachments.media_keys%2Cauthor_id\u0026amp;media.fields=url] ## Date: 2022-04-14 07:32 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 690 B   So, how do we know how to use the GET() function? Well, I am no expert on APIs but let me try to explain how I came up with the arguments I used here.\nRemember those toys you would play with as a toddler where you try to get a square through a square-shaped hole, a triangle through a triangle-shaped hole and so on? You don\u0026rsquo;t? Well, neither do I. Who remembers that stuff from very early childhood?\nBut I hear that starting a sentence with \u0026ldquo;Remember those\u0026hellip;\u0026rdquo; is good for building a rapport with your audience. So, great! Now that we feel all cozy and connected, I can tell you how I managed to get the API request to work.\nAnd the truth is actually not that far from the toddler \u0026ldquo;intelligence test\u0026rdquo;. First, I took a look at a help page from Twitter\u0026rsquo;s developer page. Then, I hammered at the GET() function until its output contained a URL that looks similar to the example I found. Here\u0026rsquo;s the example code I was aiming at.\ncurl --request GET 'https://api.twitter.com/2/tweets?ids=1263145271946551300\u0026amp; expansions=attachments.media_keys\u0026amp; media.fields=duration_ms,height,media_key,preview_image_url,public_metrics,type,url,width,alt_text' --header 'Authorization: Bearer $BEARER_TOKEN'  This is not really R code but it looks like usually you have to feed a GET request with a really long URL. In fact, it looks like the URL needs to contain everything you want to extract from the API. Specifically, the structure of said URL looks like\n the API\u0026rsquo;s base URL (in this case https://api.twitter.com/2/tweets) a question mark ? pairs of keywords (like ids) and a specific value, e.g.Â ids=1263145271946551300, that are connected via \u0026amp;  Therefore, it is only a matter of figuring out how to make the output of GET() deliver this result. Hints on that came from GET() examples in the docs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  GET(\u0026#34;http://google.com/\u0026#34;, path = \u0026#34;search\u0026#34;, query = list(q = \u0026#34;ham\u0026#34;)) ## Response [http://www.google.com/search?q=ham] ## Date: 2022-04-14 07:32 ## Status: 200 ## Content-Type: text/html; charset=ISO-8859-1 ## Size: 98.2 kB ## \u0026lt;!doctype html\u0026gt;\u0026lt;html lang=\u0026#34;de\u0026#34;\u0026gt;\u0026lt;head\u0026gt;\u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt;\u0026lt;meta content=\u0026#34;/im... ## document.documentElement.addEventListener(\u0026#34;submit\u0026#34;,function(b){var a;if(a=b.t... ## var a=window.performance;window.start=Date.now();a:{var b=window;if(a){var c=... ## var f=this||self;var g,h=null!=(g=f.mei)?g:1,m,n=null!=(m=f.sdo)?m:!0,p=0,q,r... ## e);var l=a.fileName;l\u0026amp;\u0026amp;(b+=\u0026#34;\u0026amp;script=\u0026#34;+c(l),e\u0026amp;\u0026amp;l===window.location.href\u0026amp;\u0026amp;(e=do... ## var c=[],e=0;window.ping=function(b){-1==b.indexOf(\u0026#34;\u0026amp;zx\u0026#34;)\u0026amp;\u0026amp;(b+=\u0026#34;\u0026amp;zx=\u0026#34;+Date.no... ## var k=this||self,l=function(a){var b=typeof a;return\u0026#34;object\u0026#34;==b\u0026amp;\u0026amp;null!=a||\u0026#34;fu... ## b}).join(\u0026#34; \u0026#34;))};function w(){var a=k.navigator;return a\u0026amp;\u0026amp;(a=a.userAgent)?a:\u0026#34;\u0026#34;... ## !1}e||(d=null)}}else\u0026#34;mouseover\u0026#34;==b?d=a.fromElement:\u0026#34;mouseout\u0026#34;==b\u0026amp;\u0026amp;(d=a.toElem... ## window.screen\u0026amp;\u0026amp;window.screen.width\u0026lt;=c\u0026amp;\u0026amp;window.screen.height\u0026lt;=c\u0026amp;\u0026amp;document.getE... ## ... GET(\u0026#34;http://httpbin.org/get\u0026#34;, add_headers(a = 1, b = 2)) ## Response [http://httpbin.org/get] ## Date: 2022-04-14 07:32 ## Status: 200 ## Content-Type: application/json ## Size: 397 B ## { ## \u0026#34;args\u0026#34;: {},  ## \u0026#34;headers\u0026#34;: { ## \u0026#34;A\u0026#34;: \u0026#34;1\u0026#34;,  ## \u0026#34;Accept\u0026#34;: \u0026#34;application/json, text/xml, application/xml, */*\u0026#34;,  ## \u0026#34;Accept-Encoding\u0026#34;: \u0026#34;deflate, gzip\u0026#34;,  ## \u0026#34;B\u0026#34;: \u0026#34;2\u0026#34;,  ## \u0026#34;Host\u0026#34;: \u0026#34;httpbin.org\u0026#34;,  ## \u0026#34;User-Agent\u0026#34;: \u0026#34;libcurl/7.64.1 r-curl/4.3.2 httr/1.4.2\u0026#34;,  ## \u0026#34;X-Amzn-Trace-Id\u0026#34;: \u0026#34;Root=1-6257ce20-7902b3b55883ec5d3c0a1389\u0026#34; ## ...   So, the first example shows how an argument query can be filled with a list that creates the URL we need. The second examples shows us that there is something called add_headers(). Do I know exactly what that is? I mean, from a technical perspective of what is going on behind the scenes? Definitely not. But Twitter\u0026rsquo;s example request had something called header. Therefore, add_headers() is probably something that does what the Twitter API expects.\nAnd where do the key-value pairs come from? I found these strolling through Twitter\u0026rsquo;s data dictionary. Thus, a GET() request was born and I could feel like a true Hackerman.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  auth \u0026lt;- paste(\u0026#34;Bearer\u0026#34;, bearer_token) # API needs format \u0026#34;Bearer \u0026lt;my_token\u0026gt;\u0026#34; # Make request to API and parse to list request \u0026lt;- GET( API_url, add_headers(Authorization = auth), query = list( ids = tweet_id, tweet.fields = \u0026#39;created_at\u0026#39;, # time stamp expansions = \u0026#39;attachments.media_keys,author_id\u0026#39;, # necessary expansion fields for img_url media.fields = \u0026#39;url\u0026#39; # img_url ) )   Alright, we successfully requested data. Now, it becomes time to parse it to something useful. The content() function will to that.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  parsed_request \u0026lt;- request %\u0026gt;% content(\u0026#39;parsed\u0026#39;) parsed_request ## $data ## $data[[1]] ## $data[[1]]$author_id ## [1] \u0026#34;1070306701\u0026#34; ##  ## $data[[1]]$created_at ## [1] \u0026#34;2022-04-03T08:23:01.000Z\u0026#34; ##  ## $data[[1]]$text ## [1] \u0026#34;#30DayChartChallenge #Day3 - Topic: historical\\n\\nBack to the Shakespeare data! Hamlet is is longest play, the comedies tend to be shorter.\\n\\nTool: #rstats\\nData: kaggle users LiamLarsen, aodhan\\nColor-Scale: MetBrewer\\nFonts: Niconne, Noto Sans (+Mono)\\nCode: https://t.co/iXAbniQDCb https://t.co/JCNrYH9uP4\u0026#34; ##  ## $data[[1]]$attachments ## $data[[1]]$attachments$media_keys ## $data[[1]]$attachments$media_keys[[1]] ## [1] \u0026#34;3_1510533145334104067\u0026#34; ##  ##  ##  ## $data[[1]]$id ## [1] \u0026#34;1510533315262042112\u0026#34; ##  ##  ##  ## $includes ## $includes$media ## $includes$media[[1]] ## $includes$media[[1]]$media_key ## [1] \u0026#34;3_1510533145334104067\u0026#34; ##  ## $includes$media[[1]]$type ## [1] \u0026#34;photo\u0026#34; ##  ## $includes$media[[1]]$url ## [1] \u0026#34;https://pbs.twimg.com/media/FPZ95H0XwAMHA8q.jpg\u0026#34; ##  ##  ##  ## $includes$users ## $includes$users[[1]] ## $includes$users[[1]]$id ## [1] \u0026#34;1070306701\u0026#34; ##  ## $includes$users[[1]]$name ## [1] \u0026#34;Christian Gebhard\u0026#34; ##  ## $includes$users[[1]]$username ## [1] \u0026#34;c_gebhard\u0026#34;   Extract tweet data from what the API gives us and download images We have seen that parsed_request is basically a large list that contains everything we requested from the API. Unfortunately, it is a highly nested list, so we have to do some work to extract the parts we actually want. pluck() from the purrr package is our best friend on this one. Here\u0026rsquo;s all the information we extract from the parsed_request.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  library(purrr) # for pluck and map functions # Extract necessary information from list-like structure tweet_text \u0026lt;- parsed_request %\u0026gt;% pluck(\u0026#34;data\u0026#34;, 1, \u0026#39;text\u0026#39;) tweet_text ## [1] \u0026#34;#30DayChartChallenge #Day3 - Topic: historical\\n\\nBack to the Shakespeare data! Hamlet is is longest play, the comedies tend to be shorter.\\n\\nTool: #rstats\\nData: kaggle users LiamLarsen, aodhan\\nColor-Scale: MetBrewer\\nFonts: Niconne, Noto Sans (+Mono)\\nCode: https://t.co/iXAbniQDCb https://t.co/JCNrYH9uP4\u0026#34; tweet_user \u0026lt;- parsed_request %\u0026gt;% pluck(\u0026#34;includes\u0026#34;, \u0026#39;users\u0026#39;, 1, \u0026#39;username\u0026#39;) tweet_user ## [1] \u0026#34;c_gebhard\u0026#34; # We will use the tweet date and time as part of unique file names # Replace white spaces and colons (:) for proper file names tweet_date \u0026lt;- parsed_request %\u0026gt;% pluck(\u0026#34;data\u0026#34;, 1, \u0026#39;created_at\u0026#39;) %\u0026gt;% lubridate::as_datetime() %\u0026gt;% str_replace(\u0026#39; \u0026#39;, \u0026#39;_\u0026#39;) %\u0026gt;% str_replace_all(\u0026#39;:\u0026#39;, \u0026#39;\u0026#39;) tweet_date ## [1] \u0026#34;2022-04-03_082301\u0026#34; img_urls \u0026lt;- parsed_request %\u0026gt;% pluck(\u0026#34;includes\u0026#34;, \u0026#39;media\u0026#39;) %\u0026gt;% bind_rows() %\u0026gt;% # bind_rows for multiple pictures, i.e. multiple URLS filter(type == \u0026#39;photo\u0026#39;) %\u0026gt;% pull(url) img_urls ## [1] \u0026#34;https://pbs.twimg.com/media/FPZ95H0XwAMHA8q.jpg\u0026#34;   Next, download all the images via the img_urls and download.file(). We will use walk2() to download all files (in case there are multiple images/URLs) and save the files into PNGs that are named using the unique tweet_date IDs. Remember to set mode = 'wb' in download.file(). I am not really sure why but without it you will save poor quality images.\n1 2 3  # Download image - set mode otherwise download is blurry img_names \u0026lt;- paste(\u0026#39;tweet\u0026#39;, tweet_user, tweet_date, seq_along(img_urls), sep = \u0026#34;_\u0026#34;) walk2(img_urls, img_names, ~download.file(.x, paste0(.y, \u0026#39;.png\u0026#39;), mode = \u0026#39;wb\u0026#39;))   So let\u0026rsquo;s do a quick recap of what we have done so far. We\n Assembled an API request Parsed the return of the request Cherrypicked the information that we want from the resulting list Used the image URLs to download and save the files to our working directory.  Let\u0026rsquo;s cherish this mile stone with a dedicated function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  request_twitter_data \u0026lt;- function(tweet_url, bearer_token) { # Extract tweet id by regex tweet_id \u0026lt;- tweet_url %\u0026gt;% str_match(\u0026#34;status/([0-9]+)\u0026#34;) %\u0026gt;% .[, 2] auth \u0026lt;- paste(\u0026#34;Bearer\u0026#34;, bearer_token) # API needs format \u0026#34;Bearer \u0026lt;my_token\u0026gt;\u0026#34; API_url \u0026lt;- \u0026#39;https://api.twitter.com/2/tweets\u0026#39; # Make request to API and parse to list parsed_request \u0026lt;- GET( API_url, add_headers(Authorization = auth), query = list( ids = tweet_id, tweet.fields = \u0026#39;created_at\u0026#39;, # time stamp expansions=\u0026#39;attachments.media_keys,author_id\u0026#39;, # necessary expansion fields for img_url media.fields = \u0026#39;url\u0026#39; # img_url ) ) %\u0026gt;% content(\u0026#39;parsed\u0026#39;) # Extract necassary information from list-like structure tweet_text \u0026lt;- parsed_request %\u0026gt;% pluck(\u0026#34;data\u0026#34;, 1, \u0026#39;text\u0026#39;) tweet_user \u0026lt;- parsed_request %\u0026gt;% pluck(\u0026#34;includes\u0026#34;, \u0026#39;users\u0026#39;, 1, \u0026#39;username\u0026#39;) # Make file name unique through time-date combination # Replace white spaces and colons (:) for proper file names tweet_date \u0026lt;- parsed_request %\u0026gt;% pluck(\u0026#34;data\u0026#34;, 1, \u0026#39;created_at\u0026#39;) %\u0026gt;% lubridate::as_datetime() %\u0026gt;% str_replace(\u0026#39; \u0026#39;, \u0026#39;_\u0026#39;) %\u0026gt;% str_replace_all(\u0026#39;:\u0026#39;, \u0026#39;\u0026#39;) img_urls \u0026lt;- parsed_request %\u0026gt;% pluck(\u0026#34;includes\u0026#34;, \u0026#39;media\u0026#39;) %\u0026gt;% bind_rows() %\u0026gt;% filter(type == \u0026#39;photo\u0026#39;) %\u0026gt;% pull(url) # Download image - set mode otherwise download is blurry img_names \u0026lt;- paste(\u0026#39;tweet\u0026#39;, tweet_user, tweet_date, seq_along(img_urls), sep = \u0026#34;_\u0026#34;) walk2(img_urls, img_names, ~download.file(.x, paste0(.y, \u0026#39;.png\u0026#39;), mode = \u0026#39;wb\u0026#39;)) # Return list with information list( url = tweet_url, text = tweet_text, user = tweet_user, file_names = paste0(img_names, \u0026#39;.png\u0026#39;), file_paths = paste0(getwd(), \u0026#39;/\u0026#39;, img_names, \u0026#39;.png\u0026#39;) ) }   Fill out Markdown template using extracted information and images We have our images and the original tweet now. Thanks to our previous function, we can save all of the information in a list.\n1  request \u0026lt;- request_twitter_data(tweet_url, bearer_token)   So, let\u0026rsquo;s bring all that information into a Markdown file. Here is the template.md file that I have created for this joyous occasion.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  library(readr) # for reading and writing files from/to disk cat(read_file(\u0026#39;template.md\u0026#39;)) ## #dataviz #twitter ##  ## ![[insert_img_name_here]] ##  ## ### Original Tweet ##  ## insert_text_here ##  ## Original: insert_URL_here ##  ## ### Original Mail ##  ## insert_mail_here   As you can see, I started the Markdown template with two tags #dataviz and #twitter. This helps me to search for a specific dataviz faster. Also, I have already written out the Markdown syntax for image imports ![[...]] and added a placeholder insert_img_name_here. This one will be replaced by the file path to the image. Similarly, other placeholders like insert_text_here and insert_mail_here allow me to save the tweet and the mail content into my note taking system too.\nTo do so, I will need a function that replaces all the placeholders. First, I created a helper function that changes the image import placeholder properly, when there are multiple images.\n1 2 3  md_import_strings \u0026lt;- function(file_names) { paste0(\u0026#39;![[\u0026#39;, file_names, \u0026#39;]]\u0026#39;, collapse = \u0026#39;\\n\u0026#39;) }   Then, I created a function that takes the request list that we got from calling our own request_twitter_data() function and iteratively uses str_replace_all(). This iteration is done with reduce2() which will replace all placeholders in template.md .\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  library(tibble) # for easier readable tribble creation # Replace the placeholders in the template # We change original mail place holder later on replace_template_placeholder \u0026lt;- function(template_name, request) { # Create a dictionary for what to replace in template replace_dict \u0026lt;- tribble( ~template, ~replacement, \u0026#39;\\\\!\\\\[\\\\[insert_img_name_here\\\\]\\\\]\u0026#39;, md_import_strings(request$file_names), \u0026#39;insert_text_here\u0026#39;, request$text %\u0026gt;% str_replace_all(\u0026#39;#\u0026#39;, \u0026#39;(#)\u0026#39;), \u0026#39;insert_URL_here\u0026#39;, request$url ) # Iteratively apply str_replace_all and keep only final result reduce2( .x = replace_dict$template, .y = replace_dict$replacement, .f = str_replace_all, .init = read_lines(template_name) ) %\u0026gt;% paste0(collapse = \u0026#39;\\n\u0026#39;) # Collaps lines into a single string } replace_template_placeholder(\u0026#39;template.md\u0026#39;, request) %\u0026gt;% cat() ## #dataviz #twitter ##  ## ![[tweet_c_gebhard_2022-04-03_082301_1.png]] ##  ## ### Original Tweet ##  ## (#)30DayChartChallenge (#)Day3 - Topic: historical ##  ## Back to the Shakespeare data! Hamlet is is longest play, the comedies tend to be shorter. ##  ## Tool: (#)rstats ## Data: kaggle users LiamLarsen, aodhan ## Color-Scale: MetBrewer ## Fonts: Niconne, Noto Sans (+Mono) ## Code: https://t.co/iXAbniQDCb https://t.co/JCNrYH9uP4 ##  ## Original: https://twitter.com/c_gebhard/status/1510533315262042112 ##  ## ### Original Mail ##  ## insert_mail_here   As you can see, my replace_template_placeholder() function also replaces the typical # from Twitter with (#). This is just a precaution to avoid wrong interpretation of these lines as headings in Markdown. Also, the original mail has not been inserted yet because we have no mail yet. But soooon. Finally, we need to write the replaced strings to a file. I got some helpers for that right here.\n1 2 3 4 5 6 7  write_replaced_text \u0026lt;- function(replaced_text, request) { file_name \u0026lt;- request$file_name[1] %\u0026gt;% str_replace(\u0026#39;_1.png\u0026#39;, \u0026#39;.md\u0026#39;) write_lines(replaced_text, file_name) paste0(getwd(), \u0026#39;/\u0026#39;, file_name) } replaced_template \u0026lt;- replace_template_placeholder(\u0026#39;template.md\u0026#39;, request) %\u0026gt;% write_replaced_text(request)   Shuffle files around on your file system Awesome! We created new image files and a new Markdown note in our working directory. Now, we have to move them to our Obsidian vault. This is the place where I collect all my Markdown notes for use in Obsidian. In my case, I will need to move the Markdown note to the vault directory and the images to a subdirectory within this vault. This is because I changed settings in Obsidian that makes sure that all attachments, e.g.Â images, are saved in a separate subdirectory.\nHere\u0026rsquo;s the function I created to get that job done. The function uses the request list again because it contains the file paths of the images. Here, vault_location and attachments_dir are the file paths to my Obsidian vault.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  library(tidyr) # for unnesting move_files \u0026lt;- function(request, replaced_template, vault_location, attachments_dir) { # Create from-to dictionary with file paths in each column move_dict \u0026lt;- tribble( ~from, ~to, request$file_path, paste0(vault_location, \u0026#39;/\u0026#39;, attachments_dir), replaced_template, vault_location ) %\u0026gt;% unnest(cols = \u0026#39;from\u0026#39;) # Copy files from current working directory to destination move_dict %\u0026gt;% pwalk(file.copy, overwrite = T) # Delete files in current working directory walk(move_dict$from, file.remove) }   How to extract URL and other stuff from mail Let\u0026rsquo;s take a quick breather and recap. We have written functions that\n take a tweet URL hussle the Twitter API to give us all its data download the images and tweet text save everything to a new Markdown note based on a template can move the note plus images to the location of our note-taking hub  Not to brag but that is kind of cool. But let\u0026rsquo;s not rest here. We still have to get some work done. After all, we want our workflow to be email-based. So, let\u0026rsquo;s access our mails using R. Then, we can extract a Twitter URL and apply our previous functions. Also, this lets us finally replace the insert_mail_here placeholder in our Markdown note.\nPostman gives you access I have created a dummy mail account at gmail. Using the mRpostman package, we can establish a connection to our mail inbox. After the connection is established, we can filter for all new emails that are sent from our list of allowed_senders.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  library(mRpostman) # for email communication imap_mail \u0026lt;- \u0026#39;imaps://imap.gmail.com\u0026#39; # mail client # Establish connection to imap server con \u0026lt;- configure_imap( url = imap_mail, user = user_mail, password = password_mail ) # Switch to Inbox con$select_folder(\u0026#39;Inbox\u0026#39;) ##  ## ::mRpostman: \u0026#34;Inbox\u0026#34; selected. # Extract mails that are from the list of allowed senders mails \u0026lt;- allowed_senders %\u0026gt;% map(~con$search_string(expr = ., where = \u0026#39;FROM\u0026#39;)) %\u0026gt;% unlist() %\u0026gt;% na.omit() %\u0026gt;% # Remove NAs if no mail from a sender as.numeric() # avoids attributes   Grab URLs from mail If mails is not empty, i.e.Â if there are new mails, then we need to extract the tweet URLs from them. Unfortunately, depending on where you sent your email from, the mail text can be encoded.\nFor example, I send most of the tweets via the share button on Twitter using my Android smartphone. And for some reason, my Android mail client encodes the mails in something called base64. But sending a tweet URL from Thunderbird on my computer works without any encoding. Here are two example mails I have sent to my dummy mail account.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  if (!is_empty(mails)) mail_bodys \u0026lt;- mails %\u0026gt;% con$fetch_text() cat(mail_bodys[[1]]) ## ----_com.samsung.android.email_9939060766922910 ## Content-Type: text/plain; charset=utf-8 ## Content-Transfer-Encoding: base64 ##  ## aHR0cHM6Ly90d2l0dGVyLmNvbS9tYXJpbmExMDE3MjAxMy9zdGF0dXMvMTUxNDI4MDkzNjA5NTI0 ## MDE5ND9zPTIwJnQ9TFp5TzI3aWhjZk5kbUFPTHlxbTByUUhlcmUgaXMgdGhlIHNhbWUgdHdlZXQg ## YnV0IHNlbnQgZnJvbSBteSBBbmRyb2lkIHBob25lLg== ##  ## ----_com.samsung.android.email_9939060766922910 ## Content-Type: text/html; charset=utf-8 ## Content-Transfer-Encoding: base64 ##  ## PGh0bWw+PGhlYWQ+PG1ldGEgaHR0cC1lcXVpdj0iQ29udGVudC1UeXBlIiBjb250ZW50PSJ0ZXh0 ## L2h0bWw7IGNoYXJzZXQ9VVRGLTgiPjwvaGVhZD48Ym9keSBkaXI9ImF1dG8iPmh0dHBzOi8vdHdp ## dHRlci5jb20vbWFyaW5hMTAxNzIwMTMvc3RhdHVzLzE1MTQyODA5MzYwOTUyNDAxOTQ/cz0yMCZh ## bXA7dD1MWnlPMjdpaGNmTmRtQU9MeXFtMHJRPGRpdiBkaXI9ImF1dG8iPjxicj48L2Rpdj48ZGl2 ## IGRpcj0iYXV0byI+SGVyZSBpcyB0aGUgc2FtZSB0d2VldCBidXQgc2VudCBmcm9tIG15IEFuZHJv ## aWQgcGhvbmUuPC9kaXY+PC9ib2R5PjwvaHRtbD4= ##  ## ----_com.samsung.android.email_9939060766922910-- ##  cat(mail_bodys[[2]]) ## Here is the first data viz that popped up on my Twitter feed. ##  ## https://twitter.com/marina10172013/status/1514280936095240194?s=20\u0026amp;t=1_WWgRGzcDEVmOpRMK6E8w ##  ## This mail was sent from my computer using Thunderbird. ##  ## Ahoy! (Just a random fun thing I wanted to write) ##    As you can see, the mail sent from my computer is legible but the other one is gibberish. Thankfully, Allan Cameron helped me out on Stackoverflow to decode the mail. To decode the mail, the trick was to extract the parts between base64 and ----.\nThere are two such texts in the encoded mail. Surprisingly, the first one decoded to a text without line breaks. This is why we take the second encoded part and decode it. However, this will give us an HTML text with all kinds of tags like \u0026lt;div\u0026gt; and what not. Therefore, we use html_read() and html_text2() from the rvest package to handle that. All of this is summarized in this helper function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  decode_encoded_mails \u0026lt;- function(encoded_mails) { # Ressource: https://stackoverflow.com/questions/71772972/translate-encoding-of-android-mail-in-r # Find location in each encoded string where actual text starts start_encoded \u0026lt;- encoded_mails %\u0026gt;% str_locate_all(\u0026#39;base64\\r\\n\\r\\n\u0026#39;) %\u0026gt;% map(~pluck(., 4) + 1) %\u0026gt;% unlist() # Find location in each encoded string where actual text starts end_encoded \u0026lt;- encoded_mails %\u0026gt;% str_locate_all(\u0026#39;----\u0026#39;) %\u0026gt;% map(~pluck(., 3) - 1)%\u0026gt;% unlist() # Use str_sub() to extract encoded text encoded_text \u0026lt;- tibble( string = unlist(encoded_mails), start = start_encoded, end = end_encoded ) %\u0026gt;% pmap(str_sub) # Decode: base64 -\u0026gt; raw -\u0026gt; char -\u0026gt; html -\u0026gt; text encoded_text %\u0026gt;% map(base64enc::base64decode) %\u0026gt;% map(rawToChar) %\u0026gt;% map(rvest::read_html) %\u0026gt;% map(rvest::html_text2) }   I feel like this is the most hacky part of this blog post. Unfortunately, your milage may vary here. If your phone or whatever you use encodes the mails differently, then you may have to adjust the function. But I hope that I have explained enough details and concepts for you to manage that if it comes to this.\nRecall that I send both plain mails from Thunderbird and encoded mails from Android. Therefore, here is another helper that decoded mails if neccessary from both types in one swoop.\n1 2 3 4 5 6 7 8  decode_all_mails \u0026lt;- function(mail_bodys) { # Decode in case mail is base64 decoded is_encoded \u0026lt;- str_detect(mail_bodys, \u0026#39;Content-Transfer-Encoding\u0026#39;) encoded_mails \u0026lt;- mail_bodys[is_encoded] plain_mails \u0026lt;- mail_bodys[!is_encoded] decoded_mails \u0026lt;- encoded_mails %\u0026gt;% decode_encoded_mails() c(decoded_mails, plain_mails) }   The remaining part of the code should be familiar:\n Use decode_all_mails() for decoding Grab URLs with str_extract() Use request_twitter_data() with our URLs Replace placeholders with replace_template_placeholder() This time, replace mail placeholders too with another str_replace() iteration Move files with move_files()  The only new thing is that we use our postman connection to move the processed mails into a new directory (which I called \u0026ldquo;Processed\u0026rdquo;) on the email server. This way, the inbox is empty again or filled only with mails from unauthorized senders.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  if (!is_empty(mails)) { # Grab mail texts and URLs mail_bodys \u0026lt;- mails %\u0026gt;% con$fetch_text() %\u0026gt;% decode_all_mails urls \u0026lt;- mail_bodys %\u0026gt;% str_extract(\u0026#39;https.*\u0026#39;) # Remove mails from vector in case s.th. goes wrong  # and urls cannot be detected mail_bodys \u0026lt;- mail_bodys[!is.na(urls)] mails \u0026lt;- mails[!is.na(urls)] urls \u0026lt;- urls[!is.na(urls)] # For each url request twitter data requests \u0026lt;- map(urls, request_twitter_data, bearer_token = bearer_token) # Use requested twitter data to insert texts into Markdown template  # and write to current working directory replaced_templates_wo_mails \u0026lt;- map(requests, replace_template_placeholder, template = \u0026#39;template.md\u0026#39;) # Now that we have mails, replace that placeholder too replaced_templates \u0026lt;- replaced_templates_wo_mails %\u0026gt;% map2(mail_bodys, ~str_replace(.x, \u0026#39;insert_mail_here\u0026#39; ,.y)) %\u0026gt;% map2(requests, ~write_replaced_text(.x, .y)) # Move markdown files and extracted pngs to correct place on HDD walk2( requests, replaced_templates, move_files, vault_location = vault_location, attachments_dir = attachments_dir ) # Move emails on imap server to Processed directory con$move_msg(mails, to_folder = \u0026#39;Processed\u0026#39;) }   Last Step: Execute R script automatically Alright, alright, alright. We made it. We have successfully\n extracted URLs from mails, created new notes and moved them to their designated place  The only thing that is left to do is execute this script automatically. Again, if you don\u0026rsquo;t want to assemble the R script yourself using the code chunks in this blog post, check out this GitHub gist.\nOn Windows, you can write a VBS script that will execute the R script. Window\u0026rsquo;s task scheduler is easily set up to run that VBS script regularly, say every hour. For completeness' sake let me give you an example VBS script. But beware that I have no frikkin clue how VBS scripts work beyond this simple call.\nSet wshshell = WScript.CreateObject (\u0026quot;wscript.shell\u0026quot;) wshshell.run \u0026quot;\u0026quot;\u0026quot;C:\\Program Files\\R\\R-4.0.5\\bin\\Rscript.exe\u0026quot;\u0026quot; \u0026quot;\u0026quot;D:\\Local R Projects\\Playground\\TwitterTracking\\my_twitter_script.R\u0026quot;\u0026quot;\u0026quot;, 6, True set wshshell = nothing  The idea of this script is to call Rscript.exe and give it the location of the R script that we want to execute. Of course, you will need to adjust the paths to your file system. Notice that there are super many double quotes in this script. This is somewhat dumb but it\u0026rsquo;s the only way I could find to make file paths with white spaces work (see StackOverflow).\nOn Ubuntu (and probably other Unix-based systems), I am sure that every Unix user knows that there is CronTab to schedule regular tasks. On Mac, I am sure there is something. But instead of wandering even further from my expertise, I will refer to your internet search skills.\nMind the possibilities We made it! We connected to Twitter\u0026rsquo;s API and our dummy email to get data viz (what\u0026rsquo;s the plural here? viz, vizz, vizzes, vizzeses?) into our note-taking system. Honestly, I think that was quite an endeavor. But now we can use the same ideas for all kind of other applications! From the top of my head I can think of more scenarios where similar solutions should be manageable. Here are two ideas.\n  Take notes on the fly using emails and automatically incorporate the emails into your note-taking system.\n  Take a photo from a book/text you\u0026rsquo;re reading and send it to another dummy mail. Run a script that puts the photo and the mail directly into your vault.\n  So, enjoy the possibilities! If you liked this blog post, then consider following me on Twitter and/or subscribing to my RSS feed. Until next time!\n","description":"The goal is to send a tweet's URL to a dummy mail account and let R extract it, call the Twitter API and then put everything as a Markdown file into your note-taking system.","id":5,"section":"post","tags":["API","automatization"],"title":"How to collect dataviz from Twitter into your note-taking system","uri":"https://albert-rapp.de/post/2022-04-11-get-twitter-posts-into-your-notetaking-system/"},{"content":"So, I found a great video from Storytelling with Data (SWD). In this video, a data storyteller demonstrates how a dataviz that does not demonstrate a clear story can be improved. Let\u0026rsquo;s take a look at the dataviz but, first, here\u0026rsquo;s the data.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  library(tidyverse) dat \u0026lt;- tibble( id = 1:19, fulfilled = c(803, 865, 795, 683, 566, 586, 510, 436, 418, 364, 379, 372, 374, 278, 286, 327, 225, 222, 200), accuracy = c(86, 80, 84, 82, 86, 80, 80, 93, 88, 87, 85, 85, 83, 94, 86, 78, 89, 88, 91), error = c(10, 14, 10, 14, 10, 16, 15, 6, 11, 7, 12, 13, 8, 4, 12, 12, 7, 10, 7), null = 100 - accuracy - error ) %\u0026gt;% mutate(across(accuracy:null, ~. / 100)) dat ## # A tibble: 19 x 5 ## id fulfilled accuracy error null ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 803 0.86 0.1 0.04 ## 2 2 865 0.8 0.14 0.06 ## 3 3 795 0.84 0.1 0.06 ## 4 4 683 0.82 0.14 0.04 ## 5 5 566 0.86 0.1 0.04 ## 6 6 586 0.8 0.16 0.04 ## 7 7 510 0.8 0.15 0.05 ## 8 8 436 0.93 0.06 0.01 ## 9 9 418 0.88 0.11 0.01 ## 10 10 364 0.87 0.07 0.06 ## 11 11 379 0.85 0.12 0.03 ## 12 12 372 0.85 0.13 0.02 ## 13 13 374 0.83 0.08 0.09 ## 14 14 278 0.94 0.04 0.02 ## 15 15 286 0.86 0.12 0.02 ## 16 16 327 0.78 0.12 0.1  ## 17 17 225 0.89 0.07 0.04 ## 18 18 222 0.88 0.1 0.02 ## 19 19 200 0.91 0.07 0.02   This data set contains a lot of accuracy and error rates from different (anonymous) warehouses. Additionally, there are \u0026ldquo;null rates\u0026rdquo;. These are likely related to data quality issues. Furthermore, this data set is apparently taken from a client the data storytellers helped. In any case, here is a ggplot2 recreation of the client\u0026rsquo;s initial plot. Note that the plot does not match exactly but it\u0026rsquo;s close enough to get the gist.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  theme_set(theme_minimal()) dat_long \u0026lt;- dat %\u0026gt;% pivot_longer( cols = accuracy:null, names_to = \u0026#39;type\u0026#39;, values_to = \u0026#39;percent\u0026#39; ) dat_long %\u0026gt;% ggplot(aes(id, percent, fill = factor(type, levels = c(\u0026#39;null\u0026#39;, \u0026#39;accuracy\u0026#39;, \u0026#39;error\u0026#39;)))) + geom_col() + labs( title = \u0026#39;Warehouse Accuracy Rates\u0026#39;, x = \u0026#39;Warehouse ID\u0026#39;, y = \u0026#39;% of total orders\u0026#39;, fill = element_blank() ) + scale_y_continuous(labels = ~scales::percent(., accuracy = 1), breaks = seq(0, 1, 0.1))   As it is right know, the plot shows data. But what is the message of this dataviz? To make the message more explicit, the plot is transformed during the course of the video. Take a look at what story the exact same data can tell.\nFrom reading the SWD book, I know that some of the techniques that were used in this picture can be used in many settings. Therefore, I decided to document the steps I took to recreate the dataviz with ggplot.\nI tried to make this documentation as accessible as possible. Consequently, if you are already quite familiar with how to customize a ggplot\u0026rsquo;s details, then some of the explanations or references may be superfluous. Feel free to skip them. That being said, let\u0026rsquo;s transform the plot.\nFlip the axes for long names Although it is not really an issue here, warehouses or other places might be more identifiable by a (long) name rather than an ID. To make sure that these names are legible, show them on the y-axes. When I first learned ggplot, there was the layer coord_flip() to do that job for us. Nowadays, though, you can often avoid coord_flip() because a lot of geoms already understand what you mean, when you map categorical data to the y-aesthetic. But make sure that ggplot will know that you mean categorical data (especially if the labels are numerical like here).\n1 2 3 4 5 6 7 8 9 10 11  categorial_dat \u0026lt;- dat_long %\u0026gt;% mutate( id = as.character(id), ) categorial_dat %\u0026gt;% ggplot(aes(x = percent, y = id)) + geom_col( aes(group = factor(type, levels = c(\u0026#39;error\u0026#39;, \u0026#39;null\u0026#39;, \u0026#39;accuracy\u0026#39;))), col = \u0026#39;white\u0026#39;, # set color to distinguish bars better )   Notice that I used the group- instead of fill-aesthetic because I only need grouping. Also, it is always a good idea to avoid excessive use of colors. This will allow us to emphasize parts of our story with colors later on.\nAdd reference points Another good idea it to put your data into perspective. To do so, include a reference point. This can be a summary statistic like the average error rate. For more great demonstration of reference points you can also check out the evolution of a ggplot by CÃ©dric Scherer.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  averages \u0026lt;- dat_long %\u0026gt;% group_by(type) %\u0026gt;% summarise(percent = mean(percent)) %\u0026gt;% mutate(id = \u0026#39;ALL\u0026#39;) dat_with_summary \u0026lt;- categorial_dat %\u0026gt;% bind_rows(averages) dat_with_summary %\u0026gt;% ggplot(aes(x = percent, y = id)) + geom_col( aes(group = factor(type, levels = c(\u0026#39;error\u0026#39;, \u0026#39;null\u0026#39;, \u0026#39;accuracy\u0026#39;))), col = \u0026#39;white\u0026#39;, )   Order your data To allow your reader to gain a quick overview, put your data into some form of sensible ordering. This eases the burden of having to make sense of what the visual shows. Also, notice that we already did part of that. See, with the order of the levels in the group aesthetic, we influenced the ordering of the stacked bars. Here, we made sure that important quantities start at the left resp. right edges.\nWhy is that helpful, you ask? Well, the bars that start on the left all start at the same reference point. Therefore comparisons are quite easy for these bars. The same holds true for the right edge. Consequently, it is best that we reserve these vip seats for the important data. Check out what happens if I were to put the accuracy in the middle.\n1 2 3 4 5 6  dat_with_summary %\u0026gt;% ggplot(aes(x = percent, y = id)) + geom_col( aes(group = factor(type, levels = c(\u0026#39;error\u0026#39;, \u0026#39;accuracy\u0026#39;, \u0026#39;null\u0026#39;))), col = \u0026#39;white\u0026#39;, )   Now, we can\u0026rsquo;t really make out which warehouses have a higher accuracy. Given that the accuracy is likely something we care about, this is bad. But we can change the order even more. For instance, we can also order the bars by error rate. Here, fct_reorder() is our friend.\n1 2 3 4 5 6 7 8 9 10 11 12  ordered_dat \u0026lt;- dat_with_summary %\u0026gt;% mutate( type = factor(type, levels = c(\u0026#39;error\u0026#39;, \u0026#39;null\u0026#39;, \u0026#39;accuracy\u0026#39;)), id = fct_reorder(id, percent, .desc = T) ) ordered_dat %\u0026gt;% ggplot(aes(x = percent, y = id)) + geom_col( aes(group = type), col = \u0026#39;white\u0026#39;, )   Highlight your story points Next, it\u0026rsquo;s time to highlight your story points. This can be done with the gghighlight as I have demonstrated in another blog post. Alternatively, we can set the colors manually. The latter approach gave me the best results in this case, so we\u0026rsquo;ll go with that. But I am still a big fan of gghighlight, so don\u0026rsquo;t discard its power just yet.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # Set colors as variable for easy change later unhighlighted_color \u0026lt;- \u0026#39;grey80\u0026#39; highlighted_color \u0026lt;- \u0026#39;#E69F00\u0026#39; avg_error \u0026lt;- \u0026#39;black\u0026#39; avg_rest \u0026lt;- \u0026#39;grey40\u0026#39; # Compute new column with colors of each bar colored_dat \u0026lt;- ordered_dat %\u0026gt;% mutate( custom_colors = case_when( id == \u0026#39;ALL\u0026#39; \u0026amp; type == \u0026#39;error\u0026#39; ~ avg_error, id == \u0026#39;ALL\u0026#39; ~ avg_rest, type == \u0026#39;error\u0026#39; \u0026amp; percent \u0026gt; 0.1 ~ highlighted_color, T ~ unhighlighted_color ) ) p \u0026lt;- colored_dat %\u0026gt;% ggplot(aes(x = percent, y = id)) + geom_col( aes(group = type), col = \u0026#39;white\u0026#39;, fill = colored_dat$custom_colors # Set colors manually ) p   Notice how your eyes are immediately drawn to the intended region. That\u0026rsquo;s the power of colors! Also, note that setting the colors manually like this worked because fill in geom_col() is vectorized. This is not always the case. In these instances, you may find that functional programming solves your problem.\nRemove axes expansion and allow drawing outside of grid Did you notice that there is still some clutter in the plot? Removing clutter from a plot is a central element of the SWD look. Personally, I like this approach. So, let\u0026rsquo;s get down to the essentials and remove what does not need to be there. In this case, there are still (faint) horizontal lines behind each bar. Furthermore, this causes the warehouse IDs to be slightly removed from the bars. We change that through formatting the coordinate system with coord_cartesian().\n1 2 3 4 5 6 7 8  p \u0026lt;- p + coord_cartesian( xlim = c(0, 1), ylim = c(0.5, 20.5), expand = F, # removes white spaces at edge of plot clip = \u0026#39;off\u0026#39; # allows drawing outside of panel ) p   Here, we turned off the expansion to avoid wasting white space. Now, the IDs are at their designated place and we do not see lines from their names to the bars anymore. If you want even more power on the space expansion you can leave expand = T and modify the expansion for each axis with scale_*_continuous() and the expansion() function. Check out Christian Burkhart\u0026rsquo;s neat cheatsheet that teaches you everything you need to understand expansions.\nOn an unrelated note, you may wonder why I set clip = 'off'. This little secret will be revealed soon. For now, just know that it allows you to draw geoms outside the regular panel.\nMove and format axes You may have noticed that the x-axis in the finished plot is at the top of the panel rather than at the bottom. While that is unusual, it helps the reader to get straight to the point as the data is in view earlier. This assumes that the eyes of a typical dataviz reader will first look at the top left corner and then zigzag downwards.\nIn ggplot2, moving the axes and setting the break points happens in a scale layer. It is here where we use the scales::percent() function to transform the axes labels. Additionally, changing labels happens in labs() and the remaining axes and text changes happen in theme().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  unhighlighed_col_darker \u0026lt;- \u0026#39;grey60\u0026#39; p \u0026lt;- p + scale_x_continuous( breaks = seq(0, 1, 0.2), labels = scales::percent, position = \u0026#39;top\u0026#39; ) + labs( title = \u0026#39;Accuracy rates for highest volume warehouses\u0026#39;, y = \u0026#39;WAREHOUSE ID\u0026#39;, x = \u0026#39;% OF TOTAL ORDERS FULFILLED\u0026#39;, ) + theme( axis.line.x = element_line(colour = unhighlighed_col_darker), axis.ticks.x = element_line(colour = unhighlighed_col_darker), axis.text = element_text(colour = unhighlighed_col_darker), text = element_text(colour = unhighlighed_col_darker), plot.title = element_text(colour = \u0026#39;black\u0026#39;) ) p   Notice that we have customized the theme elements via element_*() functions. Basically, each geom type like \u0026ldquo;line\u0026rdquo;, \u0026ldquo;rect\u0026rdquo;, \u0026ldquo;text\u0026rdquo;, etc. has their own element_*() function. The theme() function expects attributes to be changed using these. If you are unfamiliar with this concept, maybe the corresponding part in my YARDS lecture notes will help you.\nAlign labels Aligning plot elements, e.g. labels, to form clean lines is another major aspect of the SWD look. Before I read about it, I did not even notice it but once you see it you cannot go back. Basically, plots feel \u0026ldquo;more harmonious\u0026rdquo; if there are clear (not necessarily drawn) lines like with the left and right edge of the stacked bars. But this concept does not stop with the bars and can be used for the labels too. Let\u0026rsquo;s demonstrate that by moving the labels with more of theme().\n1 2 3 4 5 6 7 8  p \u0026lt;- p + theme( axis.title.x = element_text(hjust = 0), axis.title.y = element_text(hjust = 1), plot.title.position = \u0026#39;plot\u0026#39; # aligns the title to the whole plot and not the (inner) panel ) p   Once again, the design enforces that important information like what\u0026rsquo;s on an axis is in the top left corner. This was done by changing hjust. In this case hjust = 0 corresponds to left-justified whereas hjust = 1 corresponds to right-justified. Of course, vjust works similarly. For more details w.r.t. hjust and vjust, check out this stackoverflow answer that gives you everything that you need in one visual. For your convenience, here is a slightly changed form of that visual.\nBut once you start aligning the axes titles, you notice that the 0% and 100% labels fall outside the grid. We could try to set hjust of axis.text.x in theme() but sadly this is not vectorized. Subsequently, all hjust values must be the same. That\u0026rsquo;s not bueno. Therefore, I drew the axes labels manually with annotate() but make sure that you remove the current labels in scale_x_continuous(). Also, now you know why we had to set clip = 'off' earlier. The axes labels are outside of the regular panel.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  p \u0026lt;- p + # Overwriting previous scale will generate a warning but that\u0026#39;s ok scale_x_continuous( breaks = seq(0, 1, 0.2), # We still want the axes ticks labels = rep(\u0026#39;\u0026#39;, 6), # Empty strings as labels position = \u0026#39;top\u0026#39; ) + annotate( \u0026#39;text\u0026#39;, x = seq(0, 1, 0.2), y = 20.75, label = scales::percent(seq(0, 1, 0.2), accuracy = 1), size = 3, hjust = c(0, rep(0.5, 4), 1), # individual hjust here vjust = 0, col = unhighlighed_col_darker ) + theme( axis.title.x = element_text(hjust = 0, vjust = 0) # change vjust to avoid overplotting ) ## Scale for \u0026#39;x\u0026#39; is already present. Adding another scale for \u0026#39;x\u0026#39;, which will ## replace the existing scale. p   Add text labels The same trick can be used to add the category description (accuracy, null, error) to the right top corner and label the highlighted bars. For the latter part, we simply extract the corresponding rows from our data and use that in conjunction with geom_text().\n1 2 3 4 5 6 7 8 9 10 11 12 13  text_labels \u0026lt;- colored_dat %\u0026gt;% filter(type == \u0026#39;error\u0026#39;, percent \u0026gt; 0.1) %\u0026gt;% mutate(percent = scales::percent(percent, accuracy = 1)) p \u0026lt;- p + geom_text( data = text_labels, aes(x = 1, label = percent), hjust = 1.1, col = \u0026#39;white\u0026#39;, size = 4 ) p   Notice that I used a hjust value greater than 1 here to add some white space on the right side of the labels. Otherwise, the percent sign will be too close to the bar\u0026rsquo;s edge.\nNext, we add the category descriptions. This is a bit more tricky, though, because we want to highlight a word too, So, we will add a richtext as described in my previous blog post.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  library(ggtext) p \u0026lt;- p + annotate( \u0026#39;richtext\u0026#39;, x = 1, y = 21.25, label = \u0026#34;ACCURATE | NULL | \u0026lt;span style = \u0026#39;color:#E69F00\u0026#39;\u0026gt;ERROR\u0026lt;/span\u0026gt;\u0026#34;, hjust = 1, vjust = 0, col = unhighlighed_col_darker, size = 4, label.colour = NA, fill = NA ) p   Add story text Now that the bar plot is finished we can work on the story text. For that, we create another plot that contains only the text. Later on, we will combine both of our plots with the patchwork package. There are no really knew techniques here, so let\u0026rsquo;s get straight to the code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  # Save text data in a tibble tib_summary_text \u0026lt;- tibble( x = 0, y = c(1.65, 0.5), label = c(\u0026#34;\u0026lt;span style = \u0026#39;color:grey60\u0026#39;\u0026gt;OVERALL:\u0026lt;/span\u0026gt; **The error rate is 10% across all\u0026lt;br\u0026gt;66 warehouses**. \u0026lt;span style = \u0026#39;color:grey60\u0026#39;\u0026gt;The good news is that\u0026lt;br\u0026gt;the accuracy rate is 85% so we\\\u0026#39;re hitting\u0026lt;br\u0026gt;the mark in nearly all our centers due to\u0026lt;br\u0026gt;the quality initiatives implemented last year.\u0026lt;/span\u0026gt;\u0026#34;, \u0026#34;\u0026lt;span style = \u0026#39;color:#E69F00\u0026#39;\u0026gt;OPPORTUNITY TO IMPROVE:\u0026lt;/span\u0026gt; \u0026lt;span style = \u0026#39;color:grey60\u0026#39;\u0026gt;10 centers\u0026lt;br\u0026gt;have higher than average error rates of\u0026lt;br\u0026gt;10%-16%.\u0026lt;/span\u0026gt; \u0026lt;span style = \u0026#39;color:#E69F00\u0026#39;\u0026gt;We recommend investigating\u0026lt;br\u0026gt;specific details and **scheduling meetings\u0026lt;br\u0026gt;with operations managers to\u0026lt;br\u0026gt;determine what\u0026#39;s driving this.**\u0026lt;/span\u0026gt;\u0026#34; ) ) # Create text plot with geom_richtext() and theme_void() text_plot \u0026lt;- tib_summary_text %\u0026gt;% ggplot() + geom_richtext( aes(x, y, label = label), size = 3, hjust = 0, vjust = 0, label.colour = NA ) + coord_cartesian(xlim = c(0, 1), ylim = c(0, 2), clip = \u0026#39;off\u0026#39;) + # clip = \u0026#39;off\u0026#39; is important for putting it together later. theme_void() text_plot   Add main message as new title and subtitle As I said before, we will put the two plots together with patchwork. If you have never dealt with patchwork, feel free to check out my short intro to patchwork. Putting the plots together gives us another opportunity: We can now set additional titles and subtitles of the whole plot. Use these to add the main message of your plot.\nBut make sure that there is enough white space around them by setting the title margins in theme(). Otherwise, your plot will feel \u0026ldquo;too full\u0026rdquo;. Adding spacing is achieved through a margin() function in element_text(). Though, in this case we use element_markdown() which works exactly the same but enables Markdown syntax like using asterisks for bold texts.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  # Save texts as variables for better code legibility # Here I used Markdown syntax # To enable its rendering, use element_markdown() in theme title_text \u0026lt;- \u0026#34;**Action needed:** 10 warehouses have \u0026lt;span style = \u0026#39;color:#E69F00\u0026#39;\u0026gt;high error rates\u0026lt;/span\u0026gt;\u0026#34; subtitle_text \u0026lt;- \u0026#34;\u0026lt;span style = \u0026#39;color:#E69F00\u0026#39;\u0026gt;DISCUSS:\u0026lt;/span\u0026gt; what are \u0026lt;span style = \u0026#39;color:#E69F00\u0026#39;\u0026gt;**next steps to improve errors**\u0026lt;/span\u0026gt; at highest volume warehouses?\u0026lt;br\u0026gt;\u0026lt;span style = \u0026#39;font-size:10pt;color:grey60\u0026#39;\u0026gt;The subset of centers shown (19 out of 66) have the highest volume of orders fulfilled\u0026lt;/span\u0026gt;\u0026#34; caption_text \u0026lt;- \u0026#34;SOURCE: ProTip Dashboard as of Q4/2021. See file xxx for additional context on remaining 47 warehouses\u0026lt;br\u0026gt;\u0026lt;span style = \u0026#39;font-size:6pt;color:grey60\u0026#39;\u0026gt;Original: Storytelling with Data - improve this graph! exercise | {ggplot2} remake by Albert Rapp (@rappa753).\u0026#34; # Compose plot library(patchwork) p + text_plot + # Make text plot narrower plot_layout(widths = c(0.6, 0.4)) + # Add main message via title and subtitle plot_annotation( title = title_text, subtitle = subtitle_text, caption = caption_text, theme = theme( plot.title = element_markdown( margin = margin(b = 0.4, unit = \u0026#39;cm\u0026#39;), # 0.4cm margin at bottom of title size = 16 ), plot.subtitle = element_markdown( margin = margin(b = 0.4, unit = \u0026#39;cm\u0026#39;), # 0.4cm margin at bottom of title size = 11.5 ), plot.caption.position = \u0026#39;plot\u0026#39;, plot.caption = element_markdown( hjust = 0, size = 7, colour = unhighlighed_col_darker, lineheight = 1.25 ), plot.background = element_rect(fill = \u0026#39;white\u0026#39;, colour = NA) # This is only a trick to make sure that background really is white # Otherwise, some browsers or photo apps will apply a dark mode ) )   Get the sizes right In the last plot, I cheated. I gave you the correct code I used to generate the picture. But I did not execute it. Instead, I only displayed the code and then showed you the (imported) picture from the start of this blog post. Why did I do this? Because getting the sizes right sucks!\nIf you have dealt with ggplot enough, then you will know that text sizes are often set in absolute rather than in relative terms. Therefore, if you make the bar plot smaller in width (like we did), then the bars may be appropriately scaled to the new width but, more often than not, the texts are not. In this case, this led to way too large fonts as beautifully demonstrated in Christophe Nicault\u0026rsquo;s helpful blog post.\nSo, how do you avoid this? First off, choose size and fonts last (choose the font first, though). This will save you a lot of repetitive work when you change the alignment in your plot. But this tip will only get you so far, because you have to fix some sizes in between to get a feeling for the visualization you are trying to create.\nTherefore, try to get you canvas into an appropriate size first. I try to do this by using the camcorder package at the start of my visualization process. This will ensure that my plots are saved as a png-file with predetermined dimensions and the resulting file is displayed in the Viewer pane in RStudio (as opposed to the Plots pane).\nFor example, at the start of working on this visualization I have called\n1 2 3  camcorder::gg_record( dir = \u0026#39;img\u0026#39;, dpi = 300, width = 16, height = 9, units = \u0026#39;cm\u0026#39; )   This made getting the sizes right for my final output somewhat easier because the canvas size remains the same throughout the process. Though be sure to call gg_record() after library(ggtext) or make sure that you call gg_record() again if you add ggtext only later. Otherwise, your plots will revert back to being displayed in the Plots pane (with relative sizing). Finally, if you want to use camcorder in conjunction with showtext, then be sure that showtext will know what dpi value you chose when calling gg_record().\n1  showtext::showtext_opts(dpi = 300)   Alright, that concludes this somewhat long blog post. I hope that you enjoyed it and learned something valuable. If you did, feel free to leave a comment. Also, you can stay in touch with my work by subscribing to my RSS feed or following me on Twitter.\n","description":"We try to imitate the Storytelling with Data look with ggplot","id":6,"section":"post","tags":[],"title":"Recreating the Storytelling with Data look with ggplot","uri":"https://albert-rapp.de/post/2022-03-29-recreating-the-swd-look/"},{"content":"A couple of weeks back, I recreated an info graphic with ggplot2. The result and the whole story is embedded in this thread on Twitter:\nThe fun thing about getting better at #ggplot2 is that you begin to mimick other #dataviz.\nHere is a practice #rstats info graphic I created after seeing a similar infographic from @EatSmarter_de Original graphic, making of, comments and some ressources below â¬ï¸ð§µ pic.twitter.com/FslScy9sc7\n\u0026mdash; Albert Rapp (@rappa753) March 5, 2022  Aside from the embarrasing typo in \u0026ldquo;What you should know\u0026hellip;\u0026rdquo;, I picked up a useful technique for what do when I want aesthetics to vary within a geom. Sounds complicated? Let\u0026rsquo;s take a look at a couple of examples.\nHow do I manually set aesthetics with aes() and scale_*_identity()? This one is the easy case when all geoms behave properly.\n1 2 3 4 5 6 7 8 9 10 11 12  library(tidyverse) theme_set(theme_minimal()) tib \u0026lt;- tribble( ~x, ~xend, ~y, ~yend, ~size_col, 0, 1, 0, 1, 1, 1, 2, 1, 1, 5 ) tib %\u0026gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend, size = size_col)) + geom_segment(col = \u0026#39;dodgerblue4\u0026#39;) + scale_size_identity()   Notice that\n the sizes were determined in the size_col column of tib. sizes were mapped to the aesthethic via aes(). the scale_size_identity() layer makes sure that the sizes are not assigned by ggplot but taken as given (identity scale layers are available for other aesthetics as well).  How do I manually set aesthetics without aes()? The last example used aes() to access size_col from tib. However, we then had to make sure that ggplot does not assign sizes based on unique values in size_col. Instead, sizes were supposed to be taken as is. This was the job of scale_size_identity(). Let\u0026rsquo;s make it work without it.\n1 2 3  tib %\u0026gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(col = \u0026#39;dodgerblue4\u0026#39;, size = tib$size_col)   This will generate the exact same plot as before (which is why I suppressed the output). In this case, we mapped the sizes manually by assigning a vector of sizes to the size aesthetic within geom_segment() but outside aes().\nOf course, now we cannot simply write size = size_col because geom_segment() won\u0026rsquo;t know that variable. Before, aes() let ggplot know that we mean size_col from the data set tib. Now, we have to pass the vector by accessing it from tib ourself through tib$size_col.\nHow do I manually set aesthethics when the previous approaches do not work? Finally, let\u0026rsquo;s switch from geom_segment() to geom_curve().\n1 2 3  tib %\u0026gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_curve(col = \u0026#39;dodgerblue4\u0026#39;, size = tib$size_col, curvature = 0.6)   This changes our straight lines from before to curved lines. What\u0026rsquo;s more, I can control how strong the curvature is supposed to be via curvature. But as it is right now, both of our differently-sized curves have the same level of curvature.\nMaybe, this ought to be different. Maybe, not all curves are made the same. Maybe, our visualization should reflect the diversity of all the curves out there in this gigantic world we inhabit. All curves are beautiful!\nLet\u0026rsquo;s make this happen as we did before.\n1 2 3 4 5 6 7 8 9 10 11 12  tib %\u0026gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_curve( col = \u0026#39;dodgerblue4\u0026#39;, size = tib$size_col, curvature = c(-0.3, 0.6) # two curves, two different curvatures ) ## Warning in if (curvature == 0) {: the condition has length \u0026gt; 1 and only the ## first element will be used ## Warning in if (curvature \u0026gt; 0) hand \u0026lt;- \u0026#34;right\u0026#34; else hand \u0026lt;- \u0026#34;left\u0026#34;: the condition ## has length \u0026gt; 1 and only the first element will be used ## Error in seq.default(0, dir * maxtheta, dir * maxtheta/(ncp + 1)): \u0026#39;to\u0026#39; must be of length 1   Oh no! It seems as if geom_curve() expects the argument of curvature to be a single number. Maybe aes() then?\n1 2 3 4 5 6 7 8  tib %\u0026gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_curve( aes(curvature = c(-0.3, 0.6)), col = \u0026#39;dodgerblue4\u0026#39;, size = tib$size_col ) ## Warning: Ignoring unknown aesthetics: curvature   Well, at least this time we can see curves. Unfortunately, the warning let\u0026rsquo;s us know that curvature is an unknown aesthetic which will be ignored. As you can see, this results in the same curvature for both curves again.\nSo, it looks like we can only hope to set each curvature separately.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ggplot(mapping = aes(x = x, xend = xend, y = y, yend = yend)) + geom_curve( data = slice(tib, 1), # first row of tib col = \u0026#39;dodgerblue4\u0026#39;, size = tib$size_col[1], # one size only curvature = -0.3 ) + geom_curve( data = slice(tib, 2), # second row of tib col = \u0026#39;dodgerblue4\u0026#39;, size = tib$size_col[2], # other size curvature = 0.6 )   Alright, this time we got what we wanted. That\u0026rsquo;s something at least. Honestly, our \u0026ldquo;solution\u0026rdquo; is not scalable though. What if we want to draw hundreds of curves?\nIn fact, this is what slowed me down when I created the info graphic that started this blog post. The text boxes were not vectorized so I would have to place each text box manually. That\u0026rsquo;s a lot of text boxes and I was having none of that.\nSo, here is where functional programming stepped in. Let\u0026rsquo;s recreate what I did based on our curve example. First, we extend tib with another curvature column.\n1 2 3 4 5 6 7  tib \u0026lt;- tib %\u0026gt;% mutate(curvature = c(-0.3, 0.6)) tib ## # A tibble: 2 x 6 ## x xend y yend size_col curvature ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 1 0 1 1 -0.3 ## 2 1 2 1 1 5 0.6   Then, we use pmap() to create a list of curve layers. If you have not used any functional programming before, checkout my YARDS lecture notes on that topic. Basically, what we will do is to apply the geom_curve() function to each row of the tib data. Via ~ (in front of the function) and ..1, ..2, etc. we can then say where to stick in the values from each of tib\u0026rsquo;s columns.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  curve_layers \u0026lt;- tib %\u0026gt;% pmap(~geom_curve( mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4), size = ..5, curvature = ..6, col = \u0026#39;dodgerblue4\u0026#39; )) curve_layers ## [[1]] ## mapping: x = 0, y = 0, xend = 1, yend = 1  ## geom_curve: arrow = NULL, arrow.fill = NULL, curvature = -0.3, angle = 90, ncp = 5, lineend = butt, na.rm = FALSE ## stat_identity: na.rm = FALSE ## position_identity  ##  ## [[2]] ## mapping: x = 1, y = 1, xend = 2, yend = 1  ## geom_curve: arrow = NULL, arrow.fill = NULL, curvature = 0.6, angle = 90, ncp = 5, lineend = butt, na.rm = FALSE ## stat_identity: na.rm = FALSE ## position_identity   Here, we have set the first column of tib (x) to the x-aesthetic within aes. Then, we proceeded similarly for all other columns. This resulted in a list of curve layers.\nThese are useless without a ggplot() head. So, let\u0026rsquo;s complete the plot.\n1 2  ggplot() + curve_layers   Damn, these are some nice functionally created curves. Now, let\u0026rsquo;s put our new technique to a test. Can it handle arbitrarily many curves?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  n_curves \u0026lt;- 50 curve_layers \u0026lt;- tibble( x = runif(n_curves), xend = runif(n_curves), y = runif(n_curves), yend = runif(n_curves), size = runif(n_curves, 0, 2), curvature = runif(n_curves, -1, 1) ) %\u0026gt;% pmap(~geom_curve( mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4), size = ..5, curvature = ..6, col = \u0026#39;dodgerblue4\u0026#39; )) ggplot() + curve_layers   Congratulations! We have successfully created drawings of a toddler. And the even better news is that we can draw as many curves as we want.\nSurprisingly, before I started this blog post, I was not aware that you can simply add lists to ggplot() and it works. As you will see in the Twitter thread on top of this post, I initially thought that one had to combine the list with more functional programming like so.\n1 2 3 4  combine_gg_elements \u0026lt;- function(...) { Reduce(`+`, list(...)) } combine_gg_elements(ggplot(), curve_layers)   This was something I picked up from Hadley Wickham\u0026rsquo;s ggplot2 book but it seems that we don\u0026rsquo;t need that anymore (the combine function, the book is still a great ressource). But I leave this here for completeness' sake. Once again, writing a blog post has taught me stuff I thought I already knew. If you want to watch me learn more stuff or want to learn more ggplot things yourself, feel free to subscribe to my RSS feed or follow me on Twitter.\n","description":"Functional programming is a mighty sword. Today, we use it to avoid tedious repetitions when things go wrong in ggplot.","id":7,"section":"post","tags":[],"title":"How to use functional programming for ggplot","uri":"https://albert-rapp.de/post/2022-03-25-functional-programming-when-geoms-are-not-vectorized/"},{"content":"For some reason, using other than the default font in plots has been a major problem for me in R. Supposedly, one can use the extrafont package to manage all of that but I found it too cumbersome. Instead, I found out that the showtext package can make my life easier.\nEven though working with text in plot is not yet completely free of troubles, showtext has made many things easier. Now, I can finally choose fonts freely and even use icons. This blogposts gives you a how-to so that you can do that too.\nImport and Use Fonts with showtext A great source for fonts is Google\u0026rsquo;s font page. What is great abut this page is that it can display texts in many different fonts.\nFigure 1: Screenshot from fonts.google.com\n\rOnce we found a nice font, we can use its name to make it available within R. This is done with showtext\u0026rsquo;s helpful font_add_google() function. Let\u0026rsquo;s import a couple of random fonts.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # Packages that we will use in this post setwd(here::here(\u0026#39;content/en/post/2022-03-04-fonts-and-icons/\u0026#39;)) library(tidyverse) library(showtext) library(ggtext) library(gghighlight) # Import fonts # First argument = google name,  # Secont name = font name in R font_add_google(\u0026#39;Lora\u0026#39;, \u0026#39;lora\u0026#39;) font_add_google(\u0026#39;Lobster\u0026#39;, \u0026#39;lobster\u0026#39;) font_add_google(\u0026#39;Anton\u0026#39;, \u0026#39;anton\u0026#39;) font_add_google(\u0026#39;Fira Sans\u0026#39;, \u0026#39;firasans\u0026#39;) font_add_google(\u0026#39;Syne Mono\u0026#39;, \u0026#39;syne\u0026#39;) # Important step to enable showtext font rendering! showtext_auto()   Notice that we have also used showtext_auto(). This is necessary for showtext to take over the show. Otherwise, the new fonts would not be usable. Now, let\u0026rsquo;s take a look at our new fonts.\n1 2 3 4 5 6 7 8 9 10 11 12  tib \u0026lt;- tibble( family = c(\u0026#39;firasans\u0026#39;, \u0026#39;lora\u0026#39;, \u0026#39;lobster\u0026#39;, \u0026#39;anton\u0026#39;, \u0026#39;syne\u0026#39;), x = 0, y = seq(0.0, 1, length.out = 5), label = \u0026#39;Showtext shows text. Wow. What an insight.\u0026#39; ) tib %\u0026gt;% ggplot(aes(x, y, label = label)) + geom_text(family = tib$family, size = 13, hjust = 0, col = \u0026#39;dodgerblue4\u0026#39;) + coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) + theme_void()   You may wonder why we have used coord_cartesian() here. We did this in order to ensure that the x-axis is not centered at 0 and our example texts won\u0026rsquo;t be outside of the plot. Personally, I find this somewhat tedious but this can\u0026rsquo;t be helped, I guess. With text elements we always run at the risk of writing outside of the plot area.\nNext, let\u0026rsquo;s make our use of fonts somewhat more practical. In my last blog post, I stressed the use of highlighting a few important things instead of using many colors. Combine this with direct labels instead of a legend and you get this plot I created using the Fira Sans font.\nNow, see what it would look like had I used the Lobster font instead.\nFeels different doesn\u0026rsquo;t it? And this is still different than the Anton font.\nImport and Use Icon Fonts with showtext We can not only use regular text fonts but also icons with showtext. For example, we may want to use one of the free Fontawesome icons. To do so, download the newest version and extract the .otf-files into your working directory. These contain the font information that you need. Importing these (and any other font for that matter) works with font_add() and the path to the .otf-files.\n1 2 3 4 5  # First argument = name in R # Second argument = path to .otf-file font_add(\u0026#39;fa-reg\u0026#39;, \u0026#39;fonts/Font Awesome 6 Free-Regular-400.otf\u0026#39;) font_add(\u0026#39;fa-brands\u0026#39;, \u0026#39;fonts/Font Awesome 6 Brands-Regular-400.otf\u0026#39;) font_add(\u0026#39;fa-solid\u0026#39;, \u0026#39;fonts/Font Awesome 6 Free-Solid-900.otf\u0026#39;)   Now that we imported the fonts, we can use ggtext\u0026rsquo;s geom_richtext() and some HTML wizardry to add icons to our previously imported fonts from Google. But first, what we need is an icon\u0026rsquo;s unicode identifier? Uni-what?\nThe easiest way to find that is to stroll through the Fontawesome icons online. Then, find one that matches the font you want to use, e.g. free and solid. Finally, find it\u0026rsquo;s unicode character in the corresponding popup menu.\nFigure 2: Screenshot from fontawesome.com. Unicode highlighted in yellow.\n\rOnce you got this, you can add \u0026amp;#x in front of the unicode and wrap \u0026lt;span\u0026gt; tags around it. Within these tags, you will have to specify font-family so that the icon is rendered.\n1 2 3 4 5 6 7 8 9 10 11 12  tib \u0026lt;- tibble( family = c(\u0026#39;firasans\u0026#39;, \u0026#39;lora\u0026#39;, \u0026#39;lobster\u0026#39;, \u0026#39;anton\u0026#39;, \u0026#39;syne\u0026#39;), x = 0, y = seq(0.0, 1, length.out = 5), label = \u0026#34;Let\u0026#39;s talk cash \u0026lt;span style=\u0026#39;font-family:fa-solid\u0026#39;\u0026gt;\u0026amp;#xf651;\u0026lt;/span\u0026gt;\u0026#34; ) tib %\u0026gt;% ggplot(aes(x, y, label = label)) + geom_richtext(family = tib$family, size = 16, hjust = 0, col = \u0026#39;dodgerblue4\u0026#39;, label.colour = NA) + coord_cartesian(xlim = c(0, 1), ylim = c(-0.1, 1.1)) + theme_void()   This way, you can also use icons in scatter plots. Though, make sure to set fill=NA if you do not want to have white boxes around the icons.\n1 2 3 4  tibble(x = runif(25), y = runif(25)) %\u0026gt;% ggplot(aes(x, y, label = \u0026#34;\u0026lt;span style=\u0026#39;font-family:fa-solid;\u0026#39;\u0026gt;\u0026amp;#xf651;\u0026lt;/span\u0026gt;\u0026#34;)) + geom_richtext(size = 12, label.colour = NA, fill = NA, col = \u0026#39;dodgerblue4\u0026#39;,) + theme_minimal()   You will notice that using the two previous code chunks will generate a lot of warnings about \u0026ldquo;native encoding\u0026rdquo;. So far, I have always been able to ignore these without any trouble. I really don\u0026rsquo;t know why they appear. And if you know, please let me know in the comments below.\n","description":"This is a short tutorial on how to import fonts and icons in R using the showtext package.","id":8,"section":"post","tags":[],"title":"How to use Fonts and Icons in ggplot","uri":"https://albert-rapp.de/post/2022-03-04-fonts-and-icons/"},{"content":"When creating a plot I frequently catch myself using way too many colors. Thus, I have to remind myself often to keep things simple. Usually, this makes a data visualization way more effective.\nLuckily, I found a neat datawrapper blogpost by Lisa Charlotte Muth that shows us how to reduce the use of colors.\nBut as I was reading the blog post, I found myself wondering how some of the mentioned principles could be implemented in ggplot. Naturally, I began experimenting and created a few example plots using fewer colors. This post will show you how you can do that too.\nPreliminaries For completeness' sake, let me mention the basic settings I will use for all visualizations. Honestly, if you have no idea what happens in the following code chunk, just skip it. More or less, this chunk makes sure that all plots are using theme_minimal() plus a small number of tweaks. These tweaks are\n The use of the Fira Sans font with help from the showtext package. The plot titles are aligned to the left, have some spacing around them and are colored using a color from the Okabe Ito color palette. Ever since I read Fundamentals of Data Visualization by Claus Wilke, I am fond of this color palette as I find the colors nice and apparently it is also color-blind safe.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  library(tidyverse) library(showtext) font_add_google(\u0026#34;Fira Sans\u0026#34;, \u0026#34;firasans\u0026#34;) showtext_auto() theme_customs \u0026lt;- theme( text = element_text(family = \u0026#39;firasans\u0026#39;, size = 16), plot.title.position = \u0026#39;plot\u0026#39;, plot.title = element_text( face = \u0026#39;bold\u0026#39;, colour = thematic::okabe_ito(8)[6], margin = margin(t = 2, r = 0, b = 7, l = 0, unit = \u0026#34;mm\u0026#34;) ), ) theme_set(theme_minimal() + theme_customs)   Show shades, not hues Alright, enough with the preliminaries. Let\u0026rsquo;s count how many different car classes are represented in the mpg dataset from the ggplot2 package. I am sure you have seen the data already when you read this ggplot post. So, no further comment on this data set.\n1 2 3  mpg %\u0026gt;% ggplot(aes(x = year, fill = class)) + geom_bar()   Ugh, this is a colorful mess and sort of reminds me of the gnome rainbow puking gif. Let\u0026rsquo;s reduce the color load by sticking to only three colors. To differentiate between classes we will make some colors more transparent.\nThus, we need to create a new variable in our data set that lumps the classes into three groups (for the colors).\n1 2 3 4 5 6 7 8 9 10  # Group classes into three groups (to reduce colors to 3) dat \u0026lt;- mpg %\u0026gt;% mutate( year = factor(year), class_group = case_when( class %in% c(\u0026#39;2seater\u0026#39;, \u0026#39;compact\u0026#39;, \u0026#39;midsize\u0026#39;) ~ \u0026#34;grp1\u0026#34;, class == \u0026#39;minivan\u0026#39; ~ \u0026#34;grp2\u0026#34;, T ~ \u0026#34;grp3\u0026#34; ) )   Now that this is done, we can map fill to our new class_group variable and the regular class variable to alpha.\n1 2 3 4 5 6 7 8 9 10  shades_plt \u0026lt;- dat %\u0026gt;% ggplot(aes(x = year, fill = class_group, alpha = class)) + geom_bar() + labs( x = \u0026#39;Year\u0026#39;, y = \u0026#39;Counts\u0026#39;, alpha = \u0026#39;Class\u0026#39;, title = \u0026#39;Show shades, not hues\u0026#39; ) shades_plt   For better control of the visuals let us manually create and assign colors and the transparency levels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # Color-blind safe colors colors \u0026lt;- thematic::okabe_ito(3) # Possible levels of transparency (one for each class) alpha_max \u0026lt;- 1 alpha_min \u0026lt;- 0.7 alpha_vals \u0026lt;- c( seq(alpha_max, alpha_min, length.out = 4), seq(alpha_min, alpha_max, length.out = 4)[-1] ) alpha_vals ## [1] 1.0 0.9 0.8 0.7 0.8 0.9 1.0 # Tweak previous plot shades_plt \u0026lt;- shades_plt + scale_fill_manual(values = colors) + scale_alpha_manual(values = alpha_vals) shades_plt   Next, let us consolidate the two legends into one. This can be done via guides(). Here, the fill guide will be set to guide_none() to get rid of the class_group legend.\nAlso, the alpha guide needs to be manually overwritten via override.aes in guide_legend() using the color codes that we saved in the vector colors. This way, the alpha legend will also depict the colors instead of only the transparency level.\n1 2 3 4 5 6 7 8 9  shades_plt \u0026lt;- shades_plt + guides( fill = guide_none(), alpha = guide_legend( override.aes = list(fill = colors[c(1, 1, 1, 2, 3, 3, 3)] ) ) ) shades_plt   Group categories together by color, but keep showing them So, this already looks better. However, adjacent colored blocks now \u0026ldquo;merge\u0026rdquo; into each other. This can make it hard to differentiate between classes.\nTo overcome this issue, add lines between blocks. Luckily, this is spectacularly easy and done by setting the color aesthetic in geom_bar() to white. Here\u0026rsquo;s the complete code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  dat %\u0026gt;% ggplot(aes(x = year, fill = class_group, alpha = class)) + geom_bar(col = \u0026#39;white\u0026#39;) + # Add lines for distinction scale_fill_manual(values = colors) + scale_alpha_manual(values = alpha_vals) + guides( fill = guide_none(), alpha = guide_legend(override.aes = list(fill = colors[c(1, 1, 1, 2, 3, 3, 3)])) ) + labs( x = \u0026#39;Year\u0026#39;, y = \u0026#39;Counts\u0026#39;, alpha = \u0026#39;Class\u0026#39;, title = \u0026#39;Group categories together by color, \\nbut keep showing them\u0026#39; )   Emphasize just one or a few categories Next, let us switch tracks and look at some other kind of data. At Our World in Data you can find a lot of interesting data sets. One of these contains survey information on who Americans spend their time with (in average minutes per day by age). If you download this data set, you can create a plot like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  # Some data wrangling time_data \u0026lt;- read_csv(\u0026#34;time-spent-with-relationships-by-age-us.csv\u0026#34;) %\u0026gt;% rename_with( ~c(\u0026#39;Entitity\u0026#39;, \u0026#39;Code\u0026#39;, \u0026#39;Age\u0026#39;, \u0026#39;alone\u0026#39;, \u0026#39;friends\u0026#39;, \u0026#39;children\u0026#39;, \u0026#39;parents\u0026#39;, \u0026#39;partner\u0026#39;, \u0026#39;coworkers\u0026#39;) ) %\u0026gt;% pivot_longer( cols = alone:coworkers, names_to = \u0026#39;person\u0026#39;, values_to = \u0026#39;minutes\u0026#39; ) %\u0026gt;% janitor::clean_names() %\u0026gt;% filter(age \u0026lt;= 80) # Color-blind safe colors colors \u0026lt;- thematic::okabe_ito(8)[-6] # Line plot p \u0026lt;- time_data %\u0026gt;% ggplot(aes(x = age, y = minutes, col = person)) + geom_line(size = 1.5) + scale_color_manual(values = colors) + coord_cartesian(xlim = c(15, 81), expand = F) + scale_y_continuous(minor_breaks = NULL) + labs(x = \u0026#39;Age (in years)\u0026#39;, y = \u0026#39;Minutes\u0026#39;, col = \u0026#39;Time spent\u0026#39;) p   Once again, we created a plot with loads of color. If this were an interactive plot where we can focus on one line at a time, this would not necessarily be a problem. However, as it is, this is a rather messy spaghetti plot and extracting meaning from it is hard.\nBut if we know what story we want to tell, then we can save this plot by emphasizing only the important parts. This is where the gghighlight package shines. It works by adding a gghighlight() layer to an existing plot with conditions for filtering. All data points that do not fulfill these conditions are greyed out.\n1 2 3 4 5  library(gghighlight) alone_plt \u0026lt;- p + gghighlight(person == \u0026#39;alone\u0026#39;, use_direct_label = F) + labs(title = \u0026#39;Emphasize just one or a few categories\u0026#39;) alone_plt   Finally, we are only one text annotation away from telling a story.\n1 2 3 4 5 6 7 8 9 10 11  alone_plt + annotate( \u0026#39;text\u0026#39;, x = 15, y = 455, label = \u0026#39;We spend a lot of time alone...\u0026#39;, hjust = 0, vjust = 0, family = \u0026#39;firasans\u0026#39;, size = 7 )   Of course, a data set may contain multiple stories that may also need multiple highlights. No problem. With gghighlight() we can combine as many conditions as we like.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  age_40_plt \u0026lt;- p + gghighlight( person %in% c(\u0026#39;alone\u0026#39;, \u0026#39;children\u0026#39;), age \u0026gt;= 38, use_direct_label = F ) + geom_segment(x = 38, xend = 38, y = -Inf, yend = 300, linetype = 2, col = \u0026#39;grey20\u0026#39;) + labs(title = \u0026#39;Emphasize just one or a few categories\u0026#39;) age_40_plt + annotate( \u0026#39;text\u0026#39;, x = 15, y = 403, label = \u0026#39;Around the age of 40, we spend \\nless time with children and \\nmore time alone.\u0026#39;, hjust = 0, vjust = 0, family = \u0026#39;firasans\u0026#39;, lineheight = 0.85, size = 5.5 )   Label directly In all previous plots, we displayed a legend at the side of the plot. However, this requires quite a large amount of space which we can save by direct labeling (either with annotate() for a single label or geom_text() for multiple labels).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  alone_plt + annotate( \u0026#39;text\u0026#39;, x = 15, y = 455, label = \u0026#39;We spend a lot of time alone...\u0026#39;, hjust = 0, vjust = 0, family = \u0026#39;firasans\u0026#39;, size = 7 ) + annotate( \u0026#39;text\u0026#39;, x = 70, y = 420, label = \u0026#39;alone\u0026#39;, hjust = 0, vjust = 0, size = 7, family = \u0026#39;firasans\u0026#39;, color = colors[1] ) + labs(title = \u0026#39;Label directly\u0026#39;) + theme(legend.position = \u0026#39;none\u0026#39;)   This way, we save a lot of space and can give the remaining part of the plot more room. Also, this saves the reader some cognitive effort because one does not have to switch back and forth between legend and actual plot.\nIn this particular case, there is another option for direct labelling. Notice how close the word \u0026lsquo;alone\u0026rsquo; from the original text annotation is to the highlighted line anyway. Therefore, we may as well save us one additional annotation and colorize a single word in the orginal annotation.\nTo do so, the ggtext package and a bit of HTML magic will help us. Basically, what we need is to change the annotation from text geom to richtext geom and create a string that contains the HTML-code for colored text. Here that is \u0026lt;span style = 'color:#E69F00;'\u0026gt;...\u0026lt;/span\u0026gt;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  library(ggtext) color_alone \u0026lt;- glue::glue( \u0026#34;We spend a lot of time \u0026lt;span style = \u0026#39;color:{colors[1]};\u0026#39;\u0026gt;alone\u0026lt;/span\u0026gt;...\u0026#34; ) color_alone ## We spend a lot of time \u0026lt;span style = \u0026#39;color:#E69F00;\u0026#39;\u0026gt;alone\u0026lt;/span\u0026gt;... alone_plt + labs(title = \u0026#39;Label directly\u0026#39;) + annotate( \u0026#39;richtext\u0026#39;, x = 15, y = 400, label = color_alone, hjust = 0, vjust = 0, family = \u0026#39;firasans\u0026#39;, size = 7, label.color = NA ) + theme(legend.position = \u0026#39;none\u0026#39;)   Naturally, we can do this for our second highlighted plot as well. In this case, the colored key words are not adjacent to the actual lines.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  age_40_text \u0026lt;- glue::glue( \u0026#34;Around the age of 40, we spent less \u0026lt;br\u0026gt; time with \u0026lt;span style = \u0026#39;color:{colors[2]};\u0026#39;\u0026gt;children\u0026lt;/span\u0026gt; and more \u0026lt;br\u0026gt; time \u0026lt;span style = \u0026#39;color:{colors[1]};\u0026#39;\u0026gt;alone\u0026lt;/span\u0026gt;.\u0026#34; ) age_40_plt + labs(title = \u0026#39;Label directly\u0026#39;) + annotate( \u0026#39;richtext\u0026#39;, x = 15, y = 400, label = age_40_text, hjust = 0, vjust = 0, family = \u0026#39;firasans\u0026#39;, lineheight = 1.25, size = 5.5, label.color = NA ) + theme(legend.position = \u0026#39;none\u0026#39;)   Consequently, the reader may have to go back and forth between text and lines again but still we used our space more efficiently. So, I will let this count as direct labeling.\nFinally, let us come full circle and return to our initial bar plot. This one could also use some direct labels. Normally, I would simply add a geom_text() layer together with position_stack() to the initial plot as described here.\nBut for some magical reason, this did not align the labels properly and it was driving me crazy. Therefore, I counted the car classes and computed the label positions manually.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  manual_counts \u0026lt;- mpg %\u0026gt;% count(year, class) %\u0026gt;% mutate( year = factor(year), class_group = case_when( class %in% c(\u0026#39;2seater\u0026#39;, \u0026#39;compact\u0026#39;, \u0026#39;midsize\u0026#39;) ~ \u0026#34;grp1\u0026#34;, class == \u0026#39;minivan\u0026#39; ~ \u0026#34;grp2\u0026#34;, T ~ \u0026#34;grp3\u0026#34; ) ) labels \u0026lt;- manual_counts %\u0026gt;% mutate(class = factor(class)) %\u0026gt;% group_by(year) %\u0026gt;% arrange(year, desc(class)) %\u0026gt;% mutate( csum = cumsum(n), n = (lag(csum, default = 0) + csum) / 2 )   But once this small detour is overcome, we can label the plot in the same manner as before. Unfortunately, the 2seater class is so small that the label wouldn\u0026rsquo;t fit into the box. Therefore, I decided to plot the label on top.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  manual_counts %\u0026gt;% ggplot(aes(x = year, y = n, fill = class_group)) + geom_col(aes(alpha = class), col = \u0026#39;white\u0026#39;) + scale_fill_manual(values = colors) + scale_alpha_manual(values = alpha_vals) + labs( x = \u0026#39;Year\u0026#39;, y = \u0026#39;Counts\u0026#39;, alpha = \u0026#39;Class\u0026#39;, title = \u0026#39;Label directly\u0026#39; ) + # Add all but one label geom_text( data = labels %\u0026gt;% filter(class != \u0026#39;2seater\u0026#39;), aes(label = class), col = \u0026#39;white\u0026#39;, family = \u0026#39;firasans\u0026#39;, size = 5, fontface = \u0026#39;bold\u0026#39; ) + # Add 2seater label geom_text( data = labels %\u0026gt;% filter(class == \u0026#39;2seater\u0026#39;), aes(y = n + 3, label = class), col = \u0026#39;black\u0026#39;, family = \u0026#39;firasans\u0026#39;, size = 5, fontface = \u0026#39;bold\u0026#39; ) + theme(legend.position = \u0026#39;none\u0026#39;)   Closing remarks The blog post that inspired this post contains a few more tips like using other indicators than color and you should definitely check it out. Also, Lisa Muth apparently writes a book on colors in data visualizations and documents her thoughts here. If you look for more content on colors, this might be a fountain of information.\nAs for using patterns instead of colors, I recently wrote a blog post that leverages the ggpattern package to do just that. Check it out here. And as always, if you don\u0026rsquo;t want to miss new blog post, either follow me on Twitter or via my RSS feed.\n","description":"Inspired by a datawrapper blogpost, we explore how to work with fewer colors in ggplot.","id":9,"section":"post","tags":[],"title":"4 Ways to use colors in ggplot more efficiently","uri":"https://albert-rapp.de/post/2022-02-19-ggplot2-color-tips-from-datawrapper/"},{"content":"I\u0026rsquo;ve been reading Mastering Shiny by Hadley Wickham lately and one of the things that intrigued me is that you can make ggplots interactive. Though I believe that there are limitation to the level of interactiveness compared to using, say, plotly, I really wanted to practice interactive ggplots with Shiny. Naturally, I build a Shiny app to figure things out. Here\u0026rsquo;s a demonstration of what the app can do. The rest of this chapter teaches you how some parts of the app were implemented.\nWorking with clicks If you have build at least one Shiny app, then you are probably aware that you can include plots on the UI with plotOutput(). (If, in fact, you have never touched Shiny, then feel free to check out how I thought my students the basics of Shiny.) But what you may not know, is that apart from its outputId, width and height arguments, this output function also uses arguments like click and dblclick. These are the secrets to unlocking interactiveness.\nImagine that you have a user interface that includes a plot output via\n1  plotOutput(\u0026#39;awesome_plot\u0026#39;, click = \u0026#39;awesome_click\u0026#39;)   Now, what this small additional argument gives you is a way to access the coordinates of something the user clicks on. What you will have to do is to observe input$awesome_click. Here\u0026rsquo;s a minimal example of how that works.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  library(shiny) library(ggplot2) library(dplyr) ui \u0026lt;- fluidPage( plotOutput(\u0026#39;awesome_plot\u0026#39;, click = \u0026#39;awesome_click\u0026#39;), ) server \u0026lt;- function(input, output, session) { # Create dummy data as reactive value my_dat \u0026lt;- reactiveVal(tibble(x = 3, y = 4, msg = \u0026#39;Click me\u0026#39;)) # Render plot with fixed coordinate system output$awesome_plot \u0026lt;- renderPlot({ ggplot(data = my_dat()) + geom_text(aes(x, y, label = msg), size = 15, vjust = 0, hjust = 0) + coord_cartesian(xlim = c(0, 7), ylim = c(0, 8)) }) # Update dummy data on click  observeEvent(input$awesome_click, { my_dat( my_dat() %\u0026gt;% mutate( x = input$awesome_click$x, y = input$awesome_click$y, msg = if (runif(1) \u0026lt; 0.5) \u0026#39;I like that. Do it again.\u0026#39; else \u0026#39;Stop that!\u0026#39; ) ) }) } shinyApp(ui, server)   This will give us the following app.\nLike a cat, this app is a master of mixed signals and wants to be touched but only a random amount of times. Unlike a cat, the app will show you a plot displaying its latest message at the most recently clicked spot.\nAll of this is powered by observing changes in input$awesome_click and then using this list\u0026rsquo;s new x- and y-values to update the reactive value my_dat that underlies the plot. Notice that I have fixed the axes of the plot because otherwise the message will always be displayed in the middle of the plot. After all, the plot will be entirely rebuilt using new underlying data. Fundamentally, this is how I build the \u0026lsquo;color my voronoi\u0026rsquo; from above.\nBut, of course, I have tried out more stuff like user feedback and even some javascript magic. Stick around if you want to learn these ancient skills as well. Destiny is calling.\nLet your user know that he messed up and stop him before it\u0026rsquo;s too late To my surprise, UI elements like numericInput() do not actually check that an input is valid even though there are arguments like min and max. Of course, a user may end up giving wrong inputs that your app can\u0026rsquo;t handle. We can\u0026rsquo;t have that now, can we?\nWe will need to stop that insubordinate and churlish behavior immediately. In case you recognized that combination of \u0026lsquo;insubordinate\u0026rsquo; and \u0026lsquo;churlish\u0026rsquo;, then I will have you now, yes, this is a reference to Mr. Garvey and the rest of this section is a homage to a skit that makes me giggle every time.\nSo, let\u0026rsquo;s build an app that works as follows:\nThe notifications in this app are all powered through the shinyFeedback package. In order to activate its powers, drop a shinyFeedback::useShinyFeedback() in the UI like so.\n1 2 3 4 5 6 7 8 9 10 11  library(shiny) library(dplyr) ui \u0026lt;- fluidPage( shinyFeedback::useShinyFeedback(), h3(\u0026#39;A Day with Mr. Garvey\u0026#39;), textInput( \u0026#39;name\u0026#39;, \u0026#39;What\\\u0026#39;s your name?\u0026#39;, ) )   Then, you are all set up to activate warnings and notifications by your server function. Here is a simplified version of the app\u0026rsquo;s remaining code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  names \u0026lt;- c(\u0026#39;Jay Quellin\u0026#39;,\u0026#39;Jacqueline\u0026#39;, \u0026#39;Balakay\u0026#39;, \u0026#39;Blake\u0026#39;, \u0026#39;Dee-nice\u0026#39;, \u0026#39;Denise\u0026#39;, \u0026#39;Ay-Ay-Ron\u0026#39;, \u0026#39;Aaron\u0026#39;) return_msg \u0026lt;- function(name) { case_when( name == \u0026#39;Balakay\u0026#39; ~ \u0026#39;My name is Blake.\u0026#39;, name == \u0026#39;Blake\u0026#39; ~ \u0026#39;Do you wanna go to war, Balakay? You better check yourself!\u0026#39;, name == \u0026#39;Jay Quellin\u0026#39; ~ \u0026#39;Do you mean Jacqueline?\u0026#39;, name == \u0026#39;Jacqueline\u0026#39; ~ \u0026#39;So that\\\u0026#39;s how it\\\u0026#39;s going to be. I got my eye on you Jay Quellin!\u0026#39;, name == \u0026#39;Dee-nice\u0026#39; ~ \u0026#39;Do you mean Denise?\u0026#39;, name == \u0026#39;Denise\u0026#39; ~ \u0026#39;You say your name right!\u0026#39;, name == \u0026#39;Ay-Ay-Ron\u0026#39; ~ \u0026#39;It is pronounced Aaron.\u0026#39;, name == \u0026#39;Aaron\u0026#39; ~ \u0026#39;You done messed up Ay-Ay-Ron!\u0026#39; ) } server \u0026lt;- function(input, output, session) { name_input \u0026lt;- reactive(input$name) observeEvent(name_input(), { shinyFeedback::feedbackDanger( \u0026#39;name\u0026#39;, show = (name_input() %in% names), text = return_msg(name_input()) ) shinyFeedback::feedbackSuccess( \u0026#39;name\u0026#39;, show = !(name_input() %in% names), text = \u0026#39;Thank you!\u0026#39; ) }) } shinyApp(ui, server)   As you can see, the feedback functions work with\n the name of an input, a rule when to show up and a text to display.  This code is pretty straightforward but, unfortunately, this app does not work like the one you have seen above. There are a couple of problems:\n For starters, if you start the app, then the text input is empty, so !(name_input() %in% names) is true and the app will immediately display \u0026ldquo;Thank you!\u0026rdquo; After you write a name on the list of defined names, then no warning will be displayed. That is because the later feedbackSuccess() will always command that there is nothing to display.  Therefore, we need something that stops the feedbackSuccess() from executing when we don\u0026rsquo;t need it. This can be achieved through the little but powerful req() function. It checks that all given conditions are met or stops the execution where it is. In this case, you will need to drop req(name_input(), !(name_input() %in% names)) in front of feedbackSuccess().\nSmall technical detail: Notice that name_input() will be '' in the beginning. Technically, this is not a boolean but that doesn\u0026rsquo;t matter to Shiny. What matters is that '' is not \u0026ldquo;truthy\u0026rdquo;. See ?isTruthy for more details.\nNow, even with this small change. Our app won\u0026rsquo;t run smoothly because sometimes the notifaction will not change from \u0026ldquo;success\u0026rdquo; to \u0026ldquo;danger\u0026rdquo;. This is is because sometimes the notifaction needs to be reseted to work with new notifications. Therefore, a hideFeedback() is in order.\nAlso, if you are not fast at typing, then a notification might already show up, when you are still typing. It is rude to interrupt our kind user like this. Therefore, let\u0026rsquo;s make sure our app waits a little before giving out notifications. We can let out app wait for a defined amount of milliseconds by sending our reactive name_input() to debounce(). In total, our server function now looks like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  server \u0026lt;- function(input, output, session) { name_input \u0026lt;- reactive(input$name) %\u0026gt;% debounce(250) observeEvent(name_input(), { shinyFeedback::hideFeedback(\u0026#39;name\u0026#39;) shinyFeedback::feedbackDanger( \u0026#39;name\u0026#39;, show = (name_input() %in% names), text = return_msg(name_input()) ) req(name_input(), !(name_input() %in% names)) shinyFeedback::feedbackSuccess( \u0026#39;name\u0026#39;, show = !(name_input() %in% names), text = \u0026#39;Thank you!\u0026#39; ) }) }   Finally, let me mention that, within the function req(), it is also possible to set cancelOutput = TRUE. This stops the code execution as usual but avoids destroying previously displayed outputs.\nSprinkle some javascript magic on top of your app For my final trick before I disappear into the ether, let me show you a little bit of javascript. As I, myself, do not know much about JS, I am particularly proud that I included some of that web magic into my voronoi coloring app. \u0026lsquo;What did you do?', you ask? Well, did you notice that the colour dropdown menu in the voronoi app contains the actual colors next to the color names? That is some JS magic right there! Impressive, I know.\nTo make that work, I had to use the options argument of selectizeInput() together with the render() function and some actual JS code. The whole thing is adapted from this SO post and looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  col2hex \u0026lt;- gplots::col2hex colorValues \u0026lt;- colors() colorNames \u0026lt;- glue::glue(\u0026#34;{colorValues} \u0026lt;span style=\u0026#39;background-color:{col2hex(colorValues)}\u0026#39;\u0026gt;{rep(\u0026#39;\u0026amp;nbsp;\u0026#39;, 15) %\u0026gt;% stringr::str_c(collapse = \u0026#39;\u0026#39;)}\u0026lt;/span\u0026gt;\u0026#34;) colors \u0026lt;- setNames(colorValues, colorNames) js_render_string \u0026lt;- I(\u0026#34; { item: function(item, escape) { return \u0026#39;\u0026lt;div\u0026gt;\u0026#39; + item.label + \u0026#39;\u0026lt;/div\u0026gt;\u0026#39;; }, option: function(item, escape) { return \u0026#39;\u0026lt;div\u0026gt;\u0026#39; + item.label + \u0026#39;\u0026lt;/div\u0026gt;\u0026#39;; } }\u0026#34;) selectizeInput( \u0026#34;color\u0026#34;, \u0026#34;Colour\u0026#34;, selected = \u0026#39;grey80\u0026#39;, choices = colors, options = list(render = js_render_string) )   Let\u0026rsquo;s untangle this step by step. The first part of this code gives us a vector colors containing the color names like \u0026ldquo;white\u0026rdquo; and \u0026ldquo;aliceblue\u0026rdquo; as values. The same vector also uses names for the vector elements that will be displayed to the user. In principal, this colors vector looks like this:\n1 2 3 4 5 6 7 8 9 10  x \u0026lt;- c(\u0026#39;white\u0026#39;, \u0026#39;aliceblue\u0026#39;) # no names x ## [1] \u0026#34;white\u0026#34; \u0026#34;aliceblue\u0026#34; x \u0026lt;- setNames(x, c(\u0026#39;name1\u0026#39;, \u0026#39;name2\u0026#39;)) # with names x ## name1 name2  ## \u0026#34;white\u0026#34; \u0026#34;aliceblue\u0026#34; x[\u0026#39;name1\u0026#39;] # named vectors can be used like dictionaries ## name1  ## \u0026#34;white\u0026#34;   In our color example, instead of using arbitrary names, I converted the color names to their hexvalues like #FFFFFF and wrapped those in some HTML code that could potentially look like \u0026quot;\u0026lt;span style='background-color#FFFFFF'\u0026gt;white\u0026lt;/span\u0026gt;\u0026quot;. This corresponds to the word white with background color #FFFFFF (also white - unspectacular).\nBut in the actual app I wanted to have colored bars next to the color names. Thus, I have used the HTML code for white space \u0026amp;nbsp; and made this into \u0026quot;white \u0026lt;span style='background-color#FFFFFF'\u0026gt;\u0026amp;nbsp;\u0026lt;/span\u0026gt;\u0026quot;. Now, to make that color bar longer, I repeated whited space with rep() and glued those into a single string using stringr::str_c(). This is what the vector looks like if I only use two white space repeats.\n1 2 3 4 5 6 7 8 9 10 11  library(dplyr, warn.conflicts = F) col2hex \u0026lt;- gplots::col2hex colorValues \u0026lt;- colors() colorNames \u0026lt;- glue::glue(\u0026#34;{colorValues} \u0026lt;span style=\u0026#39;background-color:{col2hex(colorValues)}\u0026#39;\u0026gt;{rep(\u0026#39;\u0026amp;nbsp;\u0026#39;, 2) %\u0026gt;% stringr::str_c(collapse = \u0026#39;\u0026#39;)}\u0026lt;/span\u0026gt;\u0026#34;) colors \u0026lt;- setNames(colorValues, colorNames) colors[1] ## white \u0026lt;span style=\u0026#39;background-color:#FFFFFF\u0026#39;\u0026gt;\u0026amp;nbsp;\u0026amp;nbsp;\u0026lt;/span\u0026gt;  ## \u0026#34;white\u0026#34; colors[2] ## aliceblue \u0026lt;span style=\u0026#39;background-color:#F0F8FF\u0026#39;\u0026gt;\u0026amp;nbsp;\u0026amp;nbsp;\u0026lt;/span\u0026gt;  ## \u0026#34;aliceblue\u0026#34;   In the dropdown menu of the app the user will see the names of the color vector, i.e. the HTML code and within the server function of our app the selection will then correspond to the actual value of the vector, i.e. the color name without the html stuff.\nIn our dummy example from above, the user would see name1 and name2 in the dropdown menu but within the server function a user\u0026rsquo;s selection would correspond to input$color which would evaluate to white or aliceblue.\nClearly, we don\u0026rsquo;t want the user to see the raw HTML code. This is where JS comes into play. The code that is stored in js_render_string evaluates the HTML code in order to display the actual colors instead of the raw code. Finally, to execute the JS code we need to pass it to the options of selectizeInput via options = list(render = js_render_string).\nThere you go, this is how I created the color bars in my app using a JS snippet I found on Stackoverflow. You can find the complete codes of the apps we\u0026rsquo;ve build here (click app), here (notification names app) and here (voronoi coloring app). If you liked this post and want to see more Shiny posts, let me know in the comments or simply hit the applause button below. Of course, you can also always follow my work via Twitter.\n","description":"Here's how I turned a ggplot interactive using Shiny and what else I learned while building that app","id":10,"section":"post","tags":["shiny"],"title":"Interactive ggplots, user feedback, and a little bit of javascript magic with Shiny","uri":"https://albert-rapp.de/post/2022-01-17-drawing-a-ggplot-interactively/"},{"content":"The janitor package contains only a little number of functions but nevertheless it is surprisingly convenient. I never really fully appreciated its functionality until I took a look into the documentation. Of course, other packages can achieve the same thing too but janitor makes a lot of tasks easy. Thus, here is a little showcase.\nClean column names As everyone working with data knows, data sets rarely come in a clean format. Often, the necessary cleaning process already starts with the column names. Here, take this data set from TidyTuesday, week 41.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  nurses \u0026lt;- readr::read_csv(\u0026#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-05/nurses.csv\u0026#39;) names(nurses) ## [1] \u0026#34;State\u0026#34;  ## [2] \u0026#34;Year\u0026#34;  ## [3] \u0026#34;Total Employed RN\u0026#34;  ## [4] \u0026#34;Employed Standard Error (%)\u0026#34;  ## [5] \u0026#34;Hourly Wage Avg\u0026#34;  ## [6] \u0026#34;Hourly Wage Median\u0026#34;  ## [7] \u0026#34;Annual Salary Avg\u0026#34;  ## [8] \u0026#34;Annual Salary Median\u0026#34;  ## [9] \u0026#34;Wage/Salary standard error (%)\u0026#34;  ## [10] \u0026#34;Hourly 10th Percentile\u0026#34;  ## [11] \u0026#34;Hourly 25th Percentile\u0026#34;  ## [12] \u0026#34;Hourly 75th Percentile\u0026#34;  ## [13] \u0026#34;Hourly 90th Percentile\u0026#34;  ## [14] \u0026#34;Annual 10th Percentile\u0026#34;  ## [15] \u0026#34;Annual 25th Percentile\u0026#34;  ## [16] \u0026#34;Annual 75th Percentile\u0026#34;  ## [17] \u0026#34;Annual 90th Percentile\u0026#34;  ## [18] \u0026#34;Location Quotient\u0026#34;  ## [19] \u0026#34;Total Employed (National)_Aggregate\u0026#34;  ## [20] \u0026#34;Total Employed (Healthcare, National)_Aggregate\u0026#34; ## [21] \u0026#34;Total Employed (Healthcare, State)_Aggregate\u0026#34;  ## [22] \u0026#34;Yearly Total Employed (State)_Aggregate\u0026#34;   These column names are intuitively easy to understand but not necessarily easy to process by code as there are white spaces and other special characters. Therefore, I accompany most data input by clean_names() from the janitor package.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  library(janitor) library(dplyr) # load for pipe %\u0026gt;% and later wrangling names(nurses %\u0026gt;% clean_names) ## [1] \u0026#34;state\u0026#34;  ## [2] \u0026#34;year\u0026#34;  ## [3] \u0026#34;total_employed_rn\u0026#34;  ## [4] \u0026#34;employed_standard_error_percent\u0026#34;  ## [5] \u0026#34;hourly_wage_avg\u0026#34;  ## [6] \u0026#34;hourly_wage_median\u0026#34;  ## [7] \u0026#34;annual_salary_avg\u0026#34;  ## [8] \u0026#34;annual_salary_median\u0026#34;  ## [9] \u0026#34;wage_salary_standard_error_percent\u0026#34;  ## [10] \u0026#34;hourly_10th_percentile\u0026#34;  ## [11] \u0026#34;hourly_25th_percentile\u0026#34;  ## [12] \u0026#34;hourly_75th_percentile\u0026#34;  ## [13] \u0026#34;hourly_90th_percentile\u0026#34;  ## [14] \u0026#34;annual_10th_percentile\u0026#34;  ## [15] \u0026#34;annual_25th_percentile\u0026#34;  ## [16] \u0026#34;annual_75th_percentile\u0026#34;  ## [17] \u0026#34;annual_90th_percentile\u0026#34;  ## [18] \u0026#34;location_quotient\u0026#34;  ## [19] \u0026#34;total_employed_national_aggregate\u0026#34;  ## [20] \u0026#34;total_employed_healthcare_national_aggregate\u0026#34; ## [21] \u0026#34;total_employed_healthcare_state_aggregate\u0026#34;  ## [22] \u0026#34;yearly_total_employed_state_aggregate\u0026#34;   Did you see what happened? White spaces were converted to _ and parantheses were removed. Even the % signs were converted to percent. Now, these labels are easy to understand AND process by code. This does not mean that you are finished cleaning but at least now the columns are more accessible.\nRemove empty and or constant columns and rows Data sets come with empty or superfluous rows or columns are not a rare sighting. This is especially true if you work with Excel files because there will be a lot of empty cells. Take a look at the dirty Excel data set from janitor\u0026rsquo;s GitHub page. It looks like this when you open it with Excel.\nTaking a look just at this picture we may notice a couple of things.\n  First, Jason Bourne is teaching at a school. I guess being a trained assassin qualifies him to teach physical education. Also - and this is just a hunch - undercover work likely earned him his \u0026ldquo;Theater\u0026rdquo; certification.\n  Second, the header above the actual table will be annoying, so we must skip the first line when we read the data set.\n  Third, the column names are not ideal but we know how to deal with that by now.\n  Fourth, there are empty rows and columns we can get rid of.\n  Fifth, there is a column that contains only \u0026lsquo;YES\u0026rsquo;. Therefore it contains no information at all and can be removed.\n  So, let us read and clean the data. The janitor package will help us with remove_empty() and remove_constant().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  xl_file \u0026lt;- readxl::read_excel(\u0026#39;dirty_data.xlsx\u0026#39;, skip = 1) %\u0026gt;% clean_names() %\u0026gt;% remove_empty() %\u0026gt;% remove_constant() xl_file ## # A tibble: 12 x 9 ## first_name last_name employee_status subject hire_date percent_allocated ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Jason Bourne Teacher PE 39690 0.75 ## 2 Jason Bourne Teacher Drafting 43479 0.25 ## 3 Alicia Keys Teacher Music 37118 1  ## 4 Ada Lovelace Teacher \u0026lt;NA\u0026gt; 38572 1  ## 5 Desus Nice Administration Dean 42791 1  ## 6 Chien-Shiung Wu Teacher Physics 11037 0.5  ## 7 Chien-Shiung Wu Teacher Chemistry 11037 0.5  ## 8 James Joyce Teacher English 36423 0.5  ## 9 Hedy Lamarr Teacher Science 27919 0.5  ## 10 Carlos Boozer Coach Basketball 42221 NA  ## 11 Young Boozer Coach \u0026lt;NA\u0026gt; 34700 NA  ## 12 Micheal Larsen Teacher English 40071 0.8  ## # ... with 3 more variables: full_time \u0026lt;chr\u0026gt;, certification_9 \u0026lt;chr\u0026gt;, ## # certification_10 \u0026lt;chr\u0026gt;   Here, remove_empty() defaulted to remove, both, rows and colums. If we wish, we can change that by setting e.g. which = 'rows'.\nNow, we may also want to see the hire_data in a sensible format. For example, in this dirty data set, Jason Bourne was hired on 39690. Luckily, our janitor can make sense of it all.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  xl_file %\u0026gt;% mutate(hire_date = excel_numeric_to_date(hire_date)) ## # A tibble: 12 x 9 ## first_name last_name employee_status subject hire_date percent_allocat~ ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Jason Bourne Teacher PE 2008-08-30 0.75 ## 2 Jason Bourne Teacher Drafting 2019-01-14 0.25 ## 3 Alicia Keys Teacher Music 2001-08-15 1  ## 4 Ada Lovelace Teacher \u0026lt;NA\u0026gt; 2005-08-08 1  ## 5 Desus Nice Administration Dean 2017-02-25 1  ## 6 Chien-Shiung Wu Teacher Physics 1930-03-20 0.5  ## 7 Chien-Shiung Wu Teacher Chemistry 1930-03-20 0.5  ## 8 James Joyce Teacher English 1999-09-20 0.5  ## 9 Hedy Lamarr Teacher Science 1976-06-08 0.5  ## 10 Carlos Boozer Coach Basketball 2015-08-05 NA  ## 11 Young Boozer Coach \u0026lt;NA\u0026gt; 1995-01-01 NA  ## 12 Micheal Larsen Teacher English 2009-09-15 0.8  ## # ... with 3 more variables: full_time \u0026lt;chr\u0026gt;, certification_9 \u0026lt;chr\u0026gt;, ## # certification_10 \u0026lt;chr\u0026gt;   Rounding To my surprise shock, R uses some unexpected rounding rule. In my world, whenever a number ends in .5, standard rounding would round up. Apparently, R uses something called banker\u0026rsquo;s rounding that in these cases rounds towards the next even number. Take a look.\n1 2  round(seq(0.5, 4.5, 1)) ## [1] 0 2 2 4 4   I would expect that the rounded vector contains the integers from one to five. Thankfully, janitor offers a convenient rounding function.\n1 2  round_half_up(seq(0.5, 4.5, 1)) ## [1] 1 2 3 4 5   Ok, so that gives us a new function for rounding towards integers. But what is really convenient is that janitor can round_to_fractions.\n1 2  round_to_fraction(seq(0.5, 2.0, 0.13), denominator = 4) ## [1] 0.50 0.75 0.75 1.00 1.00 1.25 1.25 1.50 1.50 1.75 1.75 2.00   Here, I rounded the numbers to the next quarters (denominator = 4) but of course any fraction is possible. You can now live the dream of rounding towards arbitrary fractions.\nFind matches in multiple characteristics In my opinion, the get_dupes() function is really powerful. It allows us to find \u0026ldquo;similar\u0026rdquo; observations in a data set based on certain characteristics. For example, the starwars data set from dplyr contains a lot of information on characters from the Star Wars movies. Possibly, we want to find out which characters are similar w.r.t. to certain traits.\n1 2 3 4 5 6 7 8 9 10 11 12 13  starwars %\u0026gt;% get_dupes(eye_color, hair_color, skin_color, sex, homeworld) %\u0026gt;% select(1:8) ## # A tibble: 7 x 8 ## eye_color hair_color skin_color sex homeworld dupe_count name height ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 blue black yellow female Mirial 2 Luminara U~ 170 ## 2 blue black yellow female Mirial 2 Barriss Of~ 166 ## 3 blue blond fair male Tatooine 2 Luke Skywa~ 172 ## 4 blue blond fair male Tatooine 2 Anakin Sky~ 188 ## 5 brown brown light female Naboo 3 CordÃ© 157 ## 6 brown brown light female Naboo 3 DormÃ© 165 ## 7 brown brown light female Naboo 3 PadmÃ© Amid~ 165   So, Luke and Anakin Skywalker are similar to one another. Who would have thought that. Sadly, I don\u0026rsquo;t enough about Star Wars to know whether the other matches are similarly \u0026ldquo;surprising\u0026rdquo;. In any case, the point here is that we can easily find matches according to arbitrarily many characteristics. Conveniently, these characteristics are the first columns of the new output and we get a dupe_count.\nAlright, this concludes our little showcase. In the janitor package, there is another set of tabyl() functions. These are meant to improve base R\u0026rsquo;s table() functions. Since I rarely use that function I did not include it but if you use table() frequently, then you should definitely check out tabyl().\n","description":"I demonstrate a couple of functions from the janitor package I find quite useful","id":11,"section":"post","tags":[],"title":"Showcasing the janitor package","uri":"https://albert-rapp.de/post/2022-01-12-janitor-showcase/"},{"content":"TidyTuesday, the weekly social data project that brings together R users, is a great way to connect to the R community and learn to wrangle and visualize data. But more importantly, it is a superb chance to learn new data visualization skills by doing thieving. Let me elaborate.\nEach week, you get a chance to work with a new data set and create a (hopefully) nice visualization1. Afterwards, you can share visualizations with the world on twitter using #tidyTuesday. Of course, being the curious person that you are, you check out contributions from other fellow R users. And more often than not, you will see really cool visualizations and wish that you could do something like that too. And you can!\nUsually, people share their code together with their viz. Consequently, you are only one ctrl-C away from stepping up your dataviz game. Do I mean that you should take the entire code and brand that as your own work? Of course not! But you can maybe ctrl-C aspects of the code and reuse it for something you have been wanting to do for a long time. Let\u0026rsquo;s make this specific. Last week, I found this gem by Georgios Karamanis.\nTransphobic hate crimes in Sweden for this week\u0026#39;s Bring Your Own Data #TidyTuesday.\nThe inspiration was a plot made by @thomasoide for this Axios article: https://t.co/zMrnr9tszG\nSource: @myndigheten_bra\ncode: https://t.co/HSCew2zrUg\n#Rstats #dataviz pic.twitter.com/IVQ1wTBZmt\n\u0026mdash; Georgios Karamanis (@geokaramanis) January 5, 2022  What intrigued me were the bars with criss-cross lines. Now, clearly I want to be able to do that too. Luckily, the tweet also contains a link to the corresponding GitHub repository. Et voilÃ , a quick glance at the code reveals the use of a so-called ggpattern package and a quick ctrl-C of the package name combined with a internet search leads me to the package\u0026rsquo;s documentation.\nThere, I find out that it is quite easy to get bars with different patterns2 using geom_col_pattern(). For example, these code snippets are taken straight from the documentation (more ctrl-Cs). For more, check out the documentation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  library(ggplot2) library(ggpattern) library(patchwork) df \u0026lt;- data.frame(level = c(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#39;d\u0026#39;), outcome = c(2.3, 1.9, 3.2, 1)) stripes \u0026lt;- ggplot(df) + geom_col_pattern( aes(level, outcome, pattern_fill = level), pattern = \u0026#39;stripe\u0026#39;, fill = \u0026#39;white\u0026#39;, colour = \u0026#39;black\u0026#39; ) + theme_bw(18) + theme(legend.position = \u0026#39;none\u0026#39;) kittens \u0026lt;- ggplot(df) + geom_col_pattern( aes(level, outcome, fill = level), pattern = \u0026#39;placeholder\u0026#39;, pattern_type = \u0026#39;kitten\u0026#39;, pattern_size = 3 ) + theme_bw(18) + theme(legend.position = \u0026#39;none\u0026#39;) stripes + kittens   There you go. So, now I \u0026ldquo;can do\u0026rdquo; bars with different patterns. \u0026ldquo;Hold on, it is not like you are totally an expert now. How does any of that help?\u0026quot;, you might think. And, clearly you are right. Having emulated something I saw online, does not make me exactly into an visual artist but now I am equipped with one more tool to try out come next TidyTuesday.\nRepeat that often enough and soon you have acquired a lot of tools to use in diverse settings. Eventually, the lines between \u0026ldquo;I copied what I found online\u0026rdquo; and \u0026ldquo;This is a trick I like to do frequently\u0026rdquo; blur. In the end, repeated practice and learning from others is what makes you into an expert. And sometimes that \u0026ldquo;learning from others\u0026rdquo; part is as simple as strolling through GitHub repositories on the lookout for your next great coup.\n Honestly, it does not really matter if your visualization is looking \u0026ldquo;nice\u0026rdquo;. I have ended up sharing a bunch of, say, average at best visualizations. (Exhibit A, Exhibit B). The point is too keep showing up and trying. In fact, even the visualizations I am not totally proud of contain elements which I have spent a lot of time working on. This practice has often ended up helping me in unexpected situations.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I know, I know. The tweet was using geom_rect_pattern(). Not exactly the same but the principles are.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"I advocate to take part in the TidyTuesday events to learn with and from others.","id":12,"section":"post","tags":["visualization","opinion"],"title":"ggplot-tips: Learning by Thieving","uri":"https://albert-rapp.de/post/2022-01-10-learning-by-thieving/"},{"content":"It is almost the beginning of a new year and I have decided to finish off this year with a quick blog post. Also, friends were shaming me that I have been slacking off on this blog lately. Therefore, let\u0026rsquo;s get started right away. We\u0026rsquo;ll keep things simple and look at a few cool plots from the ggforce package. Of course, we have already glimpsed at this package in the previous installment of this ggplot2-tips series.\nMark Point Plots Let us first take a look at the penguins data set from the palmerpenguins package. Same as last time, this will be the dummy data set we use for plots but of course any other data set would be fine too.\n1 2 3 4 5 6 7 8 9  library(dplyr) library(ggplot2) theme_set(theme_light()) dat \u0026lt;- palmerpenguins::penguins %\u0026gt;% filter(!is.na(sex)) p \u0026lt;- dat %\u0026gt;% ggplot(aes(bill_length_mm, flipper_length_mm, col = species)) + geom_point() p   Visually, we can see that the points are strongly grouped by species which makes sense as these kind of measurements often define a species. With help from ggforce we can visually emphasize this grouping by drawing rectangles or ellipses around the groups.\n1 2 3 4 5 6 7 8  library(ggforce) rect_plot \u0026lt;- p + geom_mark_rect(size = 1) ellipse_plot \u0026lt;- p + geom_mark_ellipse(aes(fill = species), alpha = 0.25) library(patchwork) # see last ggplot2-tips post rect_plot / ellipse_plot   There is also a geom_mark_hull() function that requires the concaveman package to be installed. Using this function, we can draw a hull around the points.\n1 2  p + geom_mark_hull(size = 1, concavity = 3)   Beware though that this hull is \u0026ldquo;redrawn at draw time\u0026rdquo;, so your hull may look different when you zoom into the plot. Also, let me point out that geom_mark_hull() has an argument concavity that allows you to make the hull \u0026ldquo;more wiggly\u0026rdquo;.\nAlluvial Plots With ggforce you can easily draw so-called alluvial plots. Originally, these are used to visualize a \u0026ldquo;stream over time\u0026rdquo; as for instance shown on Wikipedia. But the same visualization can be used to visualize \u0026ldquo;composition of groups\u0026rdquo; like so.\nFrom this plot, it is clear that unsurprisingly most of high weight penguins are male. What is maybe more surprising is that all Chinstrap penguins live on Dream. Obviously, the first layer in this alluvial plot is sort of redundant as the color already codes the sex but for accessibility it is often encouraged to use some form of double encoding (e.g. different shape AND color for groups). Thus, I find it practical and somewhat convenient to add this first layer.\nCreating this plot requires a couple of steps but ggforce has useful functions that make our life easier. More precisely we will need to\n count occurences in each subgroup and convert this in a suitable format for later plotting. gather_set_data() will help us doing that. draw lines between subgroups with geom_parallel_sets() draw boxes to identify subgroups with geom_parallel_sets_axes() label the boxes with geom_parallel_sets_labels  The first step is processed as follows\n1 2 3 4 5 6 7 8 9  reshaped_dat \u0026lt;- dat %\u0026gt;% mutate( mass_group = factor( cut_number(body_mass_g, 3), labels = c(\u0026#34;high\u0026#34;, \u0026#34;medium\u0026#34;, \u0026#34;low\u0026#34;) ) ) %\u0026gt;% count(species, island, sex, mass_group) %\u0026gt;% gather_set_data(x = 1:4)   This simply counts the occurences in each subgroup and then adds three columns x, y and id based on the subgroup labels. These three new columns are necessary for generating the plot which is done as follows\n1 2 3 4 5 6 7 8 9 10  reshaped_dat %\u0026gt;% ggplot(aes( x = x, split = y, id = id, value = n )) + geom_parallel_sets(aes(fill = sex), alpha = 0.5) + geom_parallel_sets_axes(axis.width = 0.2) + geom_parallel_sets_labels(colour = \u0026#39;white\u0026#39;, size = 4)   Here, value is the counts of the subgroups. Also, notice that the splits on the x-axis is not in the same order as in my original plot. The order can be easily changed by converting x to a factor whose levels have the desired ordering. The complete code is\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  reshaped_dat %\u0026gt;% ggplot(aes( x = factor(x, c(\u0026#34;sex\u0026#34;, \u0026#34;species\u0026#34;, \u0026#34;island\u0026#34;, \u0026#34;mass_group\u0026#34;)), split = y, id = id, value = n )) + geom_parallel_sets(aes(fill = sex), alpha = 0.5) + geom_parallel_sets_axes(axis.width = 0.2) + geom_parallel_sets_labels(colour = \u0026#39;white\u0026#39;, size = 4) + labs(x = element_blank()) + scale_y_continuous(breaks = NULL) + theme(text = element_text(size = 12)) + scale_fill_brewer(palette = \u0026#39;Set1\u0026#39;)   Voronoi Diagrams Next, let us explore Voronoi diagrams. These are constructed from a set of \u0026ldquo;center points\u0026rdquo; which are used to form polygons such that these fill the whole plane and each polygons consists of the points that are closest to a polygon\u0026rsquo;s center point. If you found this somewhat confusing, then you are in luck because Wikipedia has a super neat animation that illustrates this concept.\nUsing bill and flipper lengths to define the center points' x- and y-coordinates, we can create a Voronoi diagram via geom_voronoi_tile() and geom_voronoi_segment() as follows.\n1 2 3 4 5 6  dat %\u0026gt;% ggplot(aes(bill_length_mm, flipper_length_mm, group = 1)) + geom_voronoi_tile(aes(fill = species)) + geom_voronoi_segment() + scale_fill_brewer(palette = \u0026#34;Set1\u0026#34;) + theme_void()   Here, the lines between polygons are shown due to geom_voronoi_segment() and if we wish to get rid of the lines we can simply remove this layer. Also, let us ignore possible applications of Voronoi diagrams1 for a bit. What I really wanted to demonstrate is a small bit of Rtistry I found on Twitter and found really cool.\nWith a couple of random numbers and a bit of coloring one can create some visually appealing graphics (at least I like to think so). First, let\u0026rsquo;s take a look at only a few random numbers\n1 2 3 4 5 6 7 8  set.seed(23479) N \u0026lt;- 25 tibble(x = runif(N), y = runif(N)) %\u0026gt;% ggplot(aes(x, y)) + geom_voronoi_tile(aes(fill = y)) + scale_fill_viridis_c(option = \u0026#39;A\u0026#39;) + theme_void() + theme(legend.position = \u0026#39;none\u0026#39;)   Not so super impressive but using many random numbers a \u0026ldquo;smoother\u0026rdquo; picture will be created,\n1 2 3 4 5 6 7 8  set.seed(23479) N \u0026lt;- 1000 tibble(x = runif(N), y = runif(N)) %\u0026gt;% ggplot(aes(x, y)) + geom_voronoi_tile(aes(fill = y)) + scale_fill_viridis_c(option = \u0026#39;A\u0026#39;) + theme_void() + theme(legend.position = \u0026#39;none\u0026#39;)   Of course, arranging the center points differently and using other colors leads to very different pictures.\n1 2 3 4 5 6 7 8  set.seed(23479) N \u0026lt;- 1000 tibble(x = runif(N, -1, 1), y = sqrt(abs(x) + runif(N))) %\u0026gt;% ggplot(aes(x, y)) + geom_voronoi_tile(aes(fill = y)) + scale_fill_viridis_c(option = \u0026#39;E\u0026#39;) + theme_void() + theme(legend.position = \u0026#39;none\u0026#39;)   Sina Plots Coming back to less artistic plots, consider the following violin plots from the ggplot2 package.\n1 2 3  dat %\u0026gt;% ggplot(aes(x = species, y = body_mass_g)) + geom_violin(fill = \u0026#34;grey80\u0026#34;)   Compared with common boxplots, these kind of plots show the distribution of the data more explicitly with density estimates (rotated by 90 degrees and mirrored for symmetry). This gets rid of the intrinsic problem of boxplots, i.e. only showing quantiles. Sometimes though, we want to see the quantiles as well. In these instances, an additional boxplot is plotted within the violin plots like so.\n1 2 3 4  dat %\u0026gt;% ggplot(aes(x = species, y = body_mass_g)) + geom_violin(fill = \u0026#34;grey80\u0026#34;) + geom_boxplot(width = 0.25)   However, even with both of these plots combined we still don\u0026rsquo;t know how many points are in this data set. To make that information available in the visualizations, so-called sina plots fill the area of violin plots with jittered data points instead of depicting the estimated density directly.\n1 2 3  dat %\u0026gt;% ggplot(aes(x = species, y = body_mass_g)) + geom_sina()   If a data set is large, then the points will display the same contour as the violin plot. In any case, the violin plot can be plotted beneath the points as well for better visibility.\n1 2 3 4  dat %\u0026gt;% ggplot(aes(x = species, y = body_mass_g)) + geom_violin(fill = \u0026#34;grey80\u0026#34;) + geom_sina()   This way, we can see both the distribution AND the number of data points in a single plot. Of course, there are more ways to display the distribution of data and ggdist is just the right package to do that job. I will show you that particular package in the next installment of the ggplot2-tips series.\nAnd that concludes our small demonstration of a few ggforce functions. For more functions check out ggforce\u0026rsquo;s website. For sure, there is more cool stuff like Bezier curves and facet zooms to explore.\nFinally, here is an overview of all the cool visuals we have created. Let me know what you think in the comments or simply hit the applause button below if you liked the content.\n See Wikipedia if you\u0026rsquo;re interested in a list of applications.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"The ggplot2-tips series is continued with a few example plots from the ggforce package","id":13,"section":"post","tags":[],"title":"A couple of visualizations from ggforce","uri":"https://albert-rapp.de/post/2021-12-31-ggforce-examples/"},{"content":"A couple of weeks back, I wanted to explain to my student what I mean when I talk about the \u0026ldquo;variance of the sample variance\u0026rdquo;. In my head, this term sounds quite confusing and contains the word \u0026ldquo;variance\u0026rdquo; at least one too many times. But as I was not sure whether my subsequent explanation really came through, I decided to let my students explore the notion on their own through a Shiny app.\nHonestly, I thought this would be quite simple to code because I have already learned the basics of Shiny when I wanted to show my students what exciting web developmental things R can do. Back then, I summarized the basics in one chapter of my YARDS lecture notes.\nHowever, even though the idea of my app was simple, I soon came to realize that I would need to learn a couple more Shiny-related things to get the job done. And, as is usual with coding, I did this mostly by strolling through the web in order to find code solutions for my particular problems. Most of the time, I consulted Hadley Wickham\u0026rsquo;s Mastering Shiny but still I ended up searching for a lot of random other stuff on the web.\nConsequently, I decided that it might be nice to collect what I have learned in one place. So, here is a compilation of loosely connected troubles I solved during my Shiny learning process. May this summary serve someone well.\nUse a theme for simple customization Let\u0026rsquo;s start with something super easy. If you wish to customize the appearance of you app, you can set the theme argument of fluidPage() to either a CSS-file that contains the necessary configuration (this is the hard way) or use a theme from bslib::bs_theme(). The latter approach comes with a lot of named preimplemented themes and is easily implemented by bootswatch = \u0026quot;name\u0026quot;. In my app, I have simply added theme = bslib::bs_theme(bootswatch = \u0026quot;superhero\u0026quot;). For other themes, have a look at RStudio\u0026rsquo;s Shiny themes page.\nCheck out this super simple example that I have adapted from the default \u0026ldquo;new Shiny app\u0026rdquo; output (you will actually have to copy and run this in an R script on your own).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  library(shiny) library(tidyverse) ui \u0026lt;- fluidPage( # Theme added here theme = bslib::bs_theme(bootswatch = \u0026#34;superhero\u0026#34;), titlePanel(\u0026#34;Old Faithful Geyser Data\u0026#34;), sidebarLayout( sidebarPanel( sliderInput(\u0026#34;bins\u0026#34;, \u0026#34;Number of bins:\u0026#34;, min = 1, max = 50, value = 30) ), mainPanel( plotOutput(\u0026#34;distPlot\u0026#34;) ) ) ) server \u0026lt;- function(input, output) { output$distPlot \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) } shinyApp(ui = ui, server = server)   During the course of this text, we will extend this small example bit by bit. But, I want to avoid copy-and-pasting code each time we change something. Thus, for the remaining examples I will only describe the changes to the previous version instead of pasting the whole code. Nevertheless, I will provide links after each example so that each script can be downloaded at will. The current example can be found here.\nIsolate slider from reactivity As is currently intended, our app\u0026rsquo;s histogram changes whenever the slider is moved. Sometimes, though, this is not what we wish to do. Instead, we may want to delay the rendering of the plot until a button is clicked.\nThis can be achieved through a simple isolate() command which, well, isolates whatever is in between the function\u0026rsquo;s parentheses from changes on the UI. Here, let us put input$bins into the isolate() function and check what happens when we move the slider (full code here), i.e. we changed\n1  bins \u0026lt;- seq(min(x), max(x), length.out = isolate(input$bins) + 1)   Excellent! Nothing happens when we move the slider. Dumb and useless but excellent anyway.\nObserve that we could have also put the whole renderPlot() function call into isolate(). This app would work in the sense that we created valid code but then the reactivity of the slider is still active. The isolate() documentation hints at this with \u0026ldquo;\u0026hellip;if you assign a variable inside the isolate(), its value will be visible outside of the isolate()\u0026rdquo;.\nCreate and observe Buttons Let us bring back some reactivity to our app by adding a button that reevaluates our histogram when clicked. First, we will add a button to the UI. Second, we will implement what needs to happen on the server side of things when the button is clicked.\nThe first step is pretty simple. All we have to do is add actionButton() to the UI. Same as sliderInput() we have to specify a inputId and label for the button. Here, we could add\n1  actionButton(\u0026#34;draw_button\u0026#34;, \u0026#34;Reevaluate!\u0026#34;, width = \u0026#34;100%\u0026#34;)   Then, on the server side we will have to catch each click on the button. Once a click is registered, the plot is supposed to be rendered again. We do this with observeEvent() which expects an event expression and a handler expression. In our case, the former is simply the id of our button, i.e. input$draw_button, and the latter is what code is to be executed when the event is observed. Therefore, we move our code for rendering the plot into this part of observeEvent(). Thus, in our server function we now have\n1 2 3 4 5 6 7 8 9  observeEvent( input$draw_button, { output$distPlot \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = isolate(input$bins) + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) } )   Notice that we have wrapped our code into {}. Strictly speaking, this is not necessary because we only \u0026ldquo;do one thing\u0026rdquo; but, of course, we can easily imagine that we want to tie multiple calculations to a button click. In this case, we will need to wrap all commands into {}. In any case, our code now does what we expect it to do and on each click a new histogram is rendered using the current value of the slider input. This new app\u0026rsquo;s complete code can be found here.\nUse eventReactive() as an alternative for updating values Honestly, this part I learned just 5 minutes ago while I was writing the last section of this blog post. When I looked into the documentation of observeEvent(), I noticed that there is also a function eventReactive() which may be better suited for our current use case as it allows us to avoid manually isolating input$bins.\nThis new function works similar to observeEvent() but it creates a reactive variable instead. This, we can use for rendering. Check this out\n1 2 3 4 5 6 7 8 9  plot \u0026lt;- eventReactive( input$draw_button, { x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) } ) output$distPlot \u0026lt;- renderPlot({plot()})   Notice how we do not use isolate() anymore and use the plot variable like a reactive in renderPlot(), i.e. we have to \u0026ldquo;call\u0026rdquo; its value with ().\nHowever, be aware that eventReactive() creates a reactive variable such that you cannot change, say, multiple plots at once. Nevertheless, eventReactive() can be a great way to tie a plot to an event. So, I guess it dependes on your use case and personal preference if you want to use eventReactive() rather than observeEvent(). Anyway, this version\u0026rsquo;s code can be copied from here.\nUse reactiveVal() to manually change values on click Another neat function is reactiveVal() which helps you to construct for instance counters that increase on the click of a button. We can initialize a reactive value by writing\n1  counter \u0026lt;- reactiveVal(value = 0)   within the server function. This way, our counter is set to zero and we can update it and set it to, say, one by calling counter(value = 1). The current value of the counter can be accessed through counter().\nClearly, we can tie the updating of a reactive value to an event that we observe through observeEvent(). For instance, we count how often the draw button in our small app is clicked by changing our previous observeEvent(input$draw_button, ...). Here, we would change this particular line of code to\n1 2 3 4 5 6 7 8 9 10 11 12  observeEvent( input$draw_button, { tmp \u0026lt;- counter() counter(tmp + 1) output$distPlot \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = isolate(input$bins) + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) } )   Finally, we can show this information on our UI for demonstration purposes by adding a textOutput(\u0026quot;demonstration_text\u0026quot;) to our UI and setting\n1 2 3 4 5  output$demonstration_text \u0026lt;- renderText(paste( \u0026#34;You have clicked the draw button\u0026#34;, counter(), \u0026#34;times. Congrats!\u0026#34; ))   The complete app can be found here.\nUse tabsetPanel and unique plot names Often, you do not want to display all information at once. In my particular case, I wanted to show only one out of two plots based on the user\u0026rsquo;s chosen estimator (sample mean or sample variance). A great way to achieve that is to use tabsetPanel() in the UI.\nOrdinarily, you can create a UI this way by setting\n1 2 3 4 5 6 7  mainPanel( tabsetPanel( tabPanel(\u0026#34;Plot\u0026#34;, plotOutput(\u0026#34;plot\u0026#34;)), tabPanel(\u0026#34;Summary\u0026#34;, verbatimTextOutput(\u0026#34;summary\u0026#34;)), tabPanel(\u0026#34;Table\u0026#34;, tableOutput(\u0026#34;table\u0026#34;)) ) )   This was an example taken straight out of the documentation of tabsetPanel(). What you will get if you start an app containing a UI like this is a panel with three tabs (each one corresponding to a plot, text or table output) and the user can click on the tabs to switch between the views. This isn\u0026rsquo;t that surprising.\nHowever, if we also add an id to this and set type to hidden, like so\n1 2 3 4 5 6 7 8 9  mainPanel( tabsetPanel( id = \u0026#34;my_tabs\u0026#34;, type = \u0026#34;hidden\u0026#34;, tabPanel(\u0026#34;Plot\u0026#34;, plotOutput(\u0026#34;plot\u0026#34;)), tabPanel(\u0026#34;Summary\u0026#34;, verbatimTextOutput(\u0026#34;summary\u0026#34;)), tabPanel(\u0026#34;Table\u0026#34;, tableOutput(\u0026#34;table\u0026#34;)) ) )   then, by default, the user does not have the options to change between views by clicking on tabs. Now, the view will need to change based on other interactions of the user with the UI. This change will then need to be customized within the server function. This is where the id argument comes into play because it allows ourselves to address the tabs via updateTabsetPanel().\nHere, let us take our previous example and display the same information on a different panel, i.e. at the end we will have two panels with exactly the same information in each tab. I know. This is not particularly exciting or meaningful but it serves our current purpose well.\nNaively, we might implement our user-interface like so\n1 2 3 4 5 6 7 8 9 10 11 12  mainPanel( tabsetPanel( id = \u0026#34;my_tabs\u0026#34;, type = \u0026#34;hidden\u0026#34;, tabPanel(\u0026#34;panel1\u0026#34;, { # UI commands from before here }), tabPanel(\u0026#34;panel2\u0026#34;, { # UI commands from before here }), ) )   However, we will have to be careful! If we simply copy-and-paste our UI from before, then we won\u0026rsquo;t have unique identifiers to address e.g. the draw button or the plot output. Since this is a serious NO-NO (all caps for dramatic effect) and the app won\u0026rsquo;t work properly, let us instead write a function that draws the UI for us but creates it with different identifiers like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  create_UI \u0026lt;- function(unique_part) { sidebarLayout( sidebarPanel( # unique label here by adding unique_part to bins sliderInput(paste(\u0026#34;bins\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;), \u0026#34;Number of bins:\u0026#34;, min = 1, max = 50, value = 30), actionButton(paste(\u0026#34;draw_button\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;), \u0026#34;Reevaluate!\u0026#34;, width = \u0026#34;100%\u0026#34;), actionButton(paste(\u0026#34;change_view\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;), \u0026#34;Change view\u0026#34;, width = \u0026#34;100%\u0026#34;) ), mainPanel( textOutput(paste(\u0026#34;demonstration_text\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;)), # Counter text added textOutput(paste(\u0026#34;countEvaluations\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;)), plotOutput(paste(\u0026#34;distPlot\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;)) ) ) }   Also, notice that I have created another button called \u0026ldquo;Change view\u0026rdquo; within the UI. Further, this button\u0026rsquo;s name is so mind-baffling that I won\u0026rsquo;t even try to elaborate what it will do. Finally, using create_UI, we can set up the UI like so\n1 2 3 4 5 6 7 8 9  mainPanel( tabsetPanel( id = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel1\u0026#34;, type = \u0026#34;hidden\u0026#34;, tabPanel(\u0026#34;panel1\u0026#34;, create_UI(\u0026#34;panel1\u0026#34;)), tabPanel(\u0026#34;panel2\u0026#34;, create_UI(\u0026#34;panel2\u0026#34;)), ) )   and address everything within the UI in a unique manner. Of course, such a functional approach only works well if the two panels look sufficiently similar such that it makes sense to design them through a single function. In my particular app that deals with the variance of estimators, this was the case because the tabs for the sample mean and sample variance were quite similar in their structure.\nNow that we have covered how the UI needs to be set up, let me show you how to change the view from one panel to the next. Shockingly, let us link this to a click on the \u0026ldquo;change view\u0026rdquo; button(s) like so\n1 2 3 4 5 6 7 8  observeEvent( input$change_view_panel1, updateTabsetPanel(inputId = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel2\u0026#34;) ) observeEvent( input$change_view_panel2, updateTabsetPanel(inputId = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel1\u0026#34;) )   Also, note that the previous code\n1 2 3 4 5 6 7 8 9 10 11 12  observeEvent( input$draw_button, { tmp \u0026lt;- counter() counter(tmp + 1) output$distPlot \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = isolate(input$bins) + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) } )   won\u0026rsquo;t work anymore because the old identifiers like draw_button etc. need to be updated to draw_button_panel1 or draw_button_panel2. Clearly, this could potentially require some code duplication to implement the server-side logic for both tabs. But since we feel particularly clever today1, let us write another function that avoids a lot of code duplication.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  render_my_plot \u0026lt;- function(panel, counter, input, output) { tmp \u0026lt;- counter() # save current value of counter counter(tmp + 1) # update counter # Create identifier names bins_name \u0026lt;- paste(\u0026#34;bins\u0026#34;, panel, sep = \u0026#34;_\u0026#34;) distplot_name \u0026lt;- paste(\u0026#34;distPlot\u0026#34;, panel, sep = \u0026#34;_\u0026#34;) demonstration_text \u0026lt;- paste(\u0026#34;demonstration_text\u0026#34;, panel, sep = \u0026#34;_\u0026#34;) # Render Plot output[[distplot_name]] \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = isolate(pluck(input, bins_name)) + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) # Render counter text output[[demonstration_text]] \u0026lt;- renderText(paste( \u0026#34;You have clicked the draw button\u0026#34;, counter(), \u0026#34;times. Congrats!\u0026#34; )) }   Notice a few things here:\n Our function needs to know the objects counter, input and output to work. Also we need to switch to double-bracket notation for assigning new variables like distPlot_panel1 to output. Obviously, we couldn\u0026rsquo;t use $ for assignment anymore but single-bracket notation like output[var_name] is for some reason forbidden in Shiny. At least, that\u0026rsquo;s what an error message will kindly tell you when you dare to use only one bracket.  So, all in all our server-side logic looks like this now\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  server \u0026lt;- function(input, output) { # Counter initialization counter \u0026lt;- reactiveVal(value = 0) counter2 \u0026lt;- reactiveVal(value = 0) # Plot Rendering observeEvent( input$draw_button_panel1, { render_my_plot(\u0026#34;panel1\u0026#34;, counter, input, output) } ) observeEvent( input$draw_button_panel2, { render_my_plot(\u0026#34;panel2\u0026#34;, counter2, input, output) } ) # Panel Switching observeEvent( input$change_view_panel1, updateTabsetPanel(inputId = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel2\u0026#34;) ) observeEvent( input$change_view_panel2, updateTabsetPanel(inputId = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel1\u0026#34;) ) }   The complete app that we have just build can be found here.\nClosing Alright, I hope this helps you to build your own small Shiny app. In my particular case, I had to use another cool function from the shinyjs package to update the text on the UI such that it appears in red for a second (in order for the user to notice what changes). And because I have the feeling that shinyjs has way more in store for us, I will end this already quite long blog post here and save that (exciting) story for another time. Hope you will be there when I talk about shinyjs.\n And with that I really mean today. When I built my Shiny app, I actually used code duplication. But in hindsight, I feel somewhat embarrassed to leave it as it is for this blog post. Thus, I figured out how to make it work with a function.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"I recently built a small Shiny app where I had to search for how to do a lot of things. Here are 6 things I learned doing that. Maybe a Shiny beginner will find something useful in here.","id":14,"section":"post","tags":["shiny"],"title":"6 simple Shiny things I have learned from creating a somewhat small app","uri":"https://albert-rapp.de/post/2021-11-21-a-few-learnings-from-a-simple-shiny-app/"},{"content":"In this week\u0026rsquo;s TidyTuesday, I noticed that I am frequently not using only ggplot2 to create plots. In fact, it has become essential to me to leverage the powers of other great additional packages that align well with ggplot2. Therefore, I decided to extend my ggplot2-tips series by introducing a few packages I use quite often.\nIn this post, I want to cover how to arrange multiple plots. In particular, I will talk about the fantastic patchwork package by Thomas Lin Pedersen which helps to arrange plots quite intuitively. Further, I want to take a glance at ggforce, another package written by the same author as patchwork, because it also has a neat function for arranging plots. However, ggforce can do way more and I will demonstrate that in another installment of this series.\nSo, let us begin by creating a data set we want to fiddle with for plotting purposes. For simplicity, let us use the penguins data (without missing values) from the palmerpenguins package.\n1 2 3 4 5  library(tidyverse) theme_set(theme_light()) # All missing values can be filtered out by filtering the `sex` variable dat \u0026lt;- palmerpenguins::penguins %\u0026gt;% filter(!is.na(sex))   Arrange Plots via patchwork Often, we want to show multiple plots that tell a story when looked at together. Using patchwork, we can easily compose a single plot consisting of subplots. This is done by using the simple symbols + resp. / to display plots next to resp. on top of each other.\nFor demonstration purposes, let us generate a few simple plots.\n1 2 3 4  point_plot \u0026lt;- dat %\u0026gt;% ggplot(aes(bill_length_mm, flipper_length_mm, fill = sex)) + geom_jitter(size = 3, alpha = 0.5, shape = 21) point_plot   1 2 3 4  point_plot2 \u0026lt;- dat %\u0026gt;% ggplot(aes(bill_length_mm, bill_depth_mm, fill = sex)) + geom_jitter(size = 3, alpha = 0.5, shape = 21) point_plot2   1 2 3 4 5  # plot_plot is obviously a fun name boxplot_plot \u0026lt;- dat %\u0026gt;% ggplot(aes(x = body_mass_g, fill = sex)) + geom_boxplot() boxplot_plot   Clearly, showing each plot separately is boring and may not tell a story convincingly. Possibly, here you may want to say that the length and depth measurements give no clear distinction between male and female penguins but the weight measurements offers a better distinguishabilty between sexes. Maybe, if we see all plots together, we can tell that story without boring the reader.\n1 2 3  library(patchwork) p \u0026lt;- (point_plot + point_plot2) / boxplot_plot p   See how I have used + to put the point plots next to each other and / to plot the boxplots below the two point plots. Obviously, that was super easy and neat. But this simple arrangement leads to a doubling of the legends which is somewhat bothersome. However, this is no cause for concern. plot_layout() is there to collect those legends for you.\n1  p + plot_layout(guides = \u0026#34;collect\u0026#34;)   Of course, this leaves you with two legends which is kind of superfluous. The easy way to get rid of this is to plot no legends for the boxplots.\n1 2 3  boxplot_plot \u0026lt;- boxplot_plot + guides(fill = \u0026#34;none\u0026#34;) p \u0026lt;- (point_plot + point_plot2) / boxplot_plot p + plot_layout(guides = \u0026#34;collect\u0026#34;)   Now, what about legend positioning? Well, we already know how that usually works for a single plot (via theme() in case you forgot) and the good news is that the exact same thing works with patchwork as well. But beware to apply an additional theme() layer to the whole plot and not just to the last plot added to our composed plot. To make sure that happens, we have to add this layer via \u0026amp;.\n1  p + plot_layout(guides = \u0026#34;collect\u0026#34;) \u0026amp; theme(legend.position = \u0026#34;top\u0026#34;)   By the same logic, we can make additional changes to the whole plot e.g. to change the color mapping.\n1 2 3 4  p + plot_layout(guides = \u0026#34;collect\u0026#34;) \u0026amp; theme(legend.position = \u0026#34;top\u0026#34;) \u0026amp; scale_fill_brewer(palette = \u0026#34;Dark2\u0026#34;)   Next, let us control the layout a bit more and annotate the plot with plot_annotation().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  (point_plot + point_plot2 + plot_layout(widths = c(0.7, 0.3))) / boxplot_plot + plot_layout(guides = \u0026#34;collect\u0026#34;, heights = c(0.4, 0.6)) + plot_annotation( title = \u0026#34;Look at that arrangement!\u0026#34;, subtitle = \u0026#34;Wow\u0026#34;, caption = \u0026#34;OlÃ .\u0026#34;, tag_levels = \u0026#34;A\u0026#34;, tag_prefix = \u0026#34;(\u0026#34;, tag_suffix = \u0026#34;)\u0026#34; ) \u0026amp; labs(fill = \u0026#34;Penguin sex\u0026#34;) \u0026amp; theme(legend.position = \u0026#34;top\u0026#34;) \u0026amp; scale_fill_brewer(palette = \u0026#34;Dark2\u0026#34;)   We did quite a lot here, so let\u0026rsquo;s recap:\n We changed the widths of the plots in the first row by passing a vector of relative widths to widths in plot_layout(). Same thing with heights in plot_layout() to make the boxplots larger. Renamed legend label with the regular labs() function. Added a title, subtitle, caption and tags to the whole plot with plot_annotation().  Also, if you want to have the tags to only label the upper and lower row, you may want to wrap the first row together by wrap_elements(). Think of this as creating a new single unit.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  wrapped_plots \u0026lt;- wrap_elements( point_plot + point_plot2 + plot_layout(widths = c(0.7, 0.3)) ) (wrapped_plots) / boxplot_plot + plot_layout(guides = \u0026#34;collect\u0026#34;, heights = c(0.4, 0.6)) + plot_annotation( title = \u0026#34;Look at that arrangement!\u0026#34;, subtitle = \u0026#34;Wow\u0026#34;, caption = \u0026#34;OlÃ .\u0026#34;, tag_levels = \u0026#34;A\u0026#34;, tag_prefix = \u0026#34;(\u0026#34;, tag_suffix = \u0026#34;)\u0026#34; ) \u0026amp; theme(legend.position = \u0026#34;top\u0026#34;) \u0026amp; scale_fill_brewer(palette = \u0026#34;Dark2\u0026#34;)   Notice how the upper row reinstated the default colors and has two legends. This demonstrates how wrap_elements() made the plots \u0026ldquo;independent\u0026rdquo; from the overall theming via \u0026amp;, so to speak. On the bright side, there is no (C) tag anymore.\nUnsurprisingly, patchwork can do much more but for starters I think the previous examples will already get you quite far. They are you \u0026ldquo;80/20 leverage points\u0026rdquo;, if you will. But in order to add one more neat feature, let me finish our intro to patchwork by showing you how to create plots in plots via insets.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Tweak boxplots a bit for better visual fit to point_plot plt \u0026lt;- boxplot_plot + theme_minimal() + coord_flip() + theme(plot.background = element_rect(fill = \u0026#34;grey80\u0026#34;)) point_plot + coord_cartesian(xlim = c(25, 60)) + inset_element( plt, left = 0.01, right = 0.4, top = 0.99, bottom = 0.6 )   Create Subplots via ggforce I really enjoy arranging plots with patchwork because, to me, the syntax feels quite intuitive (mostly). However, as you probably noticed, I had to design each subplot and arrange them by hand. Clearly, if I want to use a grid-like arrangement to display each combination of two variables from a given set of variables, this may become tedious.\nLuckily, there is the ggforce package that has a neat faceting function to accomplish just that. As was already mentioned above, apart from that, the ggforce package offers even more cool stuff which we will look at in a future blog post.\nWith facet_matrix() it becomes quite easy to get a grid of subplots to display multiple combinations of two variables. For instance, take a look at this.\n1 2 3 4 5 6 7  library(ggforce) dat %\u0026gt;% ggplot(aes(x = .panel_x, y = .panel_y, fill = sex)) + geom_point(alpha = 0.5, size = 2, shape = 21) + facet_matrix( vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g) )   Now, while this is not a particular beautiful plot, it gives us a quick overview of interesting variables which might be great for an exploratory analysis. Notice how we had to use .panel_x and .panel_y as placeholder for the individual variables. We could use the geom_auto*() functions to avoid typing that as they default to the correct values for x and y. Consequently, we could have written\n1 2 3 4 5 6  dat %\u0026gt;% ggplot(aes(fill = sex)) + geom_autopoint(alpha = 0.5, size = 2, shape = 21) + facet_matrix( vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g) )   With a little bit of tweaking, we can make this plot more interesting. For example. it would be neat if we had density plots on the diagonal. No problem! Add another geom_autodensity() layer and make sure that facet_matrix() understands to map only this layer to the diagonal subplots.\n1 2 3 4 5 6 7 8  dat %\u0026gt;% ggplot(aes(fill = sex)) + geom_autopoint(alpha = 0.5, size = 2, shape = 21) + # Layer 1 geom_autodensity(alpha = 0.5, position = \u0026#34;identity\u0026#34;) + # Layer 2 facet_matrix( vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g), layer.diag = 2 )   See how layer.diag = 2 maps the diagonal elements to the second line of geom_* code. Similarly, we can manipulate the content of the upper and lower triangle in this grid by changing layer.lower or layer.upper in facet_matrix(). Let\u0026rsquo;s add another layer to see that in action.\n1 2 3 4 5 6 7 8 9 10  dat %\u0026gt;% ggplot(aes(fill = sex)) + geom_autopoint(alpha = 0.5, size = 2, shape = 21) + # Layer 1 geom_autodensity(alpha = 0.75, position = \u0026#34;identity\u0026#34;) + # Layer 2 geom_hex(aes(x = .panel_x, y = .panel_y), alpha = 0.75) + # Layer 3 facet_matrix( vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g), layer.diag = 2, layer.lower = 3 )   Last but not least, let me mention that we can also easily create what is called an \u0026ldquo;asymmetric grid\u0026rdquo; in ggforce by mapping rows and columns manually. This is great for having categorical variables on one axis and numerical variables on the other axis.\n1 2 3 4 5 6 7 8 9  dat %\u0026gt;% ggplot() + geom_boxplot( aes(x = .panel_x, y = .panel_y, group = .panel_x) ) + facet_matrix( cols = vars(sex, species), rows = vars(bill_depth_mm:body_mass_g) )   Beware that geom_boxplot() is a bit tricky as it requires the group argument to be explicitly set. Furthermore, if you want to add another aesthetic, e.g. fill, you will have to set group via interaction().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  dat %\u0026gt;% ggplot() + geom_boxplot( aes( x = .panel_x, y = .panel_y, fill = island, group = interaction(.panel_x, island) ) ) + facet_matrix( cols = vars(sex, species), rows = vars(bill_depth_mm:body_mass_g) )   This concludes our short summary of possibilities to arrange plots. In the next post of this ggplot2-tips series we will take a closer look at ggforce. I hope you enjoyed today\u0026rsquo;s blog post and I look forward to \u0026ldquo;see\u0026rdquo; you at my next blog post. In the meantime, feel free to leave a comment or a click on the applause button below.\n","description":"The patchwork and ggforce packages can be used to compose plots from multiple subplots. Let's have a look at how that works.","id":15,"section":"post","tags":["visualization"],"title":"ggplot tips: Arranging plots","uri":"https://albert-rapp.de/post/2021-10-28-extend-plot-variety/"},{"content":"Currently, I am quite curious about interactive plots which is why I am reading Scott Murray\u0026rsquo;s highly recommendable book on D3. For those of you who don\u0026rsquo;t know it, D3.js is a JavaScript library that is great for creating amazing interactive Data-Driven-Documents on the web.\nUnfortunately, compared to ggplot2, D3\u0026rsquo;s learning curve feels quite steep and I am not yet able to work with it yet. Fortunately, there are other interactive graphing libraries out there that I can use to get a first feel for creating interactive plots. For instance, there is another JavaScript library plotly.js, which is built on top of D3 and can be easily used in R through the plotly package.\nTherefore, I decided to play around with this R package in the hope of figuring out how it works. Also, I summarized what I played around with in this blog post by writing what I like to call an \u0026ldquo;exploratory introduction\u0026rdquo;.\nAs the name implies, this is not really a formal introduction to plotly and more of an experience report. Nevertheless, I suspect that this can be useful for people who have already knowledge about ggplot2 and want to get started with plotly as well.\nFurther, in case you do not want to read my informal commentary, you can also check out the video version of this blog post. Think of it as a summary of this blog post.\nAs of now, I plan on doing a similar exploratory introduction to r2d3 which is an R interface to D3. Possibly, this will then be combined with knowledge I gathered from Scott Murray\u0026rsquo;s book.\nOne Side Note before We Dive in According to Wikipedia, JavaScript \u0026ldquo;is one of the core technologies of the World Wide Web\u0026rdquo;. Surprisingly, JavaScript\u0026rsquo;s ubiquity on the interwebs also messed with this blog\u0026rsquo;s theme template such that font formatting was completely destroyed when I included an interactive plotly plot.\nTo work around this issue, I had to save the plots in an auxillary html-file and include it as a separate frame. This is why you will find that, here, all plots are saved in a variable (for later export) even if it does not really make sense to save it for plotting purposes.\nFinally, be aware that, for some reason, interactive plotly plots are quite large such that it might take some time to load them all.\nFrom ggplot2 to plotly Let us begin by transforming a ggplot to a plotly plot. Using a built-in function from the plotly package, it is straight-forward to convert a ggplot.\n1 2 3 4 5 6 7 8  library(tidyverse) library(plotly) p \u0026lt;- mpg %\u0026gt;% ggplot(aes(hwy, cty, fill = class)) + geom_jitter(shape = 21, size = 2, alpha = 0.5) plotly_p \u0026lt;- ggplotly(p) plotly_p   \rMarvel at what one can do here:\n Get additional information by hovering your cursor over a point Filter classes by clicking on the corresponding legend items Click and draw a rectangle with your cursor to zoom into the plot  But there is one thing that bothers me. If I move the cursor across the plot window, then the plotly options bar at the top of the window overlaps the \u0026ldquo;class\u0026rdquo; legend label. Can I fix this by moving the legend to the bottom?\n1 2  p \u0026lt;- p + theme(legend.position = \u0026#34;bottom\u0026#34;) ggplotly(p)   \rHmm, it appears that the legend cannot be moved by this. Do other changes in theme() get registered? Let\u0026rsquo;s check by applying a theme.\n1 2  p \u0026lt;- p + theme_light() ggplotly(p)   \rApparently, at least some changes will be conducted. Let\u0026rsquo;s see, if I can use layout() as shown in the getting started documentation of plotly to move the legend.\nThe R documentation of the layout() function is somewhat minimalistic. Basically, it refers to the online plotly reference manual. Using the table of content on that website, one can easily find the layout options.\nI believe the options xanchor and yanchor are exactly what I need as both of them can be set to options like left, bottom, etc. Though, I wonder why the value auto was not changed by ggplotly() as the initial plot p contains legend.position = \u0026quot;bottom\u0026quot;.\nIn any case, I am not a hundred percent sure what kind of syntax layout() requires. An example in the R documentaion would have been nice. Well, let\u0026rsquo;s try the straight-forward approach then.\n1 2 3 4 5 6  p %\u0026gt;% ggplotly() %\u0026gt;% layout(xanchor = \u0026#34;center\u0026#34;) ## Warning: \u0026#39;layout\u0026#39; objects don\u0026#39;t have these attributes: \u0026#39;xanchor\u0026#39; ## Valid attributes include: ## \u0026#39;font\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;uniformtext\u0026#39;, \u0026#39;autosize\u0026#39;, \u0026#39;width\u0026#39;, \u0026#39;height\u0026#39;, \u0026#39;margin\u0026#39;, \u0026#39;computed\u0026#39;, \u0026#39;paper_bgcolor\u0026#39;, \u0026#39;plot_bgcolor\u0026#39;, \u0026#39;separators\u0026#39;, \u0026#39;hidesources\u0026#39;, \u0026#39;showlegend\u0026#39;, \u0026#39;colorway\u0026#39;, \u0026#39;datarevision\u0026#39;, \u0026#39;uirevision\u0026#39;, \u0026#39;editrevision\u0026#39;, \u0026#39;selectionrevision\u0026#39;, \u0026#39;template\u0026#39;, \u0026#39;modebar\u0026#39;, \u0026#39;newshape\u0026#39;, \u0026#39;activeshape\u0026#39;, \u0026#39;meta\u0026#39;, \u0026#39;transition\u0026#39;, \u0026#39;_deprecated\u0026#39;, \u0026#39;clickmode\u0026#39;, \u0026#39;dragmode\u0026#39;, \u0026#39;hovermode\u0026#39;, \u0026#39;hoverdistance\u0026#39;, \u0026#39;spikedistance\u0026#39;, \u0026#39;hoverlabel\u0026#39;, \u0026#39;selectdirection\u0026#39;, \u0026#39;grid\u0026#39;, \u0026#39;calendar\u0026#39;, \u0026#39;xaxis\u0026#39;, \u0026#39;yaxis\u0026#39;, \u0026#39;ternary\u0026#39;, \u0026#39;scene\u0026#39;, \u0026#39;geo\u0026#39;, \u0026#39;mapbox\u0026#39;, \u0026#39;polar\u0026#39;, \u0026#39;radialaxis\u0026#39;, \u0026#39;angularaxis\u0026#39;, \u0026#39;direction\u0026#39;, \u0026#39;orientation\u0026#39;, \u0026#39;editType\u0026#39;, \u0026#39;legend\u0026#39;, \u0026#39;annotations\u0026#39;, \u0026#39;shapes\u0026#39;, \u0026#39;images\u0026#39;, \u0026#39;updatemenus\u0026#39;, \u0026#39;sliders\u0026#39;, \u0026#39;colorscale\u0026#39;, \u0026#39;coloraxis\u0026#39;, \u0026#39;metasrc\u0026#39;, \u0026#39;barmode\u0026#39;, \u0026#39;bargap\u0026#39;, \u0026#39;mapType\u0026#39;   Well, that didn\u0026rsquo;t go as expected, but the warning displays valid attributes and legend is among them. Upon closer inspection, I also realize that, according to the reference manual, xanchor and yanchor have parent layout.legend. So, I guess, I will have to use this somehow. Maybe, pass a list to legend via layout()?\n1 2 3 4  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list(xanchor = \u0026#34;center\u0026#34;)) p_layout   \rAha! At least this did not crash again but the resulting plot does not look as I would expect it to. Let\u0026rsquo;s see what happens when we change yanchor as well and then we\u0026rsquo;ll go from there.\n1 2 3 4 5 6 7  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list( xanchor = \u0026#34;center\u0026#34;, yanchor = \u0026#34;bottom\u0026#34; )) p_layout   \rUnsurprisingly, this did not help at all. In retrospect, I don\u0026rsquo;t know how I could think that changing yanchor as well would magically cure things. Again, referring back to the manual (I should really read the descriptions instead of relying on the possible values), it appears that xanchor only sets a reference points for the option x. Same thing for yanchor and y.\nSo, how about changing x and y instead? Possible values for both options range from -2 to 3, so my best guess is that ranges from 0 to 1 refer to the window the points are plotted in. Consequently, moving the legend to the bottom could be as easy as using a positive x value and negative y value which are both close to zero.\n1 2 3 4  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list(x = 0.1, y = -0.1)) p_layout   \rNow, this brings us closer to what I had in mind when I set legend.position = \u0026quot;bottom\u0026quot; in theme(). Possibly, we can change the orientation of the legend from vertical to horizontal, tweak the x and y values a bit and then we\u0026rsquo;re there. Scrolling through the manual (again), reveals that there is an option orientation which can be set to h. This sounds promising.\n1 2 3 4 5 6 7 8  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list( x = 0.1, y = -0.2, orientation = \u0026#34;h\u0026#34; )) p_layout   \rNice! Finally, I am satisfied. Out of curiosity, let us investigate what had happened, if we had set orientation to \u0026quot;h\u0026quot; from the start.\n1 2 3 4  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list(orientation = \u0026#34;h\u0026#34;)) p_layout   \rThis already looks nice enough, so our manual tweaking was not technically necessary. But then again, this does not recreate what legend.position = \u0026quot;bottom\u0026quot; usually does. Now that we understand how layout() works, we can roam the reference manual and try to tweak the legend box. Let\u0026rsquo;s try to change a couple of things. This does not have to be pretty, we only want to see how plotly works.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout( legend = list( orientation = \u0026#34;h\u0026#34;, borderwidth = 3, bgcolor = \u0026#34;grey\u0026#34;, bordercolor = \u0026#34;red\u0026#34;, font = list( color = \u0026#34;white\u0026#34;, family = \u0026#34;Gravitas One\u0026#34;, size = 15 ), title = list( text = \u0026#34;Class\u0026#34;, side = \u0026#34;top\u0026#34;, font = list( color = \u0026#34;white\u0026#34;, family = \u0026#34;Gravitas One\u0026#34;, size = 15 ) ) ) ) p_layout   \rNote how we have used lists in lists (in lists) to customize the legend. Interestingly, our initial blunder of ignoring the \u0026ldquo;parent\u0026rdquo; resp. the hierarchy of options earlier helped to understand that as an option\u0026rsquo;s parent\u0026rsquo;s name gets longer, e.g. layout.legend.title.font, we will have to use more convoluted lists to change that option.\nCreating a Plotly Chart Manually Since we have learned how to tweak a plotly object, we might as well figure out how to create one without having to use ggplotly(). It is not that I want to avoid using ggplot altogether but, in principle, it cannot hurt if we can understand plotly\u0026rsquo;s implementation in the R package plotly.\nSo, from the book Interactive web-based data visualization with R, plotly, and shiny I gather that the plotly R package implements the JavaScript plotly.js library via a Grammar of Graphics approach. Thus, it works similar to ggplot2 in the sense that we can add layers of graphical objects to create a plot. In plotly\u0026rsquo;s case, we pass a plotly object from one add_* layer to the next (via %\u0026gt;%).\n1 2 3 4  plt \u0026lt;- mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers(x = ~hwy, y = ~cty, color = ~class) plt   \rNotice the ~. These are used to ensure that the variables are mapped from the data (similar to aes() in ggplot2). Alternatively, one could also use x = mpg$hwy to create the same plot.\nBecause we can see a lot of overplotting, let us jitter the points. Unfortunately, I couldn\u0026rsquo;t find a built-in option for that. Therefore, let\u0026rsquo;s do the jittering manually.\n1 2 3 4 5 6 7 8 9 10 11 12  set.seed(123) jitter_hwy \u0026lt;- 2 jitter_cty \u0026lt;- 1 jittered_mpg \u0026lt;- mpg %\u0026gt;% mutate( hwy = hwy + runif(length(hwy), -jitter_hwy, jitter_hwy), cty = cty + runif(length(cty), -jitter_cty, jitter_cty) ) plt \u0026lt;- jittered_mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers(x = ~hwy, y = ~cty, color = ~class) plt   \rRegarding the customization of the points aka markers, we can pass a list of options (taken from the reference manual again) to the marker argument in add_markers().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  set.seed(123) plt \u0026lt;- jittered_mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers( x = ~hwy, y = ~cty, color = ~class, marker = list( size = 8, opacity = 0.6, line = list(color = \u0026#34;black\u0026#34;, width = 2) ) ) plt   \rAlternatively, and what I find surprisingly convenient, we can leave our initial plotly object as it is and pass it to the style() function. This functions works just like the layout() function we have seen before.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  plt \u0026lt;- jittered_mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers( x = ~hwy, y = ~cty, color = ~class ) %\u0026gt;% style( marker = list( size = 8, opacity = 0.6, line = list(color = \u0026#34;black\u0026#34;, width = 2) ) ) plt   Similarly, we could pass this along to layout() if we want to customize the legend box again.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  plt_layout \u0026lt;- plt %\u0026gt;% layout( legend = list( orientation = \u0026#34;h\u0026#34;, borderwidth = 3, bgcolor = \u0026#34;grey\u0026#34;, bordercolor = \u0026#34;red\u0026#34;, font = list( color = \u0026#34;white\u0026#34;, family = \u0026#34;Gravitas One\u0026#34;, size = 15 ), title = list( text = \u0026#34;Class\u0026#34;, side = \u0026#34;top\u0026#34;, font = list( color = \u0026#34;white\u0026#34;, family = \u0026#34;Gravitas One\u0026#34;, size = 15 ) ) ) ) plt_layout   \rInteresting! For some obscure reason the exact same legend command behaves differently now. Honestly, I have no clue what is going on here. If I had to hazard a guess, I would say that during the conversion of a ggplot object to a plotly object via ggplotly() some default values were implemented that cause the change but this is just a hunch. Possibly, this is connected to xanchor or yanchor.\nIn any case, we got a glimpse of how the plotly R package works which uses the JavaScript library plotly.js to create interactive plots. Also, we have learned how to convert a ggplot2 object to a plotly object and how we can customize this further.\nInterestingly, when converting from ggplot2 to plotly, the pop-up window that appears when you hover over a point is already customized compared to the default from plot_ly(). Did you notice the difference already?\nSo, in order to end our \u0026ldquo;exploratory introduction\u0026rdquo; let us adjust the hovertemplate according to the description in the reference manual. Here, we will use \\n for line breaks.\n1 2 3 4  plt \u0026lt;- plt %\u0026gt;% style(hovertemplate = \u0026#34;hwy: %{x:.2f}\\ncty: %{y:.2f}\u0026#34;) %\u0026gt;% layout(legend = list(orientation = \u0026#34;h\u0026#34;, y = -0.2)) plt   \rNotice how the class labels appear outside of the box. The reference manual refers to this position as \u0026ldquo;secondary\u0026rdquo; box. To get rid of this, we simply add \u0026lt;extra\u0026gt;\u0026lt;/extra\u0026gt; to our hover template. Unfortunately, it appears as if what is not displayed in the primary box cannot be used as part of the hover template.\nThus, we cannot use %{color}. Instead, we simply map class to the text attribute of the markers as well. Then, we can use %{text}.\n1 2 3 4 5 6 7 8 9 10 11 12 13  plt \u0026lt;- jittered_mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers(x = ~hwy, y = ~cty, color = ~class, text = ~class) %\u0026gt;% style( marker = list( size = 8, opacity = 0.6, line = list(color = \u0026#34;black\u0026#34;, width = 2) ) ) %\u0026gt;% style(hovertemplate = \u0026#34;hwy: %{x:.2f}\\ncty: %{y:.2f}\\nclass: %{text} \u0026lt;extra\u0026gt;\u0026lt;/extra\u0026gt;\u0026#34;) %\u0026gt;% layout(legend = list(orientation = \u0026#34;h\u0026#34;, y = -0.2)) plt   \rAll right! That\u0026rsquo;s enough exciting plotting action for today. Hope you enjoyed this blog post and see you next time.\n","description":"We try to do a few simple things with the plotly package in order to figure out how it works.","id":16,"section":"post","tags":["visualization","exploratory-intro"],"title":"An Exploratory Introduction to the Plotly Package","uri":"https://albert-rapp.de/post/2021-10-16-exploratory-intro-plotly/"},{"content":"A bit more than two weeks ago, Germany held a federal election and, naturally, this is always reason for a lot of discussions and subjective truths. One subjective truth I encountered myself related to how fast the party CDU/CSU was able to collect and lose votes according to polls right before the election.\nAccording to the Allensbach Institute, a private polling institute based in Allensbach, Baden-WÃ¼rttemberg, on July 20th, approximately three months before the official election, the CDU/CSU could get 31.5% of the votes1. Almost four weeks later on the 19th of August, the Allensbach institute forecast only 27.5% for the CDU/CSU.\nAt that time, I had the subjective feeling that it was quite common to assume that the CDU/CSU is on a steep downward spiral. In the end, the CDU/CSU was able to slow its downward course and got 24.1% of the votes in the election. While this is still an abysmal outcome for this party, I was surprised that it was not worse.\nA \u0026ldquo;similar\u0026rdquo; surprising tale can be told for other parties too. For instance, the party SPD was gaining a lot of steam in the last three months of the election campaign and the party DIE GRÃNE, once surprisingly popular during the election, lost a lot of votes towards the end as well.\nAll of these ups and downs left a feeling of rapid change for some. For example, last weekend I had an interesting discussion about whether voters no longer cast their votes according to \u0026ldquo;belief\u0026rdquo; but are much more influenced by the spur of the moment and flip-flop back and forth between parties depending on who is making the headlines at that time. Consequently, I decided that this might be something worth looking at with data.\nThus, this blog post tries to look at historic data from election polls to see if this year\u0026rsquo;s change before the election is indeed something unprecedented. If this is so, then that might support that voters become more impulsive. So, this is why I scraped election polls2 since 1998 from the Allensbach institute and the Kantar (Emnid) institute whose election polls can be found publicly here3.\nData Warning I believe it is worth pointing out that the polling results will have to be taken with a grain of salt. Especially the fact that people might judge their current preference differently in non-election years compared to election years has to be taken into account. Obviously, I suspect that the polling institutes considered this as part of their forecast but nevertheless it cannot hurt to mention potential caveats.\nPopularity Over Time This being said, let\u0026rsquo;s take a look at the six (currently) largest parties and their popularity over time. Here, I will only look at the data from the Allensbach institute as these are already quite a lot of data points and the picture might get messy otherwise. As this is an election-focused blog post, I took the liberty of labeling only the election years on the x-axis4.\nSome recent trends are detectable but in this particular figure, I don\u0026rsquo;t see anything that points to an increased volatility in recent times.\nWhat Happens Close to an Election? Instead of looking at the overall fluctuations, we could look at the last three months before an election. Since an election takes places in September, in the next plot I have depicted only the polling results in the months July, August and September in an election year. To make trends more visible, I have added a regression line for each party and each election.\nInterestingly, the most recent election seems to have had more volatile last three months compared to previous elections. Indeed, this could indicate more impulsive voteing behavior but I am not entirely convinced yet.\nThree-Month Volatility To see if the change in the last three months of the most recent election is truly something out of the ordinary, we need context. In order to get this context, let us consult our historical data again and compute the average share of votes for each party in every quarter of every year. Then, hopefully, these computed mean percentages represent the mood of the majority of people in a given quarter and we can see how much these means change from quarter to quarter.\nTaking the fluctuations over time into account, the quarterly change right before this year\u0026rsquo;s election looks less extreme. In fact, most of the parties have had more extreme or similar changes on a quarterly basis in the past.\nClearly, the fact that we aggregate the share of votes over a period of three months could potentially obscure fluctuations. But as the next plot shows, if we do the same thing but aggregate on a monthly basis, then the overall impression of the new monthly plot is the same as with the original quarterly plot.\nCoefficient of Variation Before we try to make sense of what all that we have seen could mean in terms of voters' impulsiveness, let us take one more stab at trying to measure the volatility. This time, let us compute the coefficient of variation (CV)5 of the mean weekly share of votes for each quarter and each party and display this over time.\nExcept for the AfD and SPD, no profound trend in the CV can be detected as most regression lines appear to have a slope that is close to zero. Further, the AfD\u0026rsquo;s decrease in its monthly CV might be explained by the fact that it is a comparatively new party which means that might not have had a solid voter base in the beginning.\nSince we are interested in more recent voter behavior, let us try to take only the data since the beginning of 2013 into account. This should give us an impression about the three most recent elections.\nOverall, as the confidence bands of the regression lines indicate that the real slope of the lines might as well be close to zero, I find it hard to argue either way about a more impulsive voting behavior.\nConclusion So, we have observed something odd here. In previous elections, a party\u0026rsquo;s changes in popularity in the last three months were not as profound as the last quarter\u0026rsquo;s changes that we witnessed this year. Overall, however, the degree of how much changed right before the election is nothing that could not be witnessed in the past at other non-election times.\nIf I had to guess, I would say that this might indicate that over the last 20 years, people\u0026rsquo;s willingness to vote for a different party remained somewhat similar. But it appears like the moment, when voters eventually decide for a party, has been moved closer to the election itself. Thus, one might argue that it has become harder to pinpoint which party one feels most connected to and this leads to an indecision right until the end. This indecision could indeed lead to a flipping back and forth between two or three parties one feels similarly connected to. But this, I would argue, is a sign of people lacking a strong connection to one single party and not one of impulsiveness.\nWhat do you think? Is there more that we can extract from this kind of data? As I am quite new to this type of analysis, I am always glad about suggestions about how to improve. If you want to share your ideas, feel free to send me an e-mail or leave a message in the comment section. As always, if you liked this blog post, I would appreciate a hit on the applause button below.\n The data from the Allensbach institute can be found online here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The resulting data can be found here and the script I used to extract the data is here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n There are a couple of more polling institutes available online but I decided to not scrape all of their results to save time.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n For better legibility, I have tried to use the colors the parties are usually associated with. In some instances, using only one (primary) color resulted in a hard to read plot. Thus, whenever possible, I have consulted online party guidelines to find colors they use in their own publications.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This quantity is defined as the sample standard deviation divided by the sample mean. See also Wikipedia.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"We try to find out if voters in Germany became more impulsive over time.","id":17,"section":"post","tags":[],"title":"Did German Voters Become More Impulsive?","uri":"https://albert-rapp.de/post/2021-10-03-sonntagsfrage/"},{"content":"This week, I had to deal with two very similar tasks on two very similar but not identical data sets that required me to write a function that is versatile enough to deal with both data sets despite their subtle differences. The differences that had to be accounted for mainly related to using functions in the two cases that relied on differently many arguments. Also, some of the column names were different which meant that I could not hard-code the column names into the function I was creating.\nConsequently, I had to use a few non-standard concepts (at least not standard to me) that enabled me to create the function which did everything I asked it to do. Since these concepts seemed interesting to me, I decided to implement a small example resulting in this blog post. Actually, I was even motivated to create a video for this blog post. You can find it on YouTube.\nWhat We Want To Achieve The aim of this example is to write a function that can create two tibbles that are conceptually similar but do not necessarily use the same column names or compute the existing columns in the same way. For this blog post, I have already set up two dummy data sets like that so that we can see what we want to do.\nLet\u0026rsquo;s take a look at these data sets I creatively called dat_A and dat_B.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  library(tidyverse) dat_A %\u0026gt;% head(3) ## # A tibble: 3 x 3 ## mu sigma dat  ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt;  ## 1 -1 1 \u0026lt;tibble [5 x 2]\u0026gt; ## 2 -1 1.5 \u0026lt;tibble [5 x 2]\u0026gt; ## 3 -1 2 \u0026lt;tibble [5 x 2]\u0026gt; dat_B %\u0026gt;% head(3) ## # A tibble: 3 x 2 ## lambda dat  ## \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt;  ## 1 0.5 \u0026lt;tibble [5 x 2]\u0026gt; ## 2 0.7 \u0026lt;tibble [5 x 2]\u0026gt; ## 3 0.9 \u0026lt;tibble [5 x 2]\u0026gt;   As you can see, each tibble contains a column dat. This column consists of tibbles with multiple summarized stochastic processes which were simulated using parameters that are given by the remaining columns of dat_A and dat_B.\nYou probably have already noticed that the stochastic processes must have been simulated using differently many parameters since tibble A contains additional columns mu and sigma whereas tibble B can offer only one additional column lambda. However, even if differently many and differently named parameters are used, the logic of the generating function needs to be the same:\n Take parameters. Simulate stochastic processes with these parameters. Summarize processes  Thus, in step 1 the generating function which we want to code, needs to be versatile enough to handle different argument names and amounts. Next, let\u0026rsquo;s see what the dat column has in store for us.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  dat_A %\u0026gt;% pluck(\u0026#34;dat\u0026#34;, 1) %\u0026gt;% head(3) ## # A tibble: 3 x 2 ## n proc_mean ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 -1.01  ## 2 2 -0.958 ## 3 3 -0.968 dat_B %\u0026gt;% pluck(\u0026#34;dat\u0026#34;, 1) %\u0026gt;% head(3) ## # A tibble: 3 x 2 ## n proc_variance ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 5.27 ## 2 2 2.66 ## 3 3 3.08   First of all, notice that I accessed the first tibble in the dat column using the super neat pluck() function. In my opinion, this function is preferable to the clunky base R usage of $ and [[, e.g. like dat_A$dat[[1]].\nAs you can see, the tibbles that are saved in dat contain columns n and proc_mean resp. proc_variance. As hinted at before, each row is supposed to represent a summary of the n-th realization of a stochastic process.\nHowever, notice that the summary statistics in use are not the same! The different column names proc_mean and proc_variance indicate that in tibble A the sample mean was used whereas tibble B contains sample variances. Again, our function that generates tib_A and tib_B should be flexible enough to create differently named and differently computed columns.\nHelpful Concepts Now that we know what we want to create, let us begin by learning how to handle differently many arguments and their varying names.\ndot-dot-dot For these kinds of purposes, R offers the ...-operator (pronounced dot-dot-dot). Basically, it serves as a placeholder for everything you do not want to evaluate immediately.\nFor instance, have you ever wondered how dplyr\u0026rsquo;s select() function is able to select the correct column?1 If you\u0026rsquo;re thinking \u0026ldquo;No, but what\u0026rsquo;s so special about this?\u0026rdquo;, then you may want to notice that it is actually not that simple to define your own select() function even with the help of the dplyr function.\nThis is because defining an appropriate function to select two columns from, say, the iris data set cannot be done like this:\n1  my_select \u0026lt;- function(x, y) {select(iris, x, y)}   Now, if you want to use the function the same way you would use dplyr::select(), i.e. simply passing, say, Sepal.Width, Sepal.Length (notice no \u0026quot;\u0026quot;) to your new function, it would look like this\n1 2  my_select(Sepal.Width, Sepal.Length) #\u0026gt; Error: object \u0026#39;Sepal.Width\u0026#39; not found   This error appears because at some point, R will try to evaluate the arguments as variables from your current environment. But of course this variable is not present in your environment and only present within the iris data set. Therefore, what dplyr::select() accomplishes is that it lets R know to evaluate the input argument only later on, i.e. when the variable from the data set is \u0026ldquo;available\u0026rdquo;.\nThis is where ... comes into play. It is not by chance that select() only has arguments .data and .... Here, select() uses that everything which is thrown into ..., will be passed along to be evaluated later. This can save our my_select() function, too.\n1 2 3 4 5 6  my_select \u0026lt;- function(...) {select(iris, ...)} my_select(Sepal.Width, Sepal.Length) %\u0026gt;% head(3) ## Sepal.Width Sepal.Length ## 1 3.5 5.1 ## 2 3.0 4.9 ## 3 3.2 4.7   Works like a charm! This will help us to define a function that is flexible enough for our purposes. Before we start with that, let us learn about another ingredient we will use.\ncurly-curly If we were to only select a single column from iris using our my_select() function, we could have also written the function using {{ }} (pronounced curly-curly). It operators similar to ... in the sense that it allows for later evaluation but applies this concept to specific variable. Check out how that can be used here.\n1 2 3 4 5 6  my_select \u0026lt;- function(x) {select(iris, {{x}})} my_select(Sepal.Width) %\u0026gt;% head(3) ## Sepal.Width ## 1 3.5 ## 2 3.0 ## 3 3.2   What\u0026rsquo;s more the curly-curly variables - curly-curlied variables (?) - can also be used later on for stuff like naming a new column. For example, let us modify our previous function to demonstrate how that can be used.\n1 2 3 4 5 6 7 8 9 10  select_and_add \u0026lt;- function(x, y) { select(iris, {{x}}) %\u0026gt;% mutate({{y}} := 5) # 5 can be replaced by some meaningful calculation } select_and_add(\u0026#34;Sepal.Width\u0026#34;, \u0026#34;variable_y\u0026#34;) %\u0026gt;% head(3) ## Sepal.Width variable_y ## 1 3.5 5 ## 2 3.0 5 ## 3 3.2 5   Mind the colon! Here, if you want to use y as column name later on you cannot use the standard mutate() syntax but have to use := instead.\nFunctional Programming One last thing that we will use, is the fact that R supports functional programming. Thus, we can use functions as arguments of other functions. For instance, take a look at this super simple, yet somewhat useless wrapper function for illustration purposes.\n1 2 3 4 5 6  my_simulate \u0026lt;- function(n, func) { func(n) } set.seed(564) my_simulate(5, rnorm) ## [1] 0.4605501 -0.7750968 -0.7159321 0.6882645 -2.0544591   As you just witnessed, I simply passed rnorm (without a call using ()) to my_simulate as the func argument such that rnorm is used whenever func is called. In our use case, this functionality can be used to simulate different stochastic processes (that may depend on different parameters).\nThe Implementation Alright, we have assembled everything we need in order to create our simulate_and_summarize_proc() function. In this example, the simulation of the stochastic processes will consist of simply calling rnorm() or rexp() but, of course, these functions can be substituted with arbitrarily complex simulation functions.\nWe will use n_simus as the amount of realizations that are supposed to be simulated and each realization will be of length TMax. Further, we will use ... to handle an arbitrary amount of parameters that are supposed to be passed to simulation_func. So, let\u0026rsquo;s implement the simulation part first (detailed explanations below).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  simulate_and_summarize_proc \u0026lt;- function(..., TMax, n_simus, simulation_func) { argslist \u0026lt;- list(n = TMax, ...) %\u0026gt;% map(~rep(., n_simus)) tibble( t = list(1:TMax), n = 1:n_simus, value = pmap(argslist, simulation_func) ) } set.seed(457) simulate_and_summarize_proc( mean = 1, sd = 2, TMax = 200, n_simus = 3, simulation_func = rnorm # arguments -\u0026gt; n, mean, sd ) ## # A tibble: 3 x 3 ## t n value  ## \u0026lt;list\u0026gt; \u0026lt;int\u0026gt; \u0026lt;list\u0026gt;  ## 1 \u0026lt;int [200]\u0026gt; 1 \u0026lt;dbl [200]\u0026gt; ## 2 \u0026lt;int [200]\u0026gt; 2 \u0026lt;dbl [200]\u0026gt; ## 3 \u0026lt;int [200]\u0026gt; 3 \u0026lt;dbl [200]\u0026gt;   As you can see, this created three (simple) stochastic processes of length 200 using the parameters mean = 1 and sd = 2. We can validate that the correct parameters were used once we implement the summary functions.\nFirst, let us address the tricky part in this function. In order to pass a list of arguments to pmap() that are then used with simulation_func, we first need to rearrange the lists a bit. After the first step, by simply putting everything from ... into the list we have a list like this:\n1 2 3 4 5  list(n = 100, mean = 1, sd = 2) %\u0026gt;% str() ## List of 3 ## $ n : num 100 ## $ mean: num 1 ## $ sd : num 2   However, we will need to have each variable in the list repeated n_simus time in order to simulate more than one realization. Thus, we use map() to replicate:\n1 2 3 4 5 6 7  list(n = 200, mean = 1, sd = 2) %\u0026gt;% map(~rep(., 3)) %\u0026gt;% str() ## List of 3 ## $ n : num [1:3] 200 200 200 ## $ mean: num [1:3] 1 1 1 ## $ sd : num [1:3] 2 2 2   Note that calling rep() without map() does not cause an error but does not deliver the appropriate format:\n1 2 3 4 5 6 7 8 9 10 11 12 13  list(n = 100, mean = 1, sd = 2) %\u0026gt;% rep(3) %\u0026gt;% str() ## List of 9 ## $ n : num 100 ## $ mean: num 1 ## $ sd : num 2 ## $ n : num 100 ## $ mean: num 1 ## $ sd : num 2 ## $ n : num 100 ## $ mean: num 1 ## $ sd : num 2   Next, let us take the current output and implement the summary. To do so, we will add another variables summary_name and summary_func to the function in order to choose a column name resp. a summary statistic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  simulate_and_summarize_proc \u0026lt;- function(..., TMax, n_simus, simulation_func, summary_name, summary_func) { argslist \u0026lt;- list(n = TMax, ...) %\u0026gt;% map(~rep(., n_simus)) tibble( t = list(1:TMax), n = 1:n_simus, value = pmap(argslist, simulation_func) ) %\u0026gt;% # this part is added unnest(c(t, value)) %\u0026gt;% group_by(n) %\u0026gt;% summarise({{summary_name}} := summary_func(value)) } set.seed(457) simulate_and_summarize_proc( mean = 1, sd = 2, TMax = 200, n_simus = 5, simulation_func = rnorm, summary_name = \u0026#34;mega_awesome_mean\u0026#34;, summary_func = mean ) ## # A tibble: 5 x 2 ## n mega_awesome_mean ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0.955 ## 2 2 0.932 ## 3 3 0.987 ## 4 4 1.07  ## 5 5 1.15   Finally, we can use our super versatile function in combination with map() to create dat_A and dat_B.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  dat_A \u0026lt;- expand_grid( mu = seq(-1, 1, 0.25), sigma = seq(1, 3, 0.5) ) %\u0026gt;% mutate(dat = map2( mu, sigma, ~simulate_and_summarize_proc( mean = .x, sd = .y, TMax = 200, n_simus = 3, simulation_func = rnorm, summary_name = \u0026#34;proc_mean\u0026#34;, summary_func = mean ) )) dat_B \u0026lt;- expand_grid( lambda = seq(0.5, 1.5, 0.2) ) %\u0026gt;% mutate(dat = map( lambda, ~simulate_and_summarize_proc( rate = ., TMax = 200, n_simus = 3, simulation_func = rexp, summary_name = \u0026#34;proc_variance\u0026#34;, summary_func = var ) ))   Conclusion So, we have seen that we can combine {{ }}, ... and functional programming to create highly versatile functions. Of course, as always one might be tempted to say that one could have just programmed two different functions for our particular example.\nHowever, this would cause a lot of code duplication because a lot of steps are essentially the same which is hard to debug and maintain. Also, creating numerous functions does not scale well if we need to cover way more than two cases.\nWith that being said, I hope that you found this blog post helpful and if so, feel free to hit the comments or push the applause button below. See you next time.\n If you have read my YARDS lecture notes and this sounds familiar to you, you are absolutely right. I have reused and adapted a part of the \u0026ldquo;Choose Your Own Data Science Adventure\u0026rdquo;-chapter here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"Using concepts like dot-dot-dot and curly-curly we create functions that are more versatile and can be used in multiple settings.","id":18,"section":"post","tags":[],"title":"Writing Versatile Functions with R","uri":"https://albert-rapp.de/post/2021-09-16-similar-data-and-list-like-columns/"},{"content":"For a long time I have wondered why some people would use position_stack() for position alignment instead of the simpler version position = \u0026quot;stack\u0026quot;. Recently, though, I learned the purpose of the former approach when I tried to add data labels to a stacked bar chart for better legibility.\nFurther, I decided that this knowledge is a good addition to this ggplot2-tips series, so let\u0026rsquo;s see what position_stack() can do. To achieve this, let us create a small dummy data set.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  library(tidyverse) dummy_dat \u0026lt;- tibble( group = c(rep(\u0026#34;A\u0026#34;, 3), rep(\u0026#34;B\u0026#34;, 3)), category = factor( c(rep(c(\u0026#34;low\u0026#34;, \u0026#34;medium\u0026#34;, \u0026#34;high\u0026#34;), 2)), levels = rev(c(\u0026#34;low\u0026#34;, \u0026#34;medium\u0026#34;, \u0026#34;high\u0026#34;)), ), percent = c(0.41, 0.16, 1 - 0.41 - 0.16, 0.26, 1 - 0.26 - 0.36, 0.36) ) dummy_dat ## # A tibble: 6 x 3 ## group category percent ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 A low 0.41 ## 2 A medium 0.16 ## 3 A high 0.43 ## 4 B low 0.26 ## 5 B medium 0.38 ## 6 B high 0.36   Next, take a look at the corresponding stacked bar chart. Since we created a dataset that contains percentages, I took the liberty of appropriately transforming the y-axis via scale_y_continuous().\n1 2 3 4  dummy_dat %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + scale_y_continuous(labels = scales::percent_format())   I believe that this visualization could be improved by adding text labels to each part of the stacked bar chart in order for the reader to immediately detect how large each portion of the bars is. Let\u0026rsquo;s try this via simply converting the values to strings and adding geom_text() to the plot.\n1 2 3 4 5 6  dummy_dat %\u0026gt;% mutate(percent_labels = scales::percent(percent)) %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + geom_text(aes(label = percent_labels)) + scale_y_continuous(labels = scales::percent_format())   Clearly, this did not work as intended because geom_text() uses position = \u0026quot;identity\u0026quot; by default which is why the y-position of the labels is simply determined by its value. Now, here is where I would usually change the positioning via position = \u0026quot;stack\u0026quot;. However, the result this approach delivers is somewhat less than perfect.\n1 2 3 4 5 6  dummy_dat %\u0026gt;% mutate(percent_labels = scales::percent(percent)) %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + geom_text(aes(label = percent_labels), position = \u0026#34;stack\u0026#34;) + scale_y_continuous(labels = scales::percent_format())   Ideally, I would like the labels to appear in the middle of each colored block. We could try to use vjust to move the labels which is not a great idea since every label will be moved by the same amount and the blocks are of different height. Similarly, we could compute the block middle points by hand and use that as separate y-aesthetic in geom_text().\nClearly, this involves a tedious additional computation and we should avoid this, if possible. This is precisely where position_stack() comes in. Conveniently, using position = position_stack() stacks the bars just like position = \u0026quot;stack\u0026quot; does but the function position_stack() has another argument vjust by which we can move the labels individually.\nHere, the possible values of vjust range from 0 (bottom of the designated height) to 1 (top of the designated height). Therefore, moving the labels to the middle of each bar is as easy as setting vjust = 0.5.\n1 2 3 4 5 6 7 8 9  dummy_dat %\u0026gt;% mutate(percent_labels = scales::percent(percent)) %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + geom_text( aes(label = percent_labels), position = position_stack(vjust = 0.5) ) + scale_y_continuous(labels = scales::percent_format())   Finally, one may - and this is definitely a matter of taste - tweak this plot further by changing the color and text formatting. Personally, I like darker colors combined with a white, bold label. In this case, this would look like this.\n1 2 3 4 5 6 7 8 9 10 11 12  dummy_dat %\u0026gt;% mutate(percent_labels = scales::percent(percent)) %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + geom_text( aes(label = percent_labels), position = position_stack(vjust = 0.5), col = \u0026#34;white\u0026#34;, fontface = \u0026#34;bold\u0026#34; ) + scale_y_continuous(labels = scales::percent_format()) + scale_fill_brewer(palette = \u0026#34;Set1\u0026#34;)   In summary, we have seen that using position = position_stack() is a more powerful alternative to position = \u0026quot;stack\u0026quot; that allows individual positioning. Nevertheless, as long as the additional arguments of position_stack() are not needed I still find the latter version simpler.\n","description":"We take a look at the differences between position = 'stack' and position = position_stack().","id":19,"section":"post","tags":["visualization"],"title":"ggplot tips: Using position_stack() for Individual Positioning","uri":"https://albert-rapp.de/post/2021-09-11-position-adjustment/"},{"content":"This blog post is part of a series I am creating where I collect tips I found useful when I first learned to work with ggplot2. All posts which are part of this series can be found here. In this post I want to deal with how to manually or automatically create labels for some aesthetic.\nManually Assigning Labels Assigning labels by hand, e.g. via col = \u0026quot;some label\u0026quot;, can be a great idea in some instances. For example, when you use two different smoothing methods, a hand-written label to differentiate between the two methods helps a lot. For instance, take a look the relationship between city mileage cty and highway mileage hwy of cars in the mpg data set from the ggplot2 package.\n1 2 3  library(tidyverse) ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5)   If one suspects a linear relationship between those two variables, one might want to use geom_smooth(method = 'lm') to check that hypothesis by drawing a straight line through the points. Similarly, one may be inclined to see what geom_smooth() would return if a linear model is not enforced. Adding both smoothing methods to the plot (and removing the confidence bands) yields:\n1 2 3 4  ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5) + geom_smooth(se = F, size = 1.5) + geom_smooth(method = \u0026#39;lm\u0026#39;, se = F, size = 1.5)   Obviously, differently colored lines should be used here to differentiate between the two smoothing methods. We have two approaches to do this. Either, we can manually assign a color (without using aes()):\n1 2 3 4  ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5) + geom_smooth(se = F, size = 1.5, col = \u0026#39;red\u0026#39;) + geom_smooth(method = \u0026#39;lm\u0026#39;, se = F, size = 1.5, col = \u0026#39;blue\u0026#39;)   Or we can use aes() and assign labels instead and let ggplot2 handle the colors on its own.\n1 2 3 4 5  ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5) + geom_smooth(aes(col = \u0026#39;auto\u0026#39;), se = F, size = 1.5) + geom_smooth(method = \u0026#39;lm\u0026#39;, aes(col = \u0026#39;lm\u0026#39;), se = F, size = 1.5) + labs(col = \u0026#39;Smoothing\u0026#39;)   Personally, I prefer the latter approach because it has a couple of small advantages\n A legend is automatically generated with the corresponding labels such that even without looking at the code it becomes more obvious how each line was generated. Also, creating labels for an aesthetic is kind of the point of this post. I do not have to bother about the specific color names. For me, this is something that could take up a lot of time if I want to change the appearance of the plot later on because I might spend way too much time on finding colors that \u0026ldquo;work\u0026rdquo; together. Here, if I want to change the colors, I could simply use a Brewer color palette and hope that the creators of that palette had good reasons to arrange the palette the way they did.  1 2 3 4 5 6  ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5) + geom_smooth(aes(col = \u0026#39;auto\u0026#39;), se = F, size = 1.5) + geom_smooth(method = \u0026#39;lm\u0026#39;, aes(col = \u0026#39;lm\u0026#39;), se = F, size = 1.5) + labs(col = \u0026#39;Smoothing\u0026#39;) + scale_color_brewer(palette = \u0026#39;Set1\u0026#39;)   Automatically Assigning Labels via Pivoting Sometimes, manually coloring aspects of your data can also be a bad idea. Especially, if you find yourself using the exact same geom_* multiple times on different variables of a data set, you may want to think about using a different approach. One such approach can be to rearrange the data first. For example, take a look at the following two time series that were simulated and collected in a tibble as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  set.seed(123) x1 \u0026lt;- rnorm(10) x2 \u0026lt;- rnorm(10) tib \u0026lt;- tibble( t = seq_along(x1), ts1 = cumsum(x1), ts2 = cumsum(x2) ) tib ## # A tibble: 10 x 3 ## t ts1 ts2 ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 -0.560 1.22 ## 2 2 -0.791 1.58 ## 3 3 0.768 1.98 ## 4 4 0.839 2.10 ## 5 5 0.968 1.54 ## 6 6 2.68 3.33 ## 7 7 3.14 3.82 ## 8 8 1.88 1.86 ## 9 9 1.19 2.56 ## 10 10 0.746 2.09   Now, it is possible to plot both times series using geom_line() and use different colors for each line. To do so, one might be tempted (as I often was when I first learned ggplot2) to write code similar to the one we wrote earlier:\n1 2 3 4  tib %\u0026gt;% ggplot(aes(x = t)) + geom_line(aes(y = ts1, col = \u0026#34;Time series 1\u0026#34;)) + geom_line(aes(y = ts2, col = \u0026#34;Time series 2\u0026#34;))   Here, we basically used geom_line() twice for more or less the same plot but with only one aesthetic slightly changed. However, this may not be the best approach. This is especially true if we were to do this for, say, 100 time series as it would involve a lot of code duplication.\nInstead, let\u0026rsquo;s try to rearrange the data via pivot_longer() before even beginning to plot anything1. This way, we might even plot way more than 2 time series with only a single geom_line():\n1 2 3 4 5 6 7 8 9 10 11  set.seed(123) # Create multiple time series tib \u0026lt;- map_dfc(1:6, ~cumsum(rnorm(10))) %\u0026gt;% rename_with(~glue::glue(\u0026#34;label{1:6}\u0026#34;)) %\u0026gt;% bind_cols(t = 1:10, .) # Pivot and plot tib %\u0026gt;% pivot_longer(cols = -1, names_to = \u0026#34;ts\u0026#34;) %\u0026gt;% ggplot(aes(t, value, col = ts)) + geom_line()   As you just saw, it is also possible to, if necessary, relabel the column names in bulk before rearranging the data in order to label the aesthetic the way we want.\nSame Procedure, Different Aesthetic For the sake of an additional example, let us use the same ideas but with geom_boxplot() instead of geom_line(). Therefore, we will generate a couple of \u0026ldquo;data sets\u0026rdquo; and plot a box plot for each one:\n1 2 3 4 5 6  set.seed(123) map_dfc(1:6, rnorm, n = 100) %\u0026gt;% rename_with(~(1:6)) %\u0026gt;% pivot_longer(cols = everything(), names_to = \u0026#34;ds\u0026#34;) %\u0026gt;% ggplot(aes(col = ds, y = value)) + geom_boxplot()   Here, I have used col again but as I have recently come to realize, using fill instead of col creates the \u0026ldquo;prettier\u0026rdquo; box plots so let\u0026rsquo;s use that instead.\n1 2 3 4 5 6  set.seed(123) map_dfc(1:6, rnorm, n = 100) %\u0026gt;% rename_with(~(1:6)) %\u0026gt;% pivot_longer(cols = everything(), names_to = \u0026#34;ds\u0026#34;) %\u0026gt;% ggplot(aes(fill = ds, y = value)) + geom_boxplot()   So, as you just witnessed, what I have described so far does not only work with the color aesthetic. In fact, we can pretty much use the same approaches for all other aesthetics.\nThus, we have seen how to easily create labels for an aesthetic of our choice by either manually assigning labels or rearranging the data first in order to use the previous column names to assign labels automatically. Let me know what you think in the comments or if you liked this post, simply hit the applause button below.\n I am not describing how pivot_longer() works in detail here because I want to keep this post short by only \u0026ldquo;connecting the dots\u0026rdquo;. If you are unfamiliar with pivoting, you may check out the tidy data chapter from my YARDS lecture notes which was of course inspired by the infamous R for Data Science book. For an animation that demonstrates what pivot_longer() and pivot_wider() do, see gadenbuie/tidyexplain on GitHub.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"We talk about how to easily create labels for an aesthetic.","id":20,"section":"post","tags":["visualization"],"title":"ggplot tips: Assigning Labels to an Aesthetic","uri":"https://albert-rapp.de/post/2021-08-19-aesthetic-labels/"},{"content":"It is not that long ago when I first encountered ggplot2 and decided to learn how to use it1. By no means do I think that I have sufficiently mastered this package yet but as time has passed I have certainly picked up a few tips on my journey to get better at creating more meaningful visualizations. So, in order to remind myself of and share what I learned, I decided to create a sort of series containing tips that enhanced my visualization skills.\nHowever, this is not supposed to be an intro to ggplot2 though. I have already done that and you can find it in the data exploration chapter of my \u0026ldquo;Yet Again: R + Data Science\u0026rdquo; lecture notes (see YARDS). Currently, I plan to make each installment of the series somewhat short to keep it simple and all further posts in this series will be collected under the ggplot2-tips series tag which you can also access from this blog\u0026rsquo;s main page. So, without further ado, let us begin.\nUsing log-transforms Often, one finds variables in a data set that resemble heavy-tailed distributions and you can detect it by a simple histogram in a lot of cases. For instance, take a look at the variable sale_price of the ames dataset from the modeldata package. This variable contains the sale price of 2930 properties in Ames, Iowa and its histogram looks like this:\n1 2 3 4 5 6 7 8 9  library(tidyverse) library(modeldata) data(ames) # I like to clean names s.t. no capital letters are used in the variable names ames \u0026lt;- ames %\u0026gt;% janitor::clean_names() ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram()   As you can see, the distribution looks skewed in the sense that most of the sale prices fall within one range but there are also sale prices that are comparatively high. In effect, the histogram depicts a \u0026ldquo;long tail\u0026rdquo; and the highly priced sales are not that easily discernible in the histogram as the column heights may become really small and there may be large \u0026ldquo;gaps\u0026rdquo; between columns as seen above.\nOne common way to deal with this is to apply a logarithm to the variable of interest. It does not really matter which logarithm you use but since we like to work in a decimal system, a logarithm with base 10 is often used. Let\u0026rsquo;s see how this changes the picture.\n1 2 3  ames %\u0026gt;% ggplot(aes(x = log(sale_price, 10))) + geom_histogram()   Admittedly, we have a gap in the histogram on the left hand side now but overall the histogram looks way less skewed. In fact, this somewhat resembles what a histogram of a normally distributed random variable could look like. This is nice because Gaussian variables are something which a lot of statistical techniques can work with best.\nThus, working with a logarithmized variable might be helpful in the long run. Note that sometimes a variable benefits from being logarithmized but also contains values that are zero. To apply the logarithm anyway, often one then offsets the variable by shifting the variable by 1.\nUnfortunately, it may be nice that logarithmized variables are beneficial for statistical techniques and that the histograms are less skewed but the way we achieved that in the above example let\u0026rsquo;s the audience of the visualization somewhat clueless as to what the actual sale prices were. Sure, if in doubt, one could simply use a calculator to compute \\(10^{4}\\) and \\(10^{6}\\) to get a feeling for the range of the sale prices but of course no one will want to do that. This brings me to my next point.\nUse scale_*_log10() Honestly, I don\u0026rsquo;t know why but for a long time I have logarithmized data for visualization purposes as above because using scale_x_log10() felt somewhat frightening because I did not understand what was going on there. Take a look what happens if I add this particular layer to our initial plot instead of logarithmizing manually.\n1 2 3 4  ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram() + scale_x_log10()   Notice that the overall impression of the picture is the same as with the manually logarithmized plot. However, the x-axis is now logarithmized as opposed to being linear. So, manual logarithmization of the variable leads to just that: A transformation of the data but the axis in the plot remains linear which is why the labels on that x-axis showed values that needed to be retransformed to its original values.\nIn contrast, using scale_x_log10() leaves the data as it is but transforms the x-axis. In this case, this new axis is used for binning and counting to compute the histogram. Therefore, we can easily see that the majority of the sale prices lie between 100,000 and 300,000. Of course, things would be even simpler if the axis labels were not given in scientific notation. Luckily, we can easily change that.\nAdjust labels using the scales package As its name says, the scales package works really well in conjunction with the scale_* layers from ggplot2. In fact, this can make it somewhat comfortable to quickly adjust axis labels by simply passing a function (mind the ()) from the scales package to the scale_* layer\u0026rsquo;s argument labels. Here, we may simply use label_number() to get rid of the scientific notation.\n1 2 3 4  ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram() + scale_x_log10(labels = scales::label_number())   Even better, scales has a lot of functions that are useful for specific units such as dollar or week, month, year (in case you are working with time data whose labels can be a special kind of pain).\n1 2 3 4  ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram() + scale_x_log10(labels = scales::label_dollar())   Of course, the same thing works not only for the x-axis scale but for all kinds of other scales too. For instance, if you want to plot the same histogram but oriented vertically, you can simply change the x-aesthetic to be the y-aesthetic which means that you will need to adjust the y scale then.\n1 2 3 4  ames %\u0026gt;% ggplot(aes(y = sale_price)) + geom_histogram() + scale_y_log10(labels = scales::label_dollar())   In retrospect, it is really easy to adjust the axis with scale_* layers and the scales package and I really do not know why I hesitated in the past to use these functions. I guess adding another layer to the plot felt somewhat harder and slower than brute-forcing my way through the transformation. But believe me, in the long run this takes up way more of your time (especially if you want to interpret your plot later on).\nI hope that you enjoyed this post and if you did, feel free to hit the applause button below. In any case, I look forward to see you in the next installment of this series.\n Fun fact: Actually I somehow disliked R at first (to be fair I was not a frequent user of R back then anyway) but ggplot2 changed that and motivated me to do more in R.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"This is the beginning of a series about a few ggplot2 tricks I picked up along the way. In this first installment we talk about how logarithmizing scales can be beneficial.","id":21,"section":"post","tags":["visualization"],"title":"Beginning a ggplot2 Series: Logarithmize Your Scales","uri":"https://albert-rapp.de/post/2021-08-07-a-few-ggplot-tips/"},{"content":"Recently, I decided to try out a few stretches in an effort to stay in shape during long stretches of working from home and not leaving the house. Also, to distract me from my inflexible body I thought I would watch a video on YouTube simultaneously and as luck would have it, I saw an interesting video on Veritasium\u0026rsquo;s YouTube Channel called \u0026ldquo;Is Success Luck or Hard Work\u0026rdquo;. Personally, I agree with a lot of things being said in that video and I recommend that you check out the video if you want to get a perspective on the role of luck compared to skill (or keep on reading for my own take on this topic).\nBut more importantly, in the video Derek Muller - the guy behind Veritasium - describes a simulation he ran in order to hint at whether luck played a role in the selection of 11 out of 18300 applicants in 2017 for the NASA astronaut training program. The underlying model that is simulated in the video is described as assuming that astronauts are selected mostly based on their skill. However, 5% of the decision is also based on luck.\nThis sounds a little bit vague, so Muller elaborates: First, each applicant is assigned a random skill score (out of 100) and a luck score (out of 100). Then, the weighted sum of the two scores result in an applicant\u0026rsquo;s overall score (the weights being of course 95% for skill and 5% for luck). Finally, the top 11 applicants according to this overall score will then go on to become astronauts.\nNow, based on a thousand runs of this simulation what Veritasium finds is that it was the very lucky applicants who were selected. More precisely, the average luck score of the picked applicants was 94.7. Similarly, on average out of the 11 picked astronauts only 1.6 applicants would have been the same had the selection process been based on skill alone.\nSo, as I was watching this video, I noticed two things. One, I am really inflexible and I need to stretch more and two, this simulation sounds pretty cool and I bet I could recreate this simulation quite easily. Thus, an idea for a new blog post was born.\nThe Original Approach Later on, I want to tweak the above approach a bit but for now let us simulate the process as described above. First, we will need a function to generate the applicants' scores. Here, I want to generate the luck score according to a uniform distribution on \\((0, 100)\\) because I assume that we are all lucky in a similar way regardless of what the position we apply for is.\nHowever, I think it is conceivable that for highly specialized jobs (such as astronauts) only really skilled applicants show up whereas jobs that fall more in the \u0026ldquo;jack of all trades\u0026rdquo; category may attract applicants from all kinds of skill ranges. This will, of course, affect the distribution of skill scores and I wonder if this has a significant effect on the overall results. Therefore, I will make sure that the score generating function has the ability to use different skill distributions. Finally, let us add an option to change the skill-luck ration which we will first set to its default value of 5%.1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  library(tidyverse) simulate_applicants \u0026lt;- function(n, dist, luck_ratio = 0.05) { tibble( skill = dist(n), luck = runif(n, min = 0, max = 100), overall = (1 - luck_ratio) * skill + luck_ratio * luck ) } set.seed(123) simulate_applicants(5, function(x) rnorm(x, 50, 1)) ## # A tibble: 5 x 3 ## skill luck overall ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 49.4 95.7 51.8 ## 2 49.8 45.3 49.5 ## 3 51.6 67.8 52.4 ## 4 50.1 57.3 50.4 ## 5 50.1 10.3 48.1   Obviously, each column represents the respective score for each applicant. Regarding the skill scores I propose a couple of different distributions:\n Uniform distribution: We assume that applicants come from all kinds of skill ranges and all skill levels are equally likely. Normal distribution: We assume that most people fall within a medium skill range which we model by a normal distribution with mean 50 and standard deviation \\(50/4\\) so that for our 18000 astronauts chances are very slim that one of them falls outside the range (due to the empirical rule of the normal distribution). High specialization: To cover the scenario of only highly skilled applicants, let us use a beta distribution \\(X \\sim \\text{Beta}(1.2, 10)\\) and use the transformed variable \\(100(1-X)\\) as skill distribution.2 Low Specialization: Similarly, let us use \\(100X\\) where \\(X \\sim \\text{Beta}(1.2, 10)\\) to simulate a scenario in which mostly applicants with a low skill score occur.  The corresponding functions that realize the distributions for us are given as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  normal_skills \u0026lt;- function(n) { rnorm(n, mean = 50, sd = 50 / 4) %\u0026gt;% # Make sure that score stays in bounds pmax(0) %\u0026gt;% pmin(100) } uniform_skills \u0026lt;- function(n) { runif(n, min = 0, max = 100) } high_skills \u0026lt;- function(n) { 100 * (1 - rbeta(n, 1.2, 10)) } low_skills \u0026lt;- function(n) { 100 * rbeta(n, 1.2, 10) }   To illustrate the high and low skill distribution, let us take a look at the densities of the beta distributions in question.\nNow, let us write a function that runs a single iteration of the selection process and marks applicants as either selected or not. We will denote the number of applicants to be picked via m. In our astronaut example it holds that \\(m = 11\\).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  pick_applicants \u0026lt;- function(n, dist, m = 11) { applicants \u0026lt;- simulate_applicants(n, dist) applicants %\u0026gt;% arrange(desc(overall)) %\u0026gt;% mutate(selected = c(rep(\u0026#39;yes\u0026#39;, m), rep(\u0026#39;no\u0026#39;, n - m))) } n \u0026lt;- 18300 pick_applicants(n, normal_skills) ## # A tibble: 18,300 x 4 ## skill luck overall selected ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  ## 1 98.1 11.3 93.8 yes  ## 2 97.4 5.56 92.8 yes  ## 3 93.1 72.9 92.1 yes  ## 4 93.2 60.5 91.6 yes  ## 5 90.5 94.4 90.7 yes  ## 6 94.3 15.4 90.4 yes  ## 7 89.1 88.6 89.1 yes  ## 8 89.4 76.1 88.7 yes  ## 9 87.7 83.8 87.5 yes  ## 10 88.0 75.2 87.3 yes  ## # ... with 18,290 more rows   Using this function, we can easily simulate the whole application process a couple of times Monte Carlo style.\n1 2 3 4 5 6 7 8 9  set.seed(123) N \u0026lt;- 1000 simus \u0026lt;- expand_grid( dist = c(\u0026#34;uniform_skills\u0026#34;, \u0026#34;normal_skills\u0026#34;, \u0026#34;low_skills\u0026#34;, \u0026#34;high_skills\u0026#34;), simuID = 1:N ) %\u0026gt;% mutate(applicants = map(dist, ~pick_applicants(n, get(.)))) %\u0026gt;% unnest(applicants) %\u0026gt;% filter(selected == \u0026#39;yes\u0026#39;)   Having created a tibble simus that contains information on the skill and luck scores of success candidates for each simulation run, we can visualize the distribution of the luck scores of the successful candidates using boxplots. The idea behind that is that if luck is important, then the boxplots should differ from that of a standard uniform distribution on \\((0, 100)\\).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  library(grid) library(gridExtra) library(gtable) avgs \u0026lt;- simus %\u0026gt;% group_by(dist) %\u0026gt;% summarize(avg_luck = mean(luck)) %\u0026gt;% arrange(desc(avg_luck)) %\u0026gt;% mutate( avg_luck = scales::comma(avg_luck, accuracy = .01), dist = case_when( dist == \u0026#34;high_skills\u0026#34; ~ \u0026#34;High skills\u0026#34;, dist == \u0026#34;low_skills\u0026#34; ~ \u0026#34;Low skills\u0026#34;, dist == \u0026#34;uniform_skills\u0026#34; ~ \u0026#34;Uniform skills\u0026#34;, dist == \u0026#34;normal_skills\u0026#34; ~ \u0026#34;Normal skills\u0026#34; ) ) colorScale \u0026lt;- glue::glue(\u0026#34;dodgerblue{1:4}\u0026#34;) avgTable \u0026lt;- tableGrob( avgs, rows = NULL, cols = NULL, theme = ttheme_minimal( core=list(fg_params = list(col = colorScale, fontface = 2)) ) ) simus %\u0026gt;% mutate(dist = fct_reorder(dist, luck)) %\u0026gt;% ggplot(aes(y = dist, x = luck, col = dist)) + geom_boxplot(show.legend = F) + theme_classic() + labs( x = \u0026#34;Luck\u0026#34;, y = element_blank(), title = \u0026#34;Luck Distribution Among Successful Applicants\u0026#34;, subtitle = \u0026#34;(Average luck score depicted in the table)\u0026#34; ) + annotation_custom(avgTable, xmin = 20, xmax = 60, ymin = 2, ymax = 5) + scale_y_discrete(breaks = NULL) + scale_color_manual(values = rev(colorScale)) + theme(axis.line.y = element_blank(), plot.subtitle = element_text(size = 10))   Here, we added a table of the average luck score of the successful applicants to the boxplots that summarize the distribution of their luck scores. As it turns out, we can clearly see differences between the luck distribution of the successful candidates across the different skill distributions. Interestingly, the uniform skill distribution comes quite close to the average value Veritasium finds in his video, so I guess we can assume that he probably used that skill distribution.\nI would say that the key takeaway of this picture is that the more specialized your area of expertise is, the luckier you have to be if you have many similarly skilled competitors. In a way, this makes sense. If you and your competition is basically at the top of the game and there is not much room to differentiate candidates w.r.t. skill, then luck may just be the deciding factor.\nInterestingly, the same holds true when skills are uniformly distributed among the whole range. Further, when the skill distribution of you and your competition looks more normally distributed or if the skill scores of all applicants are rather low, then you do not need to be extremely lucky (at least not as much as before). But still, you have to be more lucky than the average applicant (recall that luck is modeled via a uniform distribution here with mean 50).\nSo, in a sense this might mean that if chances are good that there are a couple of applicants who are as good as it gets, i.e. at the maximum of the skill range (which is the case for the high and uniform skills), then the successful candidates are indeed really lucky. Similarly, if chances are low that some applicants are the best of the best (normal and low skills), then successful applicants are luckier than the average Joe but in a less extreme way than in the previous example. In total, it looks like luck plays a role in either case.\nFinally, to break this scenario down to something more realistic with a higher available spots to applicants ratio I ran the same simulation but with only one available position and 100 applications.3 The results look similar but as expected if their are less applicants for one position, then luck plays a lesser but still important role.\nOvercoming Threshold Approach There is this old joke where a recruiter in an HR department gets a large stack of applications for a specific position within the company. The recruiter immediately begins to work on the applications by simply taking half of the applications and throwing them into the trash because he \u0026ldquo;does not hire unlucky people\u0026rdquo;.\nThis may be an extreme action but then again I have heard that some companies immediately reject an application if they see only a single typo in the documents. To me, this is similar to what the recruiter from the joke is doing because a typo can happen by accident to even the most careful person regardless of their skill. So, let us use this as a way to construct another hypothetical scenario.\nIn this scenario, an application goes through two stages. In the first stage, the application is either is moved to the second stage or is rejected due to some arbitrary small reason. By this we mean some small event based on luck which we will model here with our luck score from above. In the second stage, everything is judged purely on skill alone, i.e. the person with the highest skill score gets the job.\nLet us write a new function for this second hypothetical scenario. Notice that this new function ranks applicants according to their skill score, i.e. the most skilled applicant is ranked as 1, the second most skilled applicant is ranked as 2 and so on.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  pick_applicants2 \u0026lt;- function(n, dist, m = 11, luck_thresh = 50) { applicants \u0026lt;- simulate_applicants(n, dist) applicants \u0026lt;- applicants %\u0026gt;% mutate(skill_rank = min_rank(desc(skill))) %\u0026gt;% filter(luck \u0026gt; luck_thresh) applicants %\u0026gt;% arrange(desc(skill)) %\u0026gt;% mutate(selected = c(rep(\u0026#39;yes\u0026#39;, m), rep(\u0026#39;no\u0026#39;, nrow(applicants) - m))) } pick_applicants2(n, high_skills) ## # A tibble: 49 x 5 ## skill luck overall skill_rank selected ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;  ## 1 99.9 87.4 99.3 1 yes  ## 2 99.1 54.6 96.9 3 yes  ## 3 98.8 54.5 96.6 5 yes  ## 4 98.8 97.5 98.7 6 yes  ## 5 98.2 90.6 97.8 10 yes  ## 6 97.6 60.8 95.8 12 yes  ## 7 97.0 87.0 96.5 13 yes  ## 8 96.8 71.9 95.5 14 yes  ## 9 96.7 97.0 96.8 16 yes  ## 10 96.6 55.1 94.5 18 yes  ## # ... with 39 more rows   Afterwards, we can run a similar simulation as before. However, this time we will take a look at the distribution of the ranks of the successful candidates. Here are the results for our astronaut scenario, i.e. 18300 applicants with 11 open positions.\nAs you can see, the distribution of the skill ranks of the successful applicants do not vary much across the skill distributions. Also, it looks like the average ranks are simply the amount of positions plus 1. This is somewhat unsurprising since we throw out 50% of the applicants randomly in the first stage of the process.\nNevertheless, this reinforces the idea that not necessarily the most skilled person gets hired to do the job. What is even more surprising to me is the fact that in the current scenario there is still a chance of around 8% that a successful applicant has a skill rank above 22 (recall that there are 11 open positions). Now, running the same analysis for 100 applicants for 1 job again yields similar results.\nIn these 1000 simulations, the chances that a successful applicant has a skill score of 5 or above are similarly high but depend on the skill distribution.\n## # A tibble: 4 x 3\r## # Groups: dist [4]\r## dist n prop ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;\r## 1 Normal skills 82 8.2% ## 2 Uniform skills 69 6.9% ## 3 High skills 65 6.5% ## 4 Low skills 58 5.8%\rSummary So, in our simulations we have seen that luck plays a role in whether an applicant gets a job. Further, this actually depended on the skill distribution of the applicants and the impact of luck was most strongly pronounced in the cases when there was a good chance that highly skilled people are present among the applicants. Probably, this is the most relevant scenario anyway since we all like to believe that we are skilled but are still somewhat more proficient than other skilled people.\nOf course, in terms of real world implications, this simulation can only give anecdotal evidence towards the hypothesis that luck plays a large role in success. Also, we made a lot of assumptions in our simulations that might be debatable. But, personally, I like to believe that luck cannot be neglected in the end and even the most skilled person may want to be grateful when he gets accepted for a job. If you wish to share your opinion on that, feel free to leave a comment. Finally, if you thought this post was interesting, I would also appreciate it if you clicked the applause button (just so that it feels less like I am talking into a void).\n When I first started to write this blog post, I wanted to investigate how results change for different ratios but this post is already long enough. Thus, I decided to simply leave that functionality unused but someone interested in recreating parts of this simulation may tweak that parameter.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Here, the parameters of the beta distribution were picked so that the densities somewhat fit the desired description as will be seen in the plot a few lines ahead. Of course, one could potentially change the parameters but here I decided to go with this.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I tried to research what is a realistic number of applicants for a given positions but results were varying and I found claims of less than 100 applicants on average and way more than 100 applicants. So, here I decided to go with something in between.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"I recreate a simulation study on the influence of luck on success compared to the influence of skill.","id":22,"section":"post","tags":["simulation"],"title":"Is Success Luck or Hard Work?","uri":"https://albert-rapp.de/post/2021-07-26-luck-vs-skill/"},{"content":"For my first post on this blog I decided to create an animation using the animation package. To give this animation some purpose let me demonstrate how kernel density estimators work with the help of an animation.\nIn general, kernel density estimators are, as is kind of obvious by the name, used to estimate the underlying density of a random sample. For instance, imagine that we have a sample drawn from an some unknown distribution.\n1 2  n \u0026lt;- 100 sample \u0026lt;- rexp(n)   Then, assuming that we do not actually know that the current sample was drawn from an exponential distribution, we might want to estimate the density and see if the estimate fits to some well-known parametric family of distributions. With the help of ggplot() and geom_density() this is straightforward.\n1 2 3  library(tidyverse) ggplot() + geom_density(aes(x = sample))   The underlying procedure to generate the plot it to use a kernel density estimator \\(\\hat{f}_h\\) in order to estimate the true underlying density \\(f\\) by using the formula $$ \\hat{f}_h(x) = \\frac{1}{nh} \\sum_{k = 1}^{n} K\\Big(\\frac{x - x_k}{h}\\Big) $$ for all \\(x \\in \\mathbb{R}\\) where \\(n\\) is the sample lengh and \\(h \u0026gt; 0\\) is a smoothing parameter that needs to be chosen and \\(K\\) is a \u0026ldquo;suitable\u0026rdquo; function. Usually, this parameter \\(h\\) is called bandwidth and \\(K\\) is called a kernel function which is often to be the density of a probability distribution.\nThe Bandwidth In geom_density(), the default kernel function is the Gaussian density and the bandwidth can be tweaked through the bw argument.\n1 2 3 4 5 6 7 8 9  h \u0026lt;- 1 ggplot() + geom_density(aes(x = sample), bw = h) + annotate( \u0026#34;label\u0026#34;, x = 0.5, y = 0.1, label = glue::glue(\u0026#34;bw = {h}\u0026#34;) )   Of course, I could now create multiple plots and change the value if h each time to demonstrate the effect of the bandwidth but the point of this post was to create an animation. So let\u0026rsquo;s do that instead.\nNevertheless, to create an animation, we need to be able to create multiple plots. Therefore, let us use the previous code and wrap a function depending on h around that. This function will be our plot generator depending on the bandwidth.\n1 2 3 4 5 6 7 8 9 10 11 12  plot_gen \u0026lt;- function(h) { g \u0026lt;- ggplot() + geom_density(aes(x = sample), bw = h) + annotate( \u0026#34;label\u0026#34;, x = 0.5, y = 0.1, label = glue::glue(\u0026#34;bw = {h}\u0026#34;) ) print(g) # For the animation we need this to be printed }   Now that we have that, define a function that creates all the plots we want to see in our animation, i.e. we create the animation frame by frame. This function can then be passed to saveGIF() from the animation package which then renders the animation for us. Creating a gif in R is as simple as that.\n1 2 3 4 5 6 7  all_the_plots \u0026lt;- function() { map(seq(0.05, 0.5, 0.05), plot_gen) } library(animation) saveGIF(all_the_plots()) ## Output at: animation.gif ## [1] TRUE   As you can see, the bandwidth really is a smoothing parameter. Of course, too much smoothing may not yield great results so the parameter needs to be chosen with care but let us not worry about this in this blog post.\nThe Actual Estimation Procedure Let us create another animation to visualize how kernel density estimation works on a more basic level, i.e. . In order to do so, notice that if the kernel \\(K\\) is a continuous density function of a random variable \\(X\\) (e.g. a Gaussian random variable), then the density function of the random variable \\((X + x_0)h\\) where \\(x_0 \\in \\mathbb{R}\\) and \\(h \u0026gt; 0\\) is given by \\(K((X - x_0)/h)/h.\\)\nConsequently, in the case of standard Gaussian random variables \\(X\\), the kernel density estimator is nothing but the average of the densities of \\(n\\) Gaussian random variables with individual means \\(x_k\\), \\(k = 1, \\ldots, n,\\) and common standard deviation \\(h.\\)\nTherefore, for a given sample you can run a kernel density estimation by taking the following steps:\n Check where each data point \\(x_k\\) is located on the x-axis For each data point \\(x_k\\) draw a Gaussian density \\(f_k\\) with standard deviation \\(h\\) and mean \\(x_k\\) For each \\(x \\in \\mathbb{R}\\) check what are the values of \\(f_k\\), \\(k = 1, \\ldots, n,\\) at \\(x\\) and average these.  So to create a visualization of the kernel density estimation principle, we simple create a function that plots each of those steps for us. Finally, we execute all of these functions and send them to saveGIF().\nWe begin by computing the data we need to create the plots later on, i.e. we simulate a sample and compute the values of the densities.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  compute_density \u0026lt;- function(x_0, h, K = dnorm) { xx \u0026lt;- seq(-6, 6, 0.001) * h tibble( x = xx, density = K((x - x_0) / h) / h ) } set.seed(123) # For the sake of demonstration we use a  # small uniformly distributed sample here x_sample \u0026lt;- runif(5, -5, 5) h \u0026lt;- 1 tib \u0026lt;- tibble( k = seq_along(x_sample), density = map(x_sample, compute_density, h = h) ) %\u0026gt;% unnest(density)   Then, it becomes time for our first step, i.e. check where the sample values are located.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  draw_axis \u0026lt;- function(x_sample, tib) { labs \u0026lt;- glue::glue(\u0026#34;$x_{seq_along(x_sample)}$\u0026#34;) labs \u0026lt;- latex2exp::TeX(labs) p \u0026lt;- ggplot(data = NULL, aes(x = x_sample)) + theme_minimal() + theme( axis.line.x = element_line(), panel.grid = element_blank(), axis.ticks = element_line(size = 1), axis.text = element_text(size = 14) ) + scale_x_continuous( limits = c(min(tib$x), max(tib$x)), breaks = sort(x_sample), minor_breaks = NULL, labels = labs ) + scale_y_continuous( breaks = NULL, limits = c(0, max(tib$density) + 0.025) ) + labs(x = element_blank()) p } p \u0026lt;- draw_axis(x_sample, tib) p   Once we have that, we draw the kernels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  draw_kernel \u0026lt;- function(p, tib) { p \u0026lt;- p + geom_line(data = tib, aes(x, density, group = k), size = 1) + geom_segment( aes( x = x_sample, xend = x_sample, y = 0, yend = dnorm(0) ), linetype = 2 ) + labs(y = element_blank()) p } draw_kernel(p, tib)   Next, average the densities at an arbitrary position \\(x_0.\\) If we can do that, then we can iterate through different values of \\(x_0.\\)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65  plot_until_x0 \u0026lt;- function(tib, x0) { labs \u0026lt;- glue::glue(\u0026#34;$x_{seq_along(x_sample)}$\u0026#34;) labs \u0026lt;- latex2exp::TeX(labs) tib_x0 \u0026lt;- tib %\u0026gt;% filter(x \u0026lt;= x0) %\u0026gt;% group_by(x) %\u0026gt;% summarise(est = mean(density), .groups = \u0026#34;drop\u0026#34;) anim_col \u0026lt;- \u0026#39;firebrick3\u0026#39; g \u0026lt;- ggplot() + geom_line( data = tib, aes(x, density, group = k), alpha = 0.5, size = 1 ) + geom_point( data = filter(tib, x == x0), aes(x, density), #col = anim_col, alpha = 0.75, size = 3 ) + geom_vline(xintercept = x0, col = anim_col, alpha = 0.5) + geom_point( data = slice_tail(tib_x0, n = 1), aes(x, est), col = anim_col, size = 3 ) + geom_line( data = tib_x0, aes(x, est), col = anim_col, size = 1 ) + theme_classic() + theme( axis.line.x = element_line(), axis.line.y = element_blank(), panel.grid = element_blank(), axis.ticks = element_line(size = 1), axis.text = element_text(size = 14) ) + scale_x_continuous( limits = c(min(tib$x), max(tib$x)), breaks = sort(x_sample), minor_breaks = NULL, labels = labs ) + scale_y_continuous( breaks = NULL, limits = c(0, max(tib$density) + 0.025) ) + labs(x = element_blank(), y = element_blank()) print(g) } x0 \u0026lt;- (0) plot_until_x0(tib, x0)   Last but not least, we may want to display the estimated density without the underlying kernels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  final_plot \u0026lt;- function(tib) { labs \u0026lt;- glue::glue(\u0026#34;$x_{seq_along(x_sample)}$\u0026#34;) labs \u0026lt;- latex2exp::TeX(labs) tib_x0 \u0026lt;- tib %\u0026gt;% group_by(x) %\u0026gt;% summarise(est = mean(density), .groups = \u0026#34;drop\u0026#34;) anim_col \u0026lt;- \u0026#39;firebrick3\u0026#39; g \u0026lt;- tib_x0 %\u0026gt;% ggplot() + geom_line( aes(x, est), col = anim_col, size = 1 ) + theme_classic() + theme( axis.line.x = element_line(), axis.line.y = element_blank(), panel.grid = element_blank(), axis.ticks = element_line(size = 1), axis.text = element_text(size = 14) ) + scale_x_continuous( limits = c(min(tib$x), max(tib$x)), breaks = sort(x_sample), minor_breaks = NULL, labels = labs ) + scale_y_continuous( breaks = NULL, limits = c(0, max(tib$density) + 0.025) ) + labs(x = element_blank(), y = element_blank()) print(g) } final_plot(tib)   Finally, we have all the ingredients to create the animation by collecting all of these functions in a wrapper function and using it in conjunction with saveGIF().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  gif \u0026lt;- function(x_sample, tib) { p \u0026lt;- draw_axis(x_sample, tib) map(1:3, ~print(p)) p \u0026lt;- draw_kernel(p, tib) map(1:5, ~print(p)) map(seq(min(tib$x), max(tib$x), 0.5), ~plot_until_x0(tib, .)) map(1:15, ~final_plot(tib)) } saveGIF(gif(x_sample, tib), interval = 0.4, # animation speed ani.width = 720, ani.height = 405, movie.name = \u0026#34;kernelAnimation.gif\u0026#34;)   Thus, we have created a short animation that illustrates the kernel density estimation procedure. Probably, there is some room for improving the animation by fine tuning the plots, tweaking with the animation speed or the number of frames. For now, though, let us leave everything as it is.\nBut feel free to let me know what could be improved in the comments. Similarly, if you want to leave any other form of feedback, feel free to roam the comment section too. Finally, if you enjoyed what you have seen here but do not want to bother writing a comment, you may simply hit the applause button instead.\n","description":"For my first post I create an animation using the animate package.","id":23,"section":"post","tags":["statistics","visualization"],"title":"Animating kernel density estimators","uri":"https://albert-rapp.de/post/visualize-kernel-density-estimation/"},{"content":"Welcome! My Name is Albert Rapp and I am currently a PhD student in mathematics at Ulm University. As part of my obligations as a PhD student I had to teach an applied course on statistical techniques using the statistical software R. Interestingly, I noticed that I actually enjoy using R which is why I went the extra mile and decided to write a set of lecture notes on my own and made them accessible online. In fact, I realized that the process of writing felt (mostly) fun and my retention rate of the stuff I had to learn in order to write about it was actually quite high.\nConsequently, I decided to make a habit out of writing about things I encounter in statistics and/or R. So, what you see here in this blog is the result of that decision. In the end, this is nothing but a personal project and is as much about helping me retain stuff as I learn new things as it is about making a contribution to the R community along the lines of many other blogs and tutorials that helped me learn R. Hopefully, what can be found here will be of benefit to someone other than me.\nCurrently, I aim at posting a new blog post on a biweekly schedule and regarding the content, I will write about whatever I find most interesting at the time of writing. On a long term basis though, I want to cover parts of my R/statistics bucket list which I described in the final chapter of my YARDS lecture notes. If you want to be notified when there is a new post online, you may want to check out the RSS feed which you can access here or via the RSS symbol at the bottom of the \u0026ldquo;Posts\u0026rdquo; Page.\nThis blog is build with blogdown and Hugo and all blog posts are released under a CC-BY-NC 4.0 license.\n","description":"About Me","id":24,"section":"","tags":null,"title":"About","uri":"https://albert-rapp.de/about/"}]