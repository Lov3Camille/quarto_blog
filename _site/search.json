[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Albert Rapp",
    "section": "",
    "text": "All blog posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nThe ultimate guide to starting a Quarto blog\n\n\n\n\n\n\n\n\n\n\nThis blog post is an in-depth guide on how to start blogging with Quarto.\n\n\n\n\n\n\nJul 24, 2022\n\n\n35 min\n\n\n\n\n\n\n\n\nHow to embed a Shiny app into your blog posts\n\n\n\n\n\n\n\nShiny\n\n\nShorts\n\n\n\n\nThis is a one-minute blog post to share how to incorporate Shiny apps in blog posts.\n\n\n\n\n\n\nMay 9, 2022\n\n\n0 min\n\n\n\n\n\n\n\n\nUse {lubridate} and {rtweet} to analyze your Twitter timeline\n\n\n\n\n\n\n\nVisualization\n\n\nAPI\n\n\n\n\nWe take a quick look at the rtweet and lubridate package. These help you to analyse your Twitter timeline. And they helped me to visualize my follower count as I reached 1000 followers this week.\n\n\n\n\n\n\nMay 6, 2022\n\n\n8 min\n\n\n\n\n\n\n\n\nStorytelling in ggplot using rounded rectangles\n\n\n\n\n\n\n\nVisualization\n\n\n\n\nWe build rebuild a ‘Storytelling with Data’ plot which uses rounded rectangles. I’ll show you an easy and a hard way to make rectangles round.\n\n\n\n\n\n\nMay 4, 2022\n\n\n18 min\n\n\n\n\n\n\n\n\n6 Lessons that I learned from teaching R to non-programmers\n\n\n\n\n\n\n\nOpinion\n\n\n\n\nI held a short workshop teaching R to Economics students. Here are six things that I wish I had known in advance.\n\n\n\n\n\n\nApr 15, 2022\n\n\n11 min\n\n\n\n\n\n\n\n\nHow to collect dataviz from Twitter into your note-taking system\n\n\n\n\n\n\n\nAutomation\n\n\nAPI\n\n\n\n\nThe goal is to send a tweet’s URL to a dummy mail account and let R extract it, call the Twitter API and then put everything as a Markdown file into your note-taking system.\n\n\n\n\n\n\nApr 14, 2022\n\n\n20 min\n\n\n\n\n\n\n\n\nRecreating the Storytelling with Data look with ggplot\n\n\n\n\n\n\n\nVisualization\n\n\n\n\nWe try to imitate the Storytelling with Data look with ggplot\n\n\n\n\n\n\nMar 29, 2022\n\n\n17 min\n\n\n\n\n\n\n\n\nHow to use functional programming for ggplot\n\n\n\n\n\n\n\nVisualization\n\n\nFunctional Programming\n\n\n\n\nFunctional programming is a mighty sword. Today, we use it to avoid tedious repetitions when things go wrong in ggplot.\n\n\n\n\n\n\nMar 25, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\nHow to use Fonts and Icons in ggplot\n\n\n\n\n\n\n\nVisualization\n\n\n\n\nThis is a short tutorial on how to import fonts and icons in R using the showtext package.\n\n\n\n\n\n\nMar 4, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n4 Ways to use colors in ggplot more efficiently\n\n\n\n\n\n\n\nVisualization\n\n\n\n\nInspired by a datawrapper blogpost, we explore how to work with fewer colors in ggplot.\n\n\n\n\n\n\nFeb 19, 2022\n\n\n12 min\n\n\n\n\n\n\n\n\nInteractive ggplots, user feedback, and a little bit of Javascript magic with Shiny\n\n\n\n\n\n\n\nShiny\n\n\n\n\nHere’s how I turned a ggplot interactive using Shiny and what else I learned while building that app\n\n\n\n\n\n\nJan 17, 2022\n\n\n10 min\n\n\n\n\n\n\n\n\nShowcasing the janitor package\n\n\n\n\n\n\n\n\n\n\nI demonstrate a couple of functions from the janitor package I find quite useful\n\n\n\n\n\n\nJan 12, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nggplot-tips: Learning by Thieving\n\n\n\n\n\n\n\nVisualization\n\n\nOpinion\n\n\n\n\nI advocate to take part in the TidyTuesday events to learn with and from others.\n\n\n\n\n\n\nJan 10, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nA couple of visualizations from ggforce\n\n\n\n\n\n\n\nVisualization\n\n\n\n\nThe ggplot2-tips series is continued with a few example plots from the ggforce package\n\n\n\n\n\n\nDec 31, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\n6 simple Shiny things I have learned from creating a somewhat small app\n\n\n\n\n\n\n\nShiny\n\n\n\n\nI recently built a small Shiny app where I had to search for how to do a lot of things. Here are 6 things I learned doing that. Maybe a Shiny beginner will find something useful in here.\n\n\n\n\n\n\nDec 9, 2021\n\n\n14 min\n\n\n\n\n\n\n\n\nggplot tips: Arranging plots\n\n\n\n\n\n\n\nVisualization\n\n\n\n\nThe patchwork and ggforce packages can be used to compose plots from multiple subplots. Let’s have a look at how that works.\n\n\n\n\n\n\nOct 28, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\nAn Exploratory Introduction to the Plotly Package\n\n\n\n\n\n\n\nVisualization\n\n\nExploratory Intro\n\n\n\n\nWe try to do a few simple things with the plotly package in order to figure out how it works.\n\n\n\n\n\n\nOct 16, 2021\n\n\n12 min\n\n\n\n\n\n\n\n\nDid German Voters Become More Impulsive?\n\n\n\n\n\n\n\n\n\n\nWe try to find out if voters in Germany became more impulsive over time.\n\n\n\n\n\n\nOct 3, 2021\n\n\n13 min\n\n\n\n\n\n\n\n\nWriting Versatile Functions with R\n\n\n\n\n\n\n\nFunctional Programming\n\n\n\n\nUsing concepts like dot-dot-dot and curly-curly we create functions that are more versatile and can be used in multiple settings.\n\n\n\n\n\n\nSep 16, 2021\n\n\n11 min\n\n\n\n\n\n\n\n\nggplot tips: Using position_stack() for Individual Positioning\n\n\n\n\n\n\n\nVisualization\n\n\n\n\nWe take a look at the differences between position = ‘stack’ and position = position_stack().\n\n\n\n\n\n\nSep 11, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nggplot tips: Assigning Labels to an Aesthetic\n\n\n\n\n\n\n\nVisualization\n\n\n\n\nWe talk about how to easily create labels for an aesthetic.\n\n\n\n\n\n\nAug 19, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nBeginning a ggplot2 Series: Logarithmize Your Scales\n\n\n\n\n\n\n\nVisualization\n\n\n\n\nThis is the beginning of a series about a few ggplot2 tricks I picked up along the way. In this first installment we talk about how logarithmizing scales can be beneficial.\n\n\n\n\n\n\nAug 7, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nIs Success Luck or Hard Work?\n\n\n\n\n\n\n\nSimulation\n\n\n\n\nI recreate a simulation study on the influence of luck on success compared to the influence of skill.\n\n\n\n\n\n\nJul 28, 2021\n\n\n17 min\n\n\n\n\n\n\n\n\nAnimating kernel density estimators\n\n\n\n\n\n\n\n\n\n\nFor my first post I create an animation using the animate package.\n\n\n\n\n\n\nJul 14, 2021\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Jul 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Albert Rapp",
    "section": "",
    "text": "I’m a mathematics PhD students with a passion for R, data visualization, Shiny and statistics/machine learning. On this blog, I share and teach what I learn.\nTo get started, you can check out my most popular content below. You can find me on Twitter or GitHub and YouTube. Feel free to reach out to me via mail and subscribe to my email newsletter.\n\n\n\n\n\n\n\n\nThe SWD is a great look for any visual. With ggplot you can recreate it.\n\n\n\nRounded rectangles with {ggchicklet} can give your bar chart a different feel.\n\n\n\nSpice up your visuals with different fonts and icons.\n\n\n\nAvoid plots that look like a gnome just puked a rainbow. Use colors effectively.\n\n\n\n\n\n\nCourse I taught at Ulm University. Inspired by the legendary R4DS, I take a tour through the data science workflow. Includes intro to {shiny} and {tidymodels}.\n\n\n\nBoth Poisson and logistic regression are GLMs. I explain the fundamentals.\n\n\n\nInteractive webapp I built to teach a selection of statistical concepts. Will be extended once I master {golem}.\n\n\n\n\n\n\nIf you’ve just gotten started with {shiny}, this is the post for you.\n\n\n\nNext step after the “6 simple Shiny things” post.\n\n\n\nFun app I built to ‘thin out pixels from an image’. The “about” page contains neat tricks for your Shiny learning path.\n\n\n\n\n\n\nThe {janitor} package contains great helper functions and can work magic with Excel files.\n\n\n\nYou don’t have to know much HTML or CSS to style your Quarto blog according to your wishes. This video shows you how.\n\n\n\nThis is a fun game I built to learn the street names in my city. Turned out to be a great Shiny learning experience.\n\n\n\n\n\n\nCurly-Curly and dot-dot-dot are EXTREMELY powerful concepts in R. Master them to be unstoppable.\n\n\n\nFunctional programming is great and powerful. A great motivation for learning them can come from ggplot.\n\n\n\n\n\n\nThis is an in-depth guide on how to start blogging with Quarto.\n\n\n\nI taught a intro to R course for business students. Here are things I wish I had known sooner.\n\n\n\nTidyTuesday is a great learning experience because so many people share their code."
  },
  {
    "objectID": "posts/11_track_twitter_followers/11_track_twitter_followers.html",
    "href": "posts/11_track_twitter_followers/11_track_twitter_followers.html",
    "title": "Use {lubridate} and {rtweet} to analyze your Twitter timeline",
    "section": "",
    "text": "This week, I am oddly proud to announce that I have reached 1000 followers on Twitter. Check out the visualization that I have created for this joyous occasion.\nTo me, my rising follower count and the somewhat regular mails that I receive are a sign that people like to read my blog. And to thank you all for following my work, let me give you a quick intro to the packages rtweet and lubridate. These were instrumental in creating the above visual."
  },
  {
    "objectID": "posts/11_track_twitter_followers/11_track_twitter_followers.html#working-with-rtweet",
    "href": "posts/11_track_twitter_followers/11_track_twitter_followers.html#working-with-rtweet",
    "title": "Use {lubridate} and {rtweet} to analyze your Twitter timeline",
    "section": "Working with rtweet",
    "text": "Working with rtweet\nAt the end of February 2022, I wondered how my follower count evolves over time. Unfortunately, this is not something Twitter shows you by default. The Analytics page only shows me the change within my last 28 days. To overcome this drawback, I consulted the rtweet package which is a fabulous package that lets you interact with Twitter’s API through R.\nIn my case, I only do rudimentary work with it and track my followers over time. For this to work, I have set up an R script that runs every hour to download a list of my followers. Each hour, the list’s length tells me how many followers I have.\nIf you want to do the same, install the package first. Make sure to install the development version from GitHub, though.\n\nremotes::install_github(\"rOpenSci/rtweet\")\n\n\nBasic functionalities\nrtweet comes with a lot of convenient function. Most of these start with get_. For instance, there are\n\nget_followers(): This is the function to get a list of an account’s followers.\nget_timeline(): This gives you the a user’s timeline like tweets, replies and mentions.\nget_retweets(): This gives you the most recent retweets of a given tweet.\n\nMy aforementioned R script just runs get_followers() and computes the number of rows of the resulting tibble.\n\n\n\n\nlibrary(rtweet)\ntib <- get_followers('rappa753')\ntib\n\n# A tibble: 2,207 × 2\n   from_id             to_id   \n   <chr>               <chr>   \n 1 1447053543786090504 rappa753\n 2 254563418           rappa753\n 3 382843130           rappa753\n 4 1543220302296977408 rappa753\n 5 1673477562          rappa753\n 6 1308732763726589954 rappa753\n 7 1549153427061501952 rappa753\n 8 1387151657507692545 rappa753\n 9 1374557861309775872 rappa753\n10 1479882642413793284 rappa753\n# … with 2,197 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\nnrow(tib)\n\n[1] 2207\n\n\nFor my above visualization, I used get_timeline() to extract my five most popular tweets. Here, I ranked the popularity by the count of likes resp. “favorites” as rtweet likes to call it.\n\nlibrary(tidyverse)\ntl <- get_timeline('rappa753', n = 1000)\ntl_slice <- tl %>% \n  slice_max(favorite_count, n = 5) %>% \n  select(created_at, full_text, favorite_count, retweet_count) \ntl_slice\n\n# A tibble: 5 × 4\n  created_at          full_text                                  favor…¹ retwe…²\n  <dttm>              <chr>                                        <int>   <int>\n1 2022-06-18 15:46:22 \"Ever heard of logistic regression? Or Po…    1460     231\n2 2022-07-10 16:18:10 \"The #rstats ecosystem makes splitting a …     660      95\n3 2022-03-05 21:56:33 \"The fun thing about getting better at #g…     507      82\n4 2022-07-09 13:21:04 \"Creating calendar plots with #rstats is …     419      53\n5 2022-02-19 18:32:23 \"Ever wanted to use colors in #ggplot2 mo…     347      56\n# … with abbreviated variable names ¹​favorite_count, ²​retweet_count\n\n\nNotice that I tweeted two of these before I started tracking my followers. Consequently, I ignored them for my visualization.\nUnfortunately, the rtweet package cannot do everything. For example, the snapshot functionality tweet_shot() does not work anymore. I think that’s because the Twitter API changed.\nThis bothered me during the 30 day chart challenge in April because I wanted to automatically extract great visualizations from Twitter. But as tweet_shot() was not working, I had to call Twitter’s API manually without rtweet. If you’re curious about how that works, check out my corresponding blog post. There, I’ve also explained how to set up a script that gets executed, say, every hour automatically.\n\n\nSetting up a Twitter app\nThis pretty much explains how rtweet works. In general, it is really easy to use. And if you only want to use it only occasionally, there is not much more to it.\nHowever, if you want to use the package more often - as in calling the API every hour - then you need to set up a Twitter app. You can read up on how that works in the “Preparations” section of the above blog post. Once you’ve got that down, your rtweet calls can be authenticated through your Twitter app like so.\n\nauth <- rtweet::rtweet_app(\n  bearer_token = keyring::key_get('twitter-bearer-token', keyring = 'blogpost')\n)\nauth_as(auth)\n\nHere, I have used the keyring package to hide the bearer token of my Twitter app. If that doesn’t mean anything to you, let me once again refer to the above blog post. The important thing is that after these lines ran your rtweet calls get funneled through your own Twitter app."
  },
  {
    "objectID": "posts/11_track_twitter_followers/11_track_twitter_followers.html#working-with-lubridate",
    "href": "posts/11_track_twitter_followers/11_track_twitter_followers.html#working-with-lubridate",
    "title": "Use {lubridate} and {rtweet} to analyze your Twitter timeline",
    "section": "Working with lubridate",
    "text": "Working with lubridate\nAs you saw above, the timeline that we extracted and saved in rtweet contains time data. Here it is once again.\n\ntl_slice\n\n# A tibble: 5 × 4\n  created_at          full_text                                  favor…¹ retwe…²\n  <dttm>              <chr>                                        <int>   <int>\n1 2022-06-18 15:46:22 \"Ever heard of logistic regression? Or Po…    1460     231\n2 2022-07-10 16:18:10 \"The #rstats ecosystem makes splitting a …     660      95\n3 2022-03-05 21:56:33 \"The fun thing about getting better at #g…     507      82\n4 2022-07-09 13:21:04 \"Creating calendar plots with #rstats is …     419      53\n5 2022-02-19 18:32:23 \"Ever wanted to use colors in #ggplot2 mo…     347      56\n# … with abbreviated variable names ¹​favorite_count, ²​retweet_count\n\n\nSadly, working with times and dates is rarely pleasant. But we can make our life a bit easier by using the lubridate package which was made for that. To show you how it works, it is probably best to show you a couple of use cases.\nAll of these will be taken from what I had to deal with to create my celebratory visualization. But I simplified it to minimal examples for this blog post. More use cases can be found in the lubridate cheatsheet or the tidyverse cookbook ressource by Malte Grosser.\n\nParse dates and times\nEDIT July 13, 2022: After moving my blog to quarto, {rtweet} updated its default output format. Now, parsing dates and times is not necessary anymore. I leave this section in here because the code may still be helpful in other situations.\nFirst, I needed to convert the created_at column from character to datetime format. The easiest way to do that gets rid of +0000 in the character vector and then parses the vector into the right format via parse_date_time(). But there is a catch. Check out what happens if I try this on my computer.\n\nlibrary(lubridate)\ntl_slice %>% \n  mutate(created_at = parse_date_time(\n    str_remove(created_at, '\\\\+0000'), # remove the +0000 \n    orders = 'a b d H:M:S Y'\n  ))\n\nSee how all values in created_at are NA now? That’s a problem. And we will solve it very soon. But first, let me explain how the function call works.\nThe orders argument specifies how the vector created_at (without +0000) is to be understood. We clarify that created_at contains (in the order of appearance)\n\nabbreviated week day names (a)\nabbreviated month names (b)\nthe day of the month as decimals (d)\nand so on\n\nWhere do these abbreviations a, b, d, etc. come from? They are defined in the help page of parse_date_time(). You can find them in the section “Details”. But why does the code not work? Why do we always get an NA? For once, my computer is truly the problem. Or rather, its settings.\nBy default, my computer is set to German. But even if RStudio or R’s error messages are set to English, my computer’s so-called “locale” may be still be set to German. That’s a problem because abbreviations like “Sat” and “Wed” refer to the English words “Saturday” and “Wednesday”. So, we need to make sure that parse_date_time() understands that it needs to use an English locale. Then, everything works out.\n\nparsed_highlights <- tl_slice %>% \n  mutate(created_at = parse_date_time(\n    str_remove(created_at, '\\\\+0000'), # remove the +0000 \n    orders = 'a b d H:M:S Y',\n    locale = \"en_US.UTF-8\"\n  ))\nparsed_highlights\n\n\n\n\nWe are now ready to send this data to ggplot. Since created_at is formatted in datetime now, ggplot will understand what it means when we map x = created_at.\n\nparsed_highlights %>% \n  ggplot(aes(created_at, favorite_count)) +\n  geom_line()\n\n\n\n\n\n\nUsing scale_x_date(time) and locale\nDid you see that the x-axis uses German abbreviations and doesn’t show what year we’re in? That’s not great. Let’s change that. As is always the case when we want to format the axes we will need a scale_*() function. Here, what we need is scale_x_datetime().\nBut this won’t solve our German locale problem. The easiest way to solve that tricky ordeal is to change the locale globally via Sys.setlocale(). Don’t worry, though. The locale will reset after restarting R. No permanent “damage” here.\n\nSys.setlocale(\"LC_ALL\",\"en_US.UTF-8\")\n\n[1] \"LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=de_DE.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=de_DE.UTF-8;LC_IDENTIFICATION=C\"\n\np <- parsed_highlights %>% \n  ggplot(aes(created_at, favorite_count)) +\n  geom_line() +\n  scale_x_datetime(\n    date_breaks = '2 months', # break every two months\n    date_labels = '%b %Y'\n  )\np\n\n\n\n\nNotice that we have once again used the same abbreviation as for parse_date_time(). This time, though, they have to be preceded by %. Don’t ask me why. It is just the way it is.\n\n\nCreate new dates\nLet us add a rectangle to our previous plot via an annotation. This is similar to what I needed to do when adding my “mysterious wonderland” to my plot.\nSince the x aesthetic is formatted to datetime, we have to specify dates for the xmin and xmax aesthetic of our annotation. Therefore, we need to create dates manually. In this case, make_datetime() is the way to go. If we’re dealing only with dates (without times), then make_date() is a useful pendant. Both functions are quite straightforward.\n\np <- p +\n  annotate(\n    'rect',\n    xmin = make_datetime(year = 2021, month = 11, day = 6, hour = 12),\n    xmax = make_datetime(year = 2021, month = 9), \n    ymin = 200,\n    ymax = 273,\n    alpha = 0.5\n  )\np\n\n\n\n\n\n\nFilter with intervals\nMaybe we want to highlight a part of our line. To do so, we could filter our data to check whether certain date ranges correspond to parts that we want to highlight. Usually when we want to check if a value x is within a certain set of objects we use x %in% objects.\nTo do the same with dates, we need to create an interval with interval() first. Then, we can use that in filter() in conjunction with %within% instead of %in%.\n\nmy_interval <- interval(\n  start = make_date(year = 2022, month = 2), \n  end = make_date(year = 2022, month = 3, day = 10)\n)\n\np +\n  geom_line(\n    data = parsed_highlights %>% filter(created_at %within% my_interval),\n    color = 'red',\n    size = 2\n  )\n\n\n\n\n\n\nCalculations with times\nSay that you want to highlight the first five days after a certain date. (That’s exactly what I did in my plot.) Then, you can simply add days(5) to this date. There are similar functions like minutes(), hours() and so on. Let me show you how that may look in a visualization.\n\np +\n  annotate(\n    'rect',\n    xmin = parsed_highlights[[1, 'created_at']] - hours(24),\n    xmax = parsed_highlights[[1, 'created_at']] + days(5),\n    ymin = -Inf,\n    ymax = Inf,\n    alpha = 0.25,\n    fill = 'blue'\n  )"
  },
  {
    "objectID": "posts/11_track_twitter_followers/11_track_twitter_followers.html#closing",
    "href": "posts/11_track_twitter_followers/11_track_twitter_followers.html#closing",
    "title": "Use {lubridate} and {rtweet} to analyze your Twitter timeline",
    "section": "Closing",
    "text": "Closing\nThis was a short intro to lubridate and rtweet. Naturally, the evolution of my follower count contained a lot more steps. In the end, though, these steps were merely a collection of\n\ntechniques you know from my two previous storytelling with ggplot posts (see here and here) plus\ndata wrangling using times and dates with the functions that I just showed you.\n\nOnce again, thank you all for your support. And if you liked this post and don’t follow my work yet, then consider following me on Twitter and/or subscribing to my RSS feed. See you next time!"
  },
  {
    "objectID": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html",
    "href": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html",
    "title": "The ultimate guide to starting a Quarto blog",
    "section": "",
    "text": "Blogging is great for many reasons. And with Quarto, blogging has never been easier. Within a few clicks, you can create your own Quarto blog. And the best part is: With Quarto you can customize basically everything according to your wishes.\nToday, I want to show you how to build a blog with Quarto. This in-depth guide is the result of hours of working with Quarto’s amazingly detailed documentation. Hopefully, it will save you a lot of time and helps you start your own blog. Here’s what we will cover:\nNotice that this is a lot. I don’t want to brag. But this is a lot. Do yourself a favor and skip the time-intensive steps if you’re just starting out with your blog.\nIn the beginning, the most important thing is to write regularly. This is WAAAAY more important than, say, using many fancy colors in your blog. At first, you can just go with a pre-defined theme and a standard landing page.\nReally, I cannot stress this enough: If you want to start a blog, start writing regularly first. Worry about your design later. The only way I could stress this more would be to say it in Spanish.\nAnd if you’re not sure what steps you can skip from my detailed guide, Beatriz Milz got you covered. In one of her blog posts she shows you how to get started with your Quarto blog in 10 steps. So, you can always start with her intro to get started quickly and then successively add parts from my guide.\nAlright, enough chit-chat. Let’s dive in."
  },
  {
    "objectID": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#three-reasons-for-starting-a-blog",
    "href": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#three-reasons-for-starting-a-blog",
    "title": "The ultimate guide to starting a Quarto blog",
    "section": "Three reasons for starting a blog",
    "text": "Three reasons for starting a blog\nIn case it is not self-evident of why you should start a blog, let me give you a few good reasons.\n\nLearning: Writing acts as a safeguard from shallow understanding. I cannot count how many time I erroneously thought I’ve understood something. But working on a blog post often showed me that my first “understanding” was wrong. That’s why writing is such an effective tool for learning.\nHelping others: When you share your learnings, you will eventually help people who are struggling with similar problems. Don’t worry if you’re not an expert. Usually, people don’t enjoy learning from experts.\nRather. people like to learn from other learners who are just a few steps ahead. That’s because experts tend to leave out details that are “obvious”. Yet, some things are only obvious once you are an expert.\nPortfolio and job opportunities: If you publish content on the internet, you can always show people what you’ve created. This is great for demonstrating your skills (e.g. when applying for a job). Also, from personal experience I can tell you: Writing in the open can lead to job opportunities. You never know who ends up reading your blog and wants to work with you."
  },
  {
    "objectID": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#initialize-your-blog",
    "href": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#initialize-your-blog",
    "title": "The ultimate guide to starting a Quarto blog",
    "section": "Initialize your blog",
    "text": "Initialize your blog\nOnce you have installed Quarto, it is dead-simple to start a blog. If you don’t use RStudio, you’ll have to call\nquarto create-project myblog --type website:blog\nfrom the command line. In RStudio, you’ll have to click on the “new project”-button and the menu will let you create a new Quarto blog.\n\n\n\nClick on ‘Quarto Blog’ to start a new blog project\n\n\nYou should also create a git repo for your blog project (RStudio will ask you if you want that). This lets you revert changes when you break your blog. And after answering all of RStudio’s prompts, you can render your blog with Render Website from RStudio’s Build tab.\n\n\n\nThis is what the default blog looks like.\n\n\n\nFirst simple changes\nFirst, let us make the about page into a landing page. Use it to introduce your blog to your readers instead of flooding them with a list of blog posts. To do so, rename two files:\n\nRename index.qmd to blog.qmd\nRename about.qmd to index.qmd\n\n\n\n\nYour previous about.qmd file becomes your landing page now. The navbar doesn’t look right. We’ll change that next.\n\n\nThe next easy changes happen in the _quarto.yml file. There, we will mostly change some general info about our blog and stuff in the navbar. Most of the steps are self-explanatory that’s why I only list the steps I think are necessary.\n\nName your blog via website > title\nSet a site-url for your blog (this is important for RSS feeds later. If you don’t have a URL yet, you can skip this step for now).\nAdd a description\nChange reference of about.qmd and link to blog.qmd\nLink your GitHub profile etc.\nSet theme: theme.scss. This will be used for custom styling later. But you can also set theme to one of Quarto’s pre-defined themes.\n\nYou can also take a look at my _quarto.yml file:\n\n\n _quarto.yml\n\nproject:\n  type: website\n\nwebsite:\n  title: \"Albert Rapp\" \n  site-url: https://albert-rapp.de\n  description: \"This is Albert Rapp's personal Blog.\"\n  navbar:\n    right:\n      - text: Blog\n        href: blog.qmd \n      - icon: github\n        href: https://github.com/AlbertRapp\n      - icon: twitter\n        href: https://twitter.com/rappa753\n      - icon: youtube\n        href: https://www.youtube.com/user/Alfrodo123\n#########################\n# You can ignore this part in the first step\n      - icon: rss\n        href: blog.xml\n      - text: Ressources\n        menu: \n          - text: DataViz Portfolio\n            href: dataviz_portfolio.html\n          - text: YARDS\n            href: https://yards.albert-rapp.de/\n          - text: R Weekly\n            href: https://rweekly.org/\n          - text: R Bloggers\n            href: https://www.r-bloggers.com/\n      - text: Archive\n        href: archive.qmd\n  google-analytics: <Insert your ID here>\n  cookie-consent: true\n  twitter-card: \n    image: thumbnail_blog.png\n    card-style: summary_large_image\n    creator: \"@rappa753\"\n\nfilters:\n  - code-filename\n######################### \n\nformat:\n  html:\n    theme: theme.scss\n\n\n\nCreate your first blog post\nYou can create a new blog post by creating a new .qmd-file in the posts directory. But I recommend that you create a new sub-directory in posts for each new blog post. So, starting a new blog post is just a matter of\n\nCreating a new directory, say, posts/new_blog_post.\nCreate a blank .qmd-file in this new directory, e.g posts/new_blog_post/post.qmd.\nFor demo purposes, you can now fill the new file’s YAML header with a few basic properties and fill it with a bit of text, code chunks, math parts etc. If you wish, uou can use the dummy file I used.\n\nIf you’ve followed these steps, you can render your new blog post by clicking Render Website in RStudio’s Build tab. This will render all .qmd-files that have not been rendered before. If you want to make changes to a blog post, then you will have to render the respective .qmd-file manually.\n\n\n\nThis is the dummy blog post I used to try out new features from Quarto. Later on, I used it to see how new themes look.\n\n\n\n\nChanging metadata\nIn general, it’s good that you don’t have to re-render everything when you write a new post. The posts/_metadata.yml file ensures that. Its main purpose is to set global options for all you blog posts in the posts directory.\nBy default, this file sets freeze: true. That’s the reason why you have re-render blog posts manually. In _metadata.yml, we can define more YAML options for all blog posts. Most of the changes I applied in this file are self-explanatory. Thus, I leave you with my _metadata.yml file.\n\n\n posts/_metadata.yml\n\n# Options specified here will apply to all posts in this folder\n\n# freeze computational output\nfreeze: true\n\n# Enable banner style title blocks\ntitle-block-banner: false\n\n# Author name of all blog posts\nauthor: 'Albert Rapp'\n\n# Table of content settings\ntoc: true\ntoc-depth: 3\n\npage-layout: article\n\n#########################\n# You can skip this part in the first step\nformat:\n  html:\n    include-after-body: ../footer.html\n\ncomments: \n  utterances: \n    repo:  AlbertRapp/blogComments \n#########################"
  },
  {
    "objectID": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#create-your-own-series",
    "href": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#create-your-own-series",
    "title": "The ultimate guide to starting a Quarto blog",
    "section": "Create your own series",
    "text": "Create your own series\nCreating a series (like my ggplot2 series) is great because it bundles similar content into one thread. However, we need to understand listing pages before we can create our own series.\n\nListing pages (including RSS feeds)\nThe file blog.qmd is a listing page. It determines what blog posts will be listed on our Blog page. The important stuff happens in this file’s YAML header. Here’s how my header looks.\n\n\n blog.qmd\n\n---\nlisting:\n  contents: posts/**/*.qmd\n  sort: \"date desc\"\n  type: default\n  categories: true\n  sort-ui: true\n  filter-ui: true\n  fields: [date, title, reading-time, description, categories]\n  feed: true\npage-layout: full\ntitle-block-banner: false\n---\n\nLet me briefly describe a few of these options:\n\ncontents: posts/**/*.qmd means that all .qmd-files from the posts directory (and sub-directories) will be included in this listing page. If your posts directory will only contain .qmd-files, then you may as well just write contents: posts. In my case, though, some directories contain .md-files that I don’t want to list.\nIf you wish, enable sorting & filtering via sort-ui: true and filter-ui: true. The default sorting is set via sort.\nI don’t like the title block banner. So I set the corresponding option to false.\nMore importantly, I want to display a post’s reading time instead of the author (I am the only author anyway). By changing the fields options, I can make that happen. In the Quarto’s docs you can find more options for fields.\nfeed: true instructs this listing page to generate an RSS-feed. This works only if we have set the site-url in our _quarto.yml file. If we have enabled RSS feeds, then this listing page will generate a feed that will be saved in the file blog.xml (because the generating file is blog.qmd). That’s why I have linked the RSS icon in our _quarto.yml file to blog.xml.\n\n\n\nCreating an archive\nNow that we understand listing pages we can create more of them. For example, let us create an archive. This is nothing but a listing page that lists all blog posts without a description.\nFor this archive I have created a new file archive.qmd in my blog’s main directory and filled it as follows. Notice that page-layout: full. This means that there is no additional side margin for the category labels.\n\n\n archive.qmd\n\n---\ntitle: \"Archive\"\npage-layout: full\nlisting:\n  contents: posts/**/*.qmd\n  type: default\n  fields: [date, title]\n  sort: 'date desc'\n---\n\n\n\nNew sub-directory for series\nFinally, we can create new listing pages for blog posts that belong to a series. For this to work, all posts that belong to a series must reside in a single new sub-directory of posts. For my blog, I have created a sub-directory posts/ggplot2-tips. And the corresponding listing page is generated by ggplot-series.qmd.\n\n\n ggplot-series.qmd\n\n---\ntitle: \"Series: ggplot2-tips\"\nlisting:\n  contents: posts/ggplot2-tips\n  sort: \"date desc\"\n  type: default\n  categories: false\n  sort-ui: false\n  filter-ui: false\n  fields: [date, title, reading-time, description]\npage-layout: full\ntitle-block-banner: true\n---\n\nNow, we have to link to our new listing pages somewhere. I chose to link to my archive in the navbar (see _quarto.yml). And my ggplot2 series found a home above the list of all other blog posts. I made this happen by modifying blog.qmd with regular Markdown magic. This could look as follows:\n\n\n blog.qmd\n\n---\nlisting:\n  contents: posts/**/*.qmd\n  sort: \"date desc\"\n  type: default\n  categories: true\n  sort-ui: true\n  filter-ui: true\n  fields: [date, title, reading-time, description, categories]\n  feed: true\npage-layout: full\ntitle-block-banner: false\n---\n\n# Series\n\n##### [ggplot2-series](ggplot-series.html)\nThis series contains a great deal of tips, tricks and packages \nthat you can use to level up your ggplot game.\n\nIn reality, my blog listings page looks a little bit different because I use two columns (as you can see for yourself) but we’ll come to that shortly."
  },
  {
    "objectID": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#interact-with-your-audience",
    "href": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#interact-with-your-audience",
    "title": "The ultimate guide to starting a Quarto blog",
    "section": "Interact with your audience",
    "text": "Interact with your audience\nDid you really publish a blog post if no one reads it? Let’s face it. The best content in the world won’t save you from going unnoticed if readers cannot keep in touch. Luckily, we have already taken the first steps to let your audience connect to you.\nFor starters, your newly created RSS-feed can notify readers whenever you post something new. Also, you can submit your newly created RSS-feed to feed-aggregators like Rweekly or Rbloggers. This will increase your reach in an automated fashion. More importantly, Rweekly and Rbloggers are great places to show your content to the R community.\n\nComment sections\nNext, let us add a comment section. This let’s your readers, well, comment on your superb content. My go-to tool for that is utterances. As it works via GitHub, you’ll need to create a separate public repo on GitHub. This repo will be home to your blog’s comments. But first, you need to install utterances on your newly created repo. Follow this link to do so.\nClearly, we want a comment section below every blog post but not below every page (e.g. our landing page). Therefore, a good place to include utterances is in posts/_metadata.yml. You have already seen the code necessary for that earlier. But for your convenience, here it is again.\n\n\n posts/_metadata.yml\n\ncomments: \n  utterances: \n    repo:  AlbertRapp/blogComments\n\nBeware that the RStudio viewer may not display the comments. Open your blog in an external browser to see if everything worked.\n\n\n\nCheck that your comment section appears under your blog posts.\n\n\n\n\nNewsletter\nAnother popular way to engage with your readers is via newsletters. The idea is simple:\n\nReaders sign up via email\nYou regularly inform your subscribers about new content. Personally, I also use my newsletter to share thoughts on topics that interest me (and hopefully my readers).\n\nThankfully, you don’t have to send the mails manually. There are many free providers that can help with the logistics. And most of these services provide HTML snippets for registration too. That’s fantastic because you can place this snippet anywhere you like. In my case, Beehiiv provided the following snippet.\n<iframe id=\"beehiiv-form\" src=\"https://embeds.beehiiv.com/9232d2a2-6e85-4beb-b8ed-1de94e9e4f01?slim=true\" data-test-id=\"beehiiv-embed\" frameborder=\"0\" scrolling=\"no\" style=\"margin: 0; border-radius: 0px !important; background-color: transparent; height: 75px; width: 90%;\"></iframe>\nLet’s put it somewhere easy first. Let us use Quarto’s column syntax for this. In case you don’t know it, here’s a demo.\n\nCodeOutput\n\n\n:::: {.columns}\n\n\n::: {.column width=\"50%\"}\n### Column 1\nThis is written in column 1\n:::\n\n::: {.column width=\"50%\"}\n### Column 2\nThis is written in column 2\n:::\n\n:::: \n\n\n\n\n\nColumn 1\nThis is written in column 1\n\n\n\nColumn 2\nThis is written in column 2\n\n\n\n\n\n\nNow we can use this to put our registration snippet at the top of the blog listings page. For example, we could modify blog.qmd as follows.\n\n\n blog.qmd\n\n---\nlisting:\n  contents: posts/**/*.qmd\n  sort: \"date desc\"\n  type: default\n  categories: true\n  sort-ui: true\n  filter-ui: true\n  fields: [date, title, reading-time, description, categories]\n  feed: true\npage-layout: full\ntitle-block-banner: false\n---\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n# Series\n\n##### [ggplot2-series](ggplot-series.html)\nThis series contains a great deal of tips, tricks and packages \nthat you can use to level up your ggplot game.\n:::\n\n::: {.column width=\"60%\"}\n# Subscribe\n<iframe id=\"beehiiv-form\" src=\"https://embeds.beehiiv.com/9232d2a2-6e85-4beb-b8ed-1de94e9e4f01?slim=true\" data-test-id=\"beehiiv-embed\" frameborder=\"0\" scrolling=\"no\" style=\"margin: 0; border-radius: 0px !important; background-color: transparent; height: 75px; width: 90%;\"></iframe>\n:::\n\n:::: \n\n\n\n\nOutput of previous change to blog.qmd\n\n\n\n\nAdd a footer below blog posts\nNow, we’re going to do something harder. We will include something at the end of every blog post (before the comments). That’s the perfect spot for\n\nNewsletter registrations,\n“Share on”-buttons,\n“Buy me coffee”-buttons or\nwhatever else you fancy.\n\nFor example, this could look like my current footer.\n\n\n\nMy current footer\n\n\nTo include something at the end of posts, I’ve found format > html > include-after-body helpful. That’s the part you have seen earlier in my _metadata.yml file. You don’t know which part I’m talking about? Well, it’s easy to forget. But don’t worry. I got you. Here’s the part I mean.\n\n\n posts/_metadata.yml\n\nformat:\n  html:\n    include-after-body: ../footer.html\n\nOf course, this assumes that you have whipped together a footer.html file in the main directory. If you don’t know HTML, then you can always use code from a .qmd-file’s output (more on finding HTML-code later). For my blog, I have created the following simple HTML-file.\n\n\n footer.html\n\n<div>\n<hr>\n\n<h3> Stay in touch </h3>\n\n<p> If you enjoyed this post, then don't miss out on any future posts by subscribing to my email newsletter. </p>\n\n<iframe id=\"beehiiv-form\" src=\"https://embeds.beehiiv.com/9232d2a2-6e85-4beb-b8ed-1de94e9e4f01?slim=true\" data-test-id=\"beehiiv-embed\" frameborder=\"0\" scrolling=\"no\" style=\"margin: 0; border-radius: 0px !important; background-color: transparent; height: 55px;\"></iframe>\n\n\n<h3> Support my work with a coffee </h3>\n\n<script type=\"text/javascript\" src=\"https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js\" data-name=\"bmc-button\" data-slug=\"rappa753\" data-color=\"#06436e\" data-emoji=\"☕\"  data-font=\"Lato\" data-text=\"Support me\" data-outline-color=\"#ffffff\" data-font-color=\"#ffffff\" data-coffee-color=\"#FFDD00\" data-height=\"40px\" ></script>\n\n<h3> Share </h3>\n\n<a href=\"https://twitter.com/share?ref_src=twsrc%5Etfw\" class=\"twitter-share-button\" data-size=\"large\" data-hashtags=\"#rstats\" data-show-count=\"false\">Tweet</a><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n<hr>\n</div>\n\nNote that the code for all the widgets were provided from the respective service. I certainly didn’t code that stuff myself. You know, I’m not an HTML expert."
  },
  {
    "objectID": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#style-your-blog",
    "href": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#style-your-blog",
    "title": "The ultimate guide to starting a Quarto blog",
    "section": "Style your blog",
    "text": "Style your blog\nAt some point, you will likely want to customize your blog’s visual appearance. This will include changing colors, fonts and sizes of all sorts of things.\nAnd same as the internet, Quarto runs on HTML and CSS. These two languages are the major players that style your blog. Luckily, you don’t have to know a lot about them to style your blog.\nThat’s because Quarto already generates a whole lot of HTML and CSS code when you render your blog. Consequently, it takes only a little peaking behind the HTML/CSS curtain to find out what values need tweaking.\nI know, this sounds quite complicated. But it isn’t. I don’t know a whole lot about HTML and CSS but I have customized this blog, haven’t I? In fact, working on this blog has been a fun way to learn more about these two languages. To help you do the same, I have created a video that walks you through the process of\n\nLearning a minimal amount of HMTL/CSS to understand what’s going on,\nFinding HTML/CSS snippets that you want to tweak and\nMaking the changes in your blog\n\nYou can find the video on YouTube.\n\n\nIf videos are not your thing, here’s the high-level summary:\nIn quarto.yml, we have set theme: theme.scss. Thus, all visual elements of this blog are governed by this SCSS-file. But it doesn’t exist yet. That’s why everything is set to default values. So, create theme.scss yourself by creating an empty text file and renaming it to theme.scss.\nQuarto uses a framework called Bootstrap. In this framework, you can change most parts of your blog by changing a variable’s value. For instance, there is a variable called $primary (variables use dollar signs in SCSS). It governs the main color of your blog. And you can change it to any color you like.\nSimilarly, your text color is governed by $body-color and your text font is determined by $font-family-base. Quarto lists the most common variables in their docs. For the full list of variables you can check out Bootstrap’s Github page. Here are the variables I have set at the beginning of my theme.scss file.\n\n\n theme.scss\n\n/*-- scss:defaults --*/\n$primary:  #06436e !default;\n$font-family-base:  'Source Sans Pro',  'Lato', 'Merriweather', 'Roboto Regular', 'Cabin Regular' !default;\n$body-color: $gray-700 !default;\n$headings-font-family:  'Prata', 'Roboto', 'Playfair Display', 'Montserrat'    !default;\n\nNotice that my file starts with /*-- scss:defaults --*/. This is just something an SCSS-file expects. Also, make sure to use multiple fonts. As not every computer has every font installed, it’s good to offer multiple alternatives for your reader’s computer.\nConsequently, when you want to style your blog, you can stroll through the list of variables. Once you have found a variable that you want to change (most variable names are quite self-explanatory), just throw that into your scss-file.\n\nSCSS-Rules\nUnfortunately, not everything can be changed with Bootstrap variables. In this case, you will need to take a look at your blog’s source code. To do so, open your blog in a browser of your choice. Then, right-click anywhere and press “Inspect”. This will open the developer mode of your browser.\nNow, you can see your blog’s source code. This view can be quite daunting. But don’t worry. You can ignore most of it. The only thing you need is the most specific description of the part you want to tweak.\n\n\n\nOn the left you see the website you’ve opened. On the right you see the HTML- (top) and CSS-code (bottom). As you move your cursor through the HTML-code, your browser shows you the corresponding part of the website.\n\n\nYou see, in HTML and CSS everything consists of (nested) tags, classes and IDs. All of these are indicated by a different symbol:\n\n<...> stands for tag, e.g. <div> </div> (This is a section)\n. stands for class, e.g. .grid (This is grid class object)\n# stands for ID, e.g. #my_about_page (This is a name that you can access in your CSS code)\n\nAs you move your cursor through the HTML code, your browser will show you what element in your blog corresponds to the code you’re currently hovering over. This helps you to find the HTML code responsible for the part that you want to tweak. Once you have found the desired code section, the CSS-code will also show you a mix of tags, classes and IDs that describe this element.\nFor example, each blog title is described by #title-block-header.quarto-title-block.default .quarto-title .title. Just copy this from your browser’s developer page and put it into your SCSS-file. Then, add {} and describe how you want to style the element. This could look like so.\n\n\n theme.scss\n\n/*-- scss:rules --*/\n#title-block-header.quarto-title-block.default .quarto-title .title {\n  font-family: \"Oleo Script\", \"Libre Baskerville\", $headings-font-family;\n  font-size: $font-size-base * 2.5;\n}\n\nHere I have chosen to use different fonts for blog titles (if possible) and make the font larger. I tried to avoid hard-coding the font size with something like 14pt. Instead, I have taken the Bootstrap variable $font-size-base and scaled it. Finally, notice that SCSS-rules - like the one we have just defined - need to be written after /*-- scss:rules --*/ in your SCSS-file.\nSo, now you have learned a way to style your blog. You can basically change everything you like this way. And you don’t need to know much about HTML/CSS for that. You just need some patience strolling through the code.\nOf course, you will need to know the keywords like font-family, font-size, etc. But you will stumble across the most common keywords in the HTML code anyway. Alternatively, a quick web search can help. And if you want, you can take a look at my theme.scss file.\nThis file also includes a few @media (min-width: ...px) calls that styles the blog differently on small screens (depending on the amount of pixels). You can ignore this part for now.\n\n\nLight and dark modes\nAre you a fan of dark modes? Me too! Though, at the time of writing I haven’t gotten around to add a dark mode to this blog. But the steps to include a dark mode are easy. You just have to modify your _quarto.yml file as follows.\n\n\n _quarto.yml\n\nformat:\n  html:\n    theme: \n      light: theme.scss\n      dark: theme-dark.scss\n\nThis will add a dark mode toggle to your navbar. And now you can style the dark mode in the same way as before by creating a new scss-file for it."
  },
  {
    "objectID": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#build-your-own-landing-page",
    "href": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#build-your-own-landing-page",
    "title": "The ultimate guide to starting a Quarto blog",
    "section": "Build your own landing page",
    "text": "Build your own landing page\nMy own landing page is custom built and here I’ll show you how you can do the same. But there is absolutely no reason against using one of the built-in templates (especially if you’re just starting your blogging career).\nBuilding my landing page will require a little bit more knowledge about HTML and CSS. Don’t be alarmed, though. You can easily follow along if you’ve understood how to work with classes and IDs (as described earlier or in my video).\nFirst, we need to set up index.qmd. This file determines what your blog’s landing page looks like. Delete all content from this file and replace it with\n\n\n index.qmd\n\n---\npage-layout: full\n---\n\nSecond, set up a grid system. Quarto uses the CSS-grid system. We can make use of that in index.qmd via\n\n\n index.qmd\n\n::: {.grid}\nCONTENT\n:::\n\nRecall that the dot (.) in CSS stands for class. So, the previous code means quarto will render your qmd-file into an html-file that contains a section (a div in HTML-speak) that is of class grid. We will fill this div with columns next.\nAdding columns is done by adding more divs. In this case, these divs are of the class g-col-*. Here, * describes how many columns the div should fill (out of 12 columns). The actual content is written with regular Markdown. This could look like so:\n\n\n index.qmd\n\n::: {.grid}\n\n::: {.g-col-6}\n![](profile2.jpg)\n:::\n\n::: {.g-col-6}\n# Hi, I'm Albert Rapp.\nI'm a mathematics PhD students with a passion for R, data visualization, Shiny and statistics/machine learning. On this blog, I share and teach what I learn.\n\nTo get started, you can check out my most popular content below. You can find me on [Twitter](https://twitter.com/rappa753) or\n[GitHub](https://github.com/AlbertRapp) and \n[YouTube](https://www.youtube.com/user/Alfrodo123). \nFeel free to reach out to me via [mail](mailto:blog@albert-rapp.de) and subscribe to my email newsletter.\n\n\n<iframe id=\"beehiiv-form\" src=\"https://embeds.beehiiv.com/9232d2a2-6e85-4beb-b8ed-1de94e9e4f01?slim=true\" data-test-id=\"beehiiv-embed\" frameborder=\"0\" scrolling=\"no\" style=\"margin: 0; border-radius: 0px !important; background-color: transparent; height: 75px\"></iframe>\n:::\n\nThis code will create the following landing page.\n\n\n\n\n\n\nAdd reactivity\nOur new landing page will look decent on large screens. But we should test what it looks like on small screens too.\nHowever, there’s no need to take out your phone. Use your browser and activate its responsive mode (ctrl+shift+M in Firefox). This way, you can see what your website would look like on phones or tablets.\n\n\n\nMobile view of the landing page we’ve just built.\n\n\nIf you’ve checked the view on phones, then you may notice that splitting the screen in two won’t fly. In this case, it’s better to put sections below each other. We can make that happen by adding g-col-md-* classes to our previous divs (and making the other classes full-width).\n\n\n index.qmd\n\n::: {.grid}\n\n::: {.g-col-12 .g-col-md-6}\n![](profile2.jpg)\n:::\n\n::: {.g-col-12 .g-col-md-6}\n# Hi, I'm Albert Rapp.\nI'm a mathematics PhD students with a passion for R, data visualization, Shiny and statistics/machine learning. On this blog, I share and teach what I learn.\n\nTo get started, you can check out my most popular content below. You can find me on [Twitter](https://twitter.com/rappa753) or\n[GitHub](https://github.com/AlbertRapp) and \n[YouTube](https://www.youtube.com/user/Alfrodo123). \nFeel free to reach out to me via [mail](mailto:blog@albert-rapp.de) and subscribe to my email newsletter.\n\n\n<iframe id=\"beehiiv-form\" src=\"https://embeds.beehiiv.com/9232d2a2-6e85-4beb-b8ed-1de94e9e4f01?slim=true\" data-test-id=\"beehiiv-embed\" frameborder=\"0\" scrolling=\"no\" style=\"margin: 0; border-radius: 0px !important; background-color: transparent; height: 75px\"></iframe>\n:::\n\nHere, .g-col-12 .g-col-md-6 means that the div will use 12 columns (full width) by default. But on screens of at least medium width (>=768px), the div will only take 6 columns. Congrats, you’ve now used responsive design. 🥳\n\n\n\nUse responsive design to make your landing page look good on small screens. I hope you’re not sick of my face yet 😆\n\n\nNext, we should use our landing page to highlight our best content. Think of this as a getting started guide for your readers. Here, I want to use centered columns of width 5. And to ensure that the columns are centered let us offset the first block by one column (on larger than medium screens).\nThis is accomplished by adding yet another class to the div, namely g-start-md-*. Here, *️ is the start column. And seeing this in action may look like this:\n\n\n index.qmd\n\n::: {.g-col-12 .g-col-md-5 .g-start-md-2}\n# Data Visualization\n\n- Blog post 1\n- Blog post 2\n- Blog post 3\n:::\n\n::: {.g-col-12 .g-col-md-5}\n# Stats/ML\n\n- This div...\n- ...required...\n- ...no offset (it's still in the first row)\n\n:::\n\n\n\n\nOutput of previous code chunk. Notice that the first column doesn’t start all the way to the left.\n\n\nFinally, you can always customize the appearance of a grid element by adding more custom classes (with .) or IDs (with #). Afterwards, you can target that class or ID in your SCSS file. This way, I aligned my bio with the bottom of the picture.\n\nHTML CodeContent of theme.scss\n\n\n\n\n index.qmd\n\n::: {.g-col-12 .g-col-md-6 #about-me-text}\n![](profile2.jpg)\n:::\n\n\n\n\n\n theme.scss\n\n#about-me-text {\n  align-items: end;\n  display: grid;\n}"
  },
  {
    "objectID": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#make-posts-robust-with-renv",
    "href": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#make-posts-robust-with-renv",
    "title": "The ultimate guide to starting a Quarto blog",
    "section": "Make posts robust with {renv}",
    "text": "Make posts robust with {renv}\nImagine that you have written a really cool blog post. And not just any cool blog post. I mean a SUPER DUPER cool blog post. Did you imagine that? Good. That’s your future when you start a blog. But don’t get too excited yet! Here’s the drama of this fairy tale.\nUnfortunately, your super cool blog post uses version 1.2.4. of {ggplot2}. This is unfortunate because right now we’re at version 3.3.6. And a lot has changed since your blog post was rendered last time.\nWhat’s worse is that you won’t be able to render your blog anymore if you ever need to re-render all blog posts. This can happen, for example, if you move your blog to a different format, e.g. Quarto.1\nLuckily, it’s possible to tell R which package versions it should use for your blog posts. To do so, you need to control the versions with {renv}. This is a package like any other package and you can install it with install.packages('renv').\nOnce you have installed {renv}, you can use it within your blog project. Actually, it is quite simple to set up. Here’s how {renv}’s docs describe the workflow.\n\n\nCall renv::init() to initialize a new project-local environment with a private R library,\nWork in the project as normal, installing and removing new R packages as they are needed in the project,\nCall renv::snapshot() to save the state of the project library to the lockfile (called renv.lock),\nContinue working on your project, installing and updating R packages as needed.\nCall renv::snapshot() again to save the state of your project library if your attempts to update R packages were successful, or call renv::restore() to revert to the previous state as encoded in the lockfile if your attempts to update packages introduced some new problems.\n\n\nSo, if you’ve just started your blog project, then renv:init() will set you up with a clean slate. This means that whenever you want to use a package that you have not used before in this project, you will need to install the package for this project (via install.packages()). Yes, this is true even if you have used, say, {ggplot2} in other projects.\nIn case you’re running into installation problems with {renv}, you’ll need to change your download method for new packages. Simply add the following line to your .Renviron file. Either find this file on your computer manually or open it by calling usethis::edit_r_environ() (if necessary from a not {renv}-controlled project).\n Sys.setenv(RENV_DOWNLOAD_FILE_METHOD = getOption(\"download.file.method\"))\nWhenever you have installed or updated packages in this project, then call renv::snapshot(). This will save all package information including version numbers into a so-called lock file renv.lock (which will appear in your projects main directory). Now, here’s what you need to do to make each of your blog posts robust against version changes.\n\nAfter finishing anew blog post, call renv::snapshot(). This will update your renv.lock file of your blog project.\nTake this updated renv.lock file and copy it into the directory of your newly finished blog post.\nIn your new blog post, add the following code chunk at the top of your blog post’s qmd-file (after the YAML header, of course).\n#| echo: false\n#| results: 'hide'\nrenv::use(lockfile = \"renv.lock\")\n\nThat’s it. You have successfully saved your blog post from dying by version change. Whenever your blog post needs to be re-rendered, this first code chunk will instruct R to use the package versions that are defined in this post’s lock-file.\nBut let me warn you that this is not a perfect solution. Your blog posts are still not invincible. For example, {renv} does not prevent problems when you have written your blog post on a Windows system and want to re-render it on Ubuntu. Possibly, some packages (e.g. {keyring}) behave differently on a different OS."
  },
  {
    "objectID": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#cookies-analytics-extensions-and-more-miscellaneous-stuff",
    "href": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#cookies-analytics-extensions-and-more-miscellaneous-stuff",
    "title": "The ultimate guide to starting a Quarto blog",
    "section": "Cookies, analytics, extensions and more miscellaneous stuff",
    "text": "Cookies, analytics, extensions and more miscellaneous stuff\nThere are many, many more cool features that Quarto has in store for your blog. Let me briefly mention a few more gems.\n\nAnalytics: You’re probably curious about how much attention your blog attracts. To find out, you can measure your website’s traffic with Google analytics. Simply register with Google and you will get a tracking ID. This ID can be added in your _quarto.yml file via google-analytics. For more information, check out the Quarto docs.\nCookie consent: It is rude and more importantly illegal to track your readers with cookies without their consent. So it is paramount that you ask your readers for permission if you use cookies. This can be done via a simple, yet ABSOLUTELY ANNOYING, pop-up window that asks for consent. Activate this window via cookie-consent: true in _quarto.yml.\nSocial card: A great place to share your blog posts is Twitter. But to get the most out of your promo tweets, your links should probably generate a Twitter card. Once again, you can enable this feature in your _quarto.yml file. And for custom styling of your Twitter card, please refer to the Quarto docs. Note that it takes some time (days) until Twitter registers your changed Twitter card information.\nExtensions: Quarto’s functionality can be leveled up with Extensions. For example, the code-filename extension helped me to include a file name at the top of many of this post’s code chunks. If you want to use the code-filename extensions as well, then proceed as follows. Of course, this works with any other extension as well.\n\nMake sure that your Quarto version is up-to-date. Extensions are a fairly new feature.\nInstall code-filename locally for your blog project: You will need to open a terminal at your blog’s working directory and execute\nquarto install extension quarto-ext/code-filename\nNotice that there is a new directory _extensions in your blog’s directory now. (This is not really a step that you have to do but still it’s nice to notice changes).\nAdd the new extension to your _quarto.yml file to activate it.\n\n\n _quarto.yml\n\nfilters:\n  - code-filename\n\nEnjoy your new feature. You can use it like\n``` {.yaml filename=_quarto.yml}\nfilters:\n    - code-filename\n```\n\n\nPDF Output: On Twitter, I was asked if my guide can include some info on Quarto’s PDF output options. In this context, this reminds me that a reader once emailed me and asked whether I can provide a PDF-file for one of my blog posts. So, I guess, some people may be interested in having PDF- as well as HTML-versions of your blog.\nIn principle, you could add an additional format to, say, your _metadata.yml file. In my case, this could look like\n\n\n posts/_metadata.yml\n\nformat:\n  html:\n    include-after-body: ../footer.html\npdf:\n  toc: true\n\nUnfortunately, I haven’t found a button or shortcut to render both outputs simultaneously in RStudio. But with the command line (which you can use in RStudio), it is simple. You’ll just have to call\nquarto render --to all\nThis will render all of your blog posts to both HTML and PDF. Now what’s cool is that a reader can easily access the PDF version by changing a post’s link e.g. from site-url/post_XY.html to site-url/post_XY.pdf.\nOf course, all of that will only work if your blog posts contain static elements that work in a PDF. Possibly, this can be cirumvented and Quarto’s docs on book formats (it’s a more pressing issue for books) may have some info on that. Personally, I avoid PDFs in my blog."
  },
  {
    "objectID": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#publish-your-blog-online",
    "href": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#publish-your-blog-online",
    "title": "The ultimate guide to starting a Quarto blog",
    "section": "Publish your blog online",
    "text": "Publish your blog online\nMy preferred publisher is Netlify. It’s really easy to set up to publish with them. So, let me describe how that works. Mainly, you have two choices once you have registered with Netlify.\n\nGive Netlify access to the GitHub repository that your blog lives in. Once access is granted, Netlify will deploy the most recent version of your blog whenever you push changes to your repository.\nAlternatively, you can use the command line. To do so, open a new terminal and set its working directory to your blog’s working directory. Then, you only need to call quarto publish netlify and follow the instructions on the terminal.\n\nYou can use the latter method for publishing at other services as well. For more information, let me refer you to Quarto’s docs. Also, let me mention that Netlify will give you a Netlify URL, e.g. tourmaline-faloodeh-4b36ef.netlify.com. However, I think it’s advisable to buy a custom domain and link it to the Netlify URL (Netlify provides help on that)."
  },
  {
    "objectID": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#writing-tips",
    "href": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#writing-tips",
    "title": "The ultimate guide to starting a Quarto blog",
    "section": "Writing tips",
    "text": "Writing tips\nI don’t consider my writing particularly good and I am sure that I use a fair share of imperfect grammar. But a small language barrier shouldn’t stop me or you from publishing a blog. And before I leave you, let me share a few pearls of writing wisdom that I found helpful.\n\nConsistency is key: Blogging is like any other any new habit: It takes time until it sticks. That’s why it is really important that you try to post something regularly. If you have trouble overcoming your weaker self I can recommend the book Atomic Habits by James Clear. It contains TONS of great strategies to make a new habit stick.\nPerfectionism is the enemy: One major obstacle that can stop you from writing regularly is perfectionism. Don’t worry if your blog post is perfect, or original, or super exciting or whatever. It is maddingly easy to waste hours “polishing” a blog post to perfection. So, if you catch yourself polishing a lot. Then maybe think about just hitting publish.\nExpect the indifference: When you publish a blog post, you never know if people will be excited about what you wrote. Chances are that they won’t be. In fact, if you just got started, then count on it! This may sound harsh but it’s actually great. Use this knowledge to counter your need for perfectionism. This can help you to work on your consistency until eventually people care about what you write.\nGet feedback on your writing: Nothing makes writing more boring than not getting any feedback. I recommend that you advertise your posts on Rweekly and Twitter. Depending on how people engage with your post (or not), you will have feedback about how much your community cares about the topic you chose. Eventually, you will hit on a topic that people care about. Then, it can be incredibly fun to double down on this topic or engage in conversations about it.\n\nFor more tips, you can check out David Perell’s or Dickie Bush’s work. I learned most of the above writing tips from them."
  },
  {
    "objectID": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#closing",
    "href": "posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html#closing",
    "title": "The ultimate guide to starting a Quarto blog",
    "section": "Closing",
    "text": "Closing\nPhew! This is the longest and most in-depth guide that I ever wrote. And this was also the scariest post I ever wrote. When I started on this Quarto endeavor almost two months ago, I had literally ZERO knowledge about\n\nHTML/CSS\nResponsive design\nVersion control with {renv}\n\nand many other small things I’ve learned since then. Thus, I see this guide as a culmination of everything I’ve learned and I am oddly proud about it.\nI hope that you enjoyed this guide and that it serves you well. Please reach out to me if you run into any trouble with what I wrote. I am more than happy to improve my guide so that it can help more people.\nAnd don’t forget to stay in touch via my Newsletter, Twitter or my RSS feed. See you next time!"
  },
  {
    "objectID": "posts/05_exploratory_intro_plotly/05_exploratory_intro_quarto.html",
    "href": "posts/05_exploratory_intro_plotly/05_exploratory_intro_quarto.html",
    "title": "An Exploratory Introduction to the Plotly Package",
    "section": "",
    "text": "Currently, I am quite curious about interactive plots which is why I am reading Scott Murray’s highly recommendable book on D3. For those of you who don’t know it, D3.js is a JavaScript library that is great for creating amazing interactive Data-Driven-Documents on the web.\nUnfortunately, compared to ggplot2, D3’s learning curve feels quite steep and I am not yet able to work with it yet. Fortunately, there are other interactive graphing libraries out there that I can use to get a first feel for creating interactive plots. For instance, there is another JavaScript library plotly.js, which is built on top of D3 and can be easily used in R through the plotly package.\nTherefore, I decided to play around with this R package in the hope of figuring out how it works. Also, I summarized what I played around with in this blog post by writing what I like to call an “exploratory introduction”.\nAs the name implies, this is not really a formal introduction to plotly and more of an experience report. Nevertheless, I suspect that this can be useful for people who have already knowledge about ggplot2 and want to get started with plotly as well.\nFurther, in case you do not want to read my informal commentary, you can also check out the video version of this blog post. Think of it as a summary of this blog post.\nAs of now, I plan on doing a similar exploratory introduction to r2d3 which is an R interface to D3. Possibly, this will then be combined with knowledge I gathered from Scott Murray’s book."
  },
  {
    "objectID": "posts/05_exploratory_intro_plotly/05_exploratory_intro_quarto.html#one-side-note-before-we-dive-in",
    "href": "posts/05_exploratory_intro_plotly/05_exploratory_intro_quarto.html#one-side-note-before-we-dive-in",
    "title": "An Exploratory Introduction to the Plotly Package",
    "section": "One Side Note before We Dive in",
    "text": "One Side Note before We Dive in\nAccording to Wikipedia, JavaScript “is one of the core technologies of the World Wide Web”. Surprisingly, JavaScript’s ubiquity on the interwebs also messed with this blog’s theme template such that font formatting was completely destroyed when I included an interactive plotly plot.\nTo work around this issue, I had to save the plots in an auxillary html-file and include it as a separate frame. This is why you will find that, here, all plots are saved in a variable (for later export) even if it does not really make sense to save it for plotting purposes.\nFinally, be aware that, for some reason, interactive plotly plots are quite large such that it might take some time to load them all.\nEDIT (July 10, 2022): I have moved this blog to Quarto. Now, the issues with the theme do not persist anymore. However, I have left the remaining blog post unchanged."
  },
  {
    "objectID": "posts/05_exploratory_intro_plotly/05_exploratory_intro_quarto.html#from-ggplot2-to-plotly",
    "href": "posts/05_exploratory_intro_plotly/05_exploratory_intro_quarto.html#from-ggplot2-to-plotly",
    "title": "An Exploratory Introduction to the Plotly Package",
    "section": "From ggplot2 to plotly",
    "text": "From ggplot2 to plotly\nLet us begin by transforming a ggplot to a plotly plot. Using a built-in function from the plotly package, it is straight-forward to convert a ggplot.\n\nlibrary(tidyverse)\nlibrary(plotly)\np <- mpg %>% \n  ggplot(aes(hwy, cty, fill = class)) +\n  geom_jitter(shape = 21, size = 2, alpha = 0.5)\n\nplotly_p <- ggplotly(p) \nplotly_p \n\n\n\n\n\nMarvel at what one can do here: * Get additional information by hovering your cursor over a point * Filter classes by clicking on the corresponding legend items * Click and draw a rectangle with your cursor to zoom into the plot\nBut there is one thing that bothers me. If I move the cursor across the plot window, then the plotly options bar at the top of the window overlaps the “class” legend label. Can I fix this by moving the legend to the bottom?\n\np <- p + theme(legend.position = \"bottom\")\nggplotly(p)\n\n\n\n\n\nHmm, it appears that the legend cannot be moved by this. Do other changes in theme() get registered? Let’s check by applying a theme.\n\np <- p + theme_light()\nggplotly(p) \n\n\n\n\n\nApparently, at least some changes will be conducted. Let’s see, if I can use layout() as shown in the getting started documentation of plotly to move the legend.\nThe R documentation of the layout() function is somewhat minimalistic. Basically, it refers to the online plotly reference manual. Using the table of content on that website, one can easily find the layout options.\nI believe the options xanchor and yanchor are exactly what I need as both of them can be set to options like left, bottom, etc. Though, I wonder why the value auto was not changed by ggplotly() as the initial plot p contains legend.position = \"bottom\".\nIn any case, I am not a hundred percent sure what kind of syntax layout() requires. An example in the R documentaion would have been nice. Well, let’s try the straight-forward approach then.\n\np %>% \n  ggplotly() %>% \n  layout(xanchor = \"center\")\n\nWell, that didn’t go as expected, but the warning displays valid attributes and legend is among them. Upon closer inspection, I also realize that, according to the reference manual, xanchor and yanchor have parent layout.legend. So, I guess, I will have to use this somehow. Maybe, pass a list to legend via layout()?\n\np_layout <- p %>% \n  ggplotly() %>% \n  layout(legend = list(xanchor = \"center\")) \np_layout\n\n\n\n\n\nAha! At least this did not crash again but the resulting plot does not look as I would expect it to. Let’s see what happens when we change yanchor as well and then we’ll go from there.\n\np_layout <- p %>% \n  ggplotly() %>% \n  layout(legend = list(\n    xanchor = \"center\",\n    yanchor = \"bottom\"\n  )) \np_layout\n\n\n\n\n\nUnsurprisingly, this did not help at all. In retrospect, I don’t know how I could think that changing yanchor as well would magically cure things. Again, referring back to the manual (I should really read the descriptions instead of relying on the possible values), it appears that xanchor only sets a reference points for the option x. Same thing for yanchor and y.\nSo, how about changing x and y instead? Possible values for both options range from -2 to 3, so my best guess is that ranges from 0 to 1 refer to the window the points are plotted in. Consequently, moving the legend to the bottom could be as easy as using a positive x value and negative y value which are both close to zero.\n\np_layout <- p %>% \n  ggplotly() %>% \n  layout(legend = list(x = 0.1, y = -0.1))\np_layout\n\n\n\n\n\nNow, this brings us closer to what I had in mind when I set legend.position = \"bottom\" in theme(). Possibly, we can change the orientation of the legend from vertical to horizontal, tweak the x and y values a bit and then we’re there. Scrolling through the manual (again), reveals that there is an option orientation which can be set to h. This sounds promising.\n\np_layout <- p %>% \n  ggplotly() %>% \n  layout(legend = list(\n    x = 0.1, \n    y = -0.2, \n    orientation = \"h\"\n  )) \np_layout\n\n\n\n\n\nNice! Finally, I am satisfied. Out of curiosity, let us investigate what had happened, if we had set orientation to \"h\" from the start.\n\np_layout <- p %>% \n  ggplotly() %>% \n  layout(legend = list(orientation = \"h\")) \np_layout\n\n\n\n\n\nThis already looks nice enough, so our manual tweaking was not technically necessary. But then again, this does not recreate what legend.position = \"bottom\" usually does. Now that we understand how layout() works, we can roam the reference manual and try to tweak the legend box. Let’s try to change a couple of things. This does not have to be pretty, we only want to see how plotly works.\n\np_layout <- p %>% \n  ggplotly() %>% \n  layout(\n    legend = list(\n      orientation = \"h\",\n      borderwidth = 3,\n      bgcolor = \"grey\",\n      bordercolor = \"red\",\n      font = list(\n        color = \"white\",\n        family = \"Gravitas One\",\n        size = 15\n      ),\n      title = list(\n        text = \"Class\",\n        side = \"top\",\n        font = list(\n          color = \"white\",\n          family = \"Gravitas One\",\n          size = 15\n        )\n      )\n    )\n  ) \np_layout\n\n\n\n\n\nNote how we have used lists in lists (in lists) to customize the legend. Interestingly, our initial blunder of ignoring the “parent” resp. the hierarchy of options earlier helped to understand that as an option’s parent’s name gets longer, e.g. layout.legend.title.font, we will have to use more convoluted lists to change that option."
  },
  {
    "objectID": "posts/05_exploratory_intro_plotly/05_exploratory_intro_quarto.html#creating-a-plotly-chart-manually",
    "href": "posts/05_exploratory_intro_plotly/05_exploratory_intro_quarto.html#creating-a-plotly-chart-manually",
    "title": "An Exploratory Introduction to the Plotly Package",
    "section": "Creating a Plotly Chart Manually",
    "text": "Creating a Plotly Chart Manually\nSince we have learned how to tweak a plotly object, we might as well figure out how to create one without having to use ggplotly(). It is not that I want to avoid using ggplot altogether but, in principle, it cannot hurt if we can understand plotly’s implementation in the R package plotly.\nSo, from the book Interactive web-based data visualization with R, plotly, and shiny I gather that the plotly R package implements the JavaScript plotly.js library via a Grammar of Graphics approach. Thus, it works similar to ggplot2 in the sense that we can add layers of graphical objects to create a plot. In plotly’s case, we pass a plotly object from one add_* layer to the next (via %>%).\n\nplt <- mpg %>% \n  plot_ly() %>% \n  add_markers(x = ~hwy, y = ~cty, color = ~class)\nplt\n\n\n\n\n\nNotice the ~. These are used to ensure that the variables are mapped from the data (similar to aes() in ggplot2). Alternatively, one could also use x = mpg$hwy to create the same plot.\nBecause we can see a lot of overplotting, let us jitter the points. Unfortunately, I couldn’t find a built-in option for that. Therefore, let’s do the jittering manually.\n\nset.seed(123)\njitter_hwy <- 2\njitter_cty <- 1\njittered_mpg <- mpg %>% \n  mutate(\n    hwy = hwy + runif(length(hwy), -jitter_hwy, jitter_hwy),\n    cty = cty + runif(length(cty), -jitter_cty, jitter_cty)\n  )\nplt <- jittered_mpg %>% \n  plot_ly() %>% \n  add_markers(x = ~hwy, y = ~cty, color = ~class)\nplt \n\n\n\n\n\nRegarding the customization of the points aka markers, we can pass a list of options (taken from the reference manual again) to the marker argument in add_markers().\n\nset.seed(123)\nplt <- jittered_mpg %>% \n  plot_ly() %>% \n  add_markers(\n    x = ~hwy, \n    y = ~cty, \n    color = ~class,\n    marker = list(\n      size = 8,\n      opacity = 0.6,\n      line = list(color = \"black\", width = 2)\n    )\n  ) \nplt\n\n\n\n\n\nAlternatively, and what I find surprisingly convenient, we can leave our initial plotly object as it is and pass it to the style() function. This functions works just like the layout() function we have seen before.\n\nplt <- jittered_mpg %>% \n  plot_ly() %>% \n  add_markers(\n    x = ~hwy, \n    y = ~cty, \n    color = ~class\n  ) %>% \n  style(\n    marker = list(\n      size = 8,\n      opacity = 0.6,\n      line = list(color = \"black\", width = 2)\n    )\n  ) \nplt \n\n\n\n\n\nSimilarly, we could pass this along to layout() if we want to customize the legend box again.\n\nplt_layout <- plt %>% \n  layout(\n    legend = list(\n      orientation = \"h\",\n      borderwidth = 3,\n      bgcolor = \"grey\",\n      bordercolor = \"red\",\n      font = list(\n        color = \"white\",\n        family = \"Gravitas One\",\n        size = 15\n      ),\n      title = list(\n        text = \"Class\",\n        side = \"top\",\n        font = list(\n          color = \"white\",\n          family = \"Gravitas One\",\n          size = 15\n        )\n      )\n    )\n  )\nplt_layout\n\n\n\n\n\nInteresting! For some obscure reason the exact same legend command behaves differently now. Honestly, I have no clue what is going on here. If I had to hazard a guess, I would say that during the conversion of a ggplot object to a plotly object via ggplotly() some default values were implemented that cause the change but this is just a hunch. Possibly, this is connected to xanchor or yanchor.\nIn any case, we got a glimpse of how the plotly R package works which uses the JavaScript library plotly.js to create interactive plots. Also, we have learned how to convert a ggplot2 object to a plotly object and how we can customize this further.\nInterestingly, when converting from ggplot2 to plotly, the pop-up window that appears when you hover over a point is already customized compared to the default from plot_ly(). Did you notice the difference already?\nSo, in order to end our “exploratory introduction” let us adjust the hovertemplate according to the description in the reference manual. Here, we will use \\n for line breaks.\n\nplt <- plt %>% \n  style(hovertemplate = \"hwy: %{x:.2f}\\ncty: %{y:.2f}\") %>% \n  layout(legend = list(orientation = \"h\", y = -0.2)) \nplt\n\n\n\n\n\nNotice how the class labels appear outside of the box. The reference manual refers to this position as “secondary” box. To get rid of this, we simply add <extra></extra> to our hover template. Unfortunately, it appears as if what is not displayed in the primary box cannot be used as part of the hover template.\nThus, we cannot use %{color}. Instead, we simply map class to the text attribute of the markers as well. Then, we can use %{text}.\n\nplt <- jittered_mpg %>% \n  plot_ly() %>% \n  add_markers(x = ~hwy, y = ~cty, color = ~class, text = ~class) %>% \n  style(\n    marker = list(\n      size = 8,\n      opacity = 0.6,\n      line = list(color = \"black\", width = 2)\n    )\n  ) %>% \n  style(hovertemplate = \"hwy: %{x:.2f}\\ncty: %{y:.2f}\\nclass: %{text} <extra></extra>\") %>% \n  layout(legend = list(orientation = \"h\", y = -0.2))\nplt\n\n\n\n\n\nAll right! That’s enough exciting plotting action for today. Hope you enjoyed this blog post and see you next time."
  },
  {
    "objectID": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#intro",
    "href": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#intro",
    "title": "How to collect dataviz from Twitter into your note-taking system",
    "section": "Intro",
    "text": "Intro\nIt is mid-April and the #30daychartchallenge is well on its way. One glace at the hashtag’s Twitter feed suffices to realize that there are great contributions. That’s a perfect opportunity to collect data viz examples for future inspirations.\nIdeally, I can scroll through Twitter and with a few clicks incorporate these contributions straight into my Obsidian or any other Markdown-based note-taking system. Unfortunately, rtweet’s snapshot function does not seem to work anymore. So, let’s build something on our own that gets the job done. The full script can be found on GitHub gist. Here’s what we will need:\n\nTwitter app bearer token (to access Twitter’s API) - I’ll show you how to get that\nElevated API access (just a few clicks once you have a bearer token)\nDummy mail account to send tweets to"
  },
  {
    "objectID": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#overview",
    "href": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#overview",
    "title": "How to collect dataviz from Twitter into your note-taking system",
    "section": "Overview",
    "text": "Overview\nBefore we begin, let me summarize what kind of note-taking process I have in mind:\n\nStroll through Twitter and see great data viz on twitter.\nSend tweet link and a few comments via mail to a dummy mail account\nA scheduled process accesses the dummy mail account and scans for new mails from authorized senders.\nIf there is a new mail, R extracts tweet URL and uses Twitter’s API to download the tweet’s pictures and texts.\nA template Markdown file is used to create a new note that contains the images and texts.\nMarkdown file is copied to your note-taking system within your file system.\nIdeally, your Markdown template contains tags like #dataviz and #twitter so that your new note can be easily searched for.\nNext time you look for inspiration, stroll through your collections or search for comments."
  },
  {
    "objectID": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#preparations",
    "href": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#preparations",
    "title": "How to collect dataviz from Twitter into your note-taking system",
    "section": "Preparations",
    "text": "Preparations\nOk, we know what we want to accomplish. Time to get the prelims done. First, we will need a Twitter developer account. Then, we have to mask sensitive information in our code. If you already have a twitter app resp. a bearer token and know the keyring package, feel free to skip this section.\n\nGet Twitter developer account\nLet’s create a developer account for Twitter. Unfortunately, there is no way to get such an account without providing Twitter with your phone number. Sadly, if this burden on your privacy is a problem for you, then you cannot proceed. Otherwise, create an account at developer.twitter.com.\nIn your developer portal, create a project. Within this project create an app. Along the way, you will get a bunch of keys, secrets, IDs and tokens. You will see them only once, so you will have to save them somewhere. I suggest saving them into a password manager like bitwarden.\nWhen you create your app or shortly after, you will need to set the authentication settings. I use OAuth 2.0. This requires\n\ntype of app: Automated bot or app\nCallback URI / Redirect URI: http://127.0.0.1:1410 (DISCLAIMER: This is magic to me but the rtweet docs - or possibly some other doc (not entirely sure anymore)- taught me to set up an app that way)\nWebsite URL: Your Twitter link (in my case https://twitter.com/rappa753)\n\nNext, you will likely need to upgrade your project to ‘elevated’ status. This can be done for free on your project’s dashboard. From what I recall, you will have to fill out a form and tell Twitter what you want to do with your app. Just be honest and chances are that your request will immediately be granted. Just be yourself! What could possibly go wrong? Go get the girl elevated status (ahhh, what a perfect opportunity for a Taylor song).\n\n\n\n\n\nClick on detailed features to apply for higher access\n\n\n\n\n\n\nHow to embed your bearer token and other sensitive material in your code\nUse the keyring package to first save secrets via key_set and then extract them in your session via key_get(). This way, you won’t share your sensitive information by accident when you share your code (like I do). In this post, I do this for my bearer token, my dummy mail, my dummy mail’s password and for the allowed senders (that will be the mail where the tweets come from).\n\n\n\n\nbearer_token <- keyring::key_get('twitter-bearer-token', keyring = 'blogpost')\nuser_mail <- keyring::key_get('dataviz-mail', keyring = 'blogpost')\npassword_mail <- keyring::key_get('dataviz-mail-password', keyring = 'blogpost')\nallowed_senders <- keyring::key_get('allowed_senders', keyring = 'blogpost')\n\nThe allowed_senders limitation is a precaution so that we do not accidentally download some malicious spam mail from God knows who onto our computer. I am no security expert but this feels like a prudent thing to do. If one of you fellow readers knows more about this security business, feel kindly invited to reach out to me with better security strategies."
  },
  {
    "objectID": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#what-to-do-once-we-have-a-url",
    "href": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#what-to-do-once-we-have-a-url",
    "title": "How to collect dataviz from Twitter into your note-taking system",
    "section": "What to do once we have a URL",
    "text": "What to do once we have a URL\nLet’s assume for the sake of this section that we already extracted a tweet URL from a mail. Here’s the URL that we will use. In fact, it’s Christian Gebhard’s tweet that inspired me to start this project. From the URL we can extract the tweet’s ID (the bunch of numbers after /status/). Also, we will need the URL of Twitter’s API.\n\nlibrary(stringr) # for regex matching\nlibrary(dplyr) # for binding rows and pipe\ntweet_url <- 'https://twitter.com/c_gebhard/status/1510533315262042112'\ntweet_id <- tweet_url %>% str_match(\"status/([0-9]+)\") %>% .[, 2]\nAPI_url <- 'https://api.twitter.com/2/tweets'\n\n\nUse GET() to access Twitter API\nNext, we use the GET() function from the httr package to interact with Twitter’s API.\n\nlibrary(httr) # for API communication\n\nauth <- paste(\"Bearer\", bearer_token) # API needs format \"Bearer <my_token>\"\n\n# Make request to API\nrequest <- GET(\n  API_url, \n  add_headers(Authorization = auth), \n  query = list(\n    ids = tweet_id, \n    tweet.fields = 'created_at', # time stamp\n    expansions = 'attachments.media_keys,author_id', \n    # necessary expansion fields for img_url\n    media.fields = 'url' # img_url\n  )\n) \nrequest\n\nResponse [https://api.twitter.com/2/tweets?ids=1510533315262042112&tweet.fields=created_at&expansions=attachments.media_keys%2Cauthor_id&media.fields=url]\n  Date: 2022-07-21 17:47\n  Status: 200\n  Content-Type: application/json; charset=utf-8\n  Size: 690 B\n\n\nSo, how do we know how to use the GET() function? Well, I am no expert on APIs but let me try to explain how I came up with the arguments I used here.\nRemember those toys you would play with as a toddler where you try to get a square through a square-shaped hole, a triangle through a triangle-shaped hole and so on? You don’t? Well, neither do I. Who remembers that stuff from very early childhood?\nBut I hear that starting a sentence with “Remember those…” is good for building a rapport with your audience. So, great! Now that we feel all cozy and connected, I can tell you how I managed to get the API request to work.\nAnd the truth is actually not that far from the toddler “intelligence test”. First, I took a look at a help page from Twitter’s developer page. Then, I hammered at the GET() function until its output contained a URL that looks similar to the example I found. Here’s the example code I was aiming at.\ncurl --request GET 'https://api.twitter.com/2/tweets?ids=1263145271946551300&\nexpansions=attachments.media_keys&\nmedia.fields=duration_ms,height,media_key,preview_image_url,public_metrics,type,url,width,alt_text' \n--header 'Authorization: Bearer $BEARER_TOKEN'\nThis is not really R code but it looks like usually you have to feed a GET request with a really long URL. In fact, it looks like the URL needs to contain everything you want to extract from the API. Specifically, the structure of said URL looks like\n\nthe API’s base URL (in this case https://api.twitter.com/2/tweets)\na question mark ?\npairs of keywords (like ids) and a specific value, e.g. ids=1263145271946551300, that are connected via &\n\nTherefore, it is only a matter of figuring out how to make the output of GET() deliver this result. Hints on that came from GET() examples in the docs.\n\nGET(\"http://google.com/\", path = \"search\", query = list(q = \"ham\"))\n\nResponse [http://www.google.com/search?q=ham]\n  Date: 2022-07-21 17:47\n  Status: 200\n  Content-Type: text/html; charset=ISO-8859-1\n  Size: 122 kB\n<!doctype html><html lang=\"de\"><head><meta charset=\"UTF-8\"><meta content=\"/im...\ndocument.documentElement.addEventListener(\"submit\",function(b){var a;if(a=b.t...\nvar a=window.performance;window.start=Date.now();a:{var b=window;if(a){var c=...\nvar f=this||self;var g,h=null!=(g=f.mei)?g:1,m,n=null!=(m=f.sdo)?m:!0,p=0,q,r...\ne);var l=a.fileName;l&&(b+=\"&script=\"+c(l),e&&l===window.location.href&&(e=do...\nvar c=[],e=0;window.ping=function(b){-1==b.indexOf(\"&zx\")&&(b+=\"&zx=\"+Date.no...\nvar k=this||self,l=function(a){var b=typeof a;return\"object\"==b&&null!=a||\"fu...\nb}).join(\" \"))};function w(){var a=k.navigator;return a&&(a=a.userAgent)?a:\"\"...\n!1}e||(d=null)}}else\"mouseover\"==b?d=a.fromElement:\"mouseout\"==b&&(d=a.toElem...\nvar a=document.getElementById(\"st-toggle\"),b=document.getElementById(\"st-card...\n...\n\nGET(\"http://httpbin.org/get\", add_headers(a = 1, b = 2))\n\nResponse [http://httpbin.org/get]\n  Date: 2022-07-21 17:47\n  Status: 200\n  Content-Type: application/json\n  Size: 403 B\n{\n  \"args\": {}, \n  \"headers\": {\n    \"A\": \"1\", \n    \"Accept\": \"application/json, text/xml, application/xml, */*\", \n    \"Accept-Encoding\": \"deflate, gzip, br\", \n    \"B\": \"2\", \n    \"Host\": \"httpbin.org\", \n    \"User-Agent\": \"libcurl/7.68.0 r-curl/4.3.2 httr/1.4.3\", \n    \"X-Amzn-Trace-Id\": \"Root=1-62d9913b-1838afb6392fdf321bc3479c\"\n...\n\n\nSo, the first example shows how an argument query can be filled with a list that creates the URL we need. The second examples shows us that there is something called add_headers(). Do I know exactly what that is? I mean, from a technical perspective of what is going on behind the scenes? Definitely not. But Twitter’s example request had something called header. Therefore, add_headers() is probably something that does what the Twitter API expects.\nAnd where do the key-value pairs come from? I found these strolling through Twitter’s data dictionary. Thus, a GET() request was born and I could feel like a true Hackerman.\n\nauth <- paste(\"Bearer\", bearer_token) # API needs format \"Bearer <my_token>\"\n\n# Make request to API and parse to list\nrequest <- GET(\n  API_url, \n  add_headers(Authorization = auth), \n  query = list(\n    ids = tweet_id, \n    tweet.fields = 'created_at', # time stamp\n    expansions = 'attachments.media_keys,author_id', \n    # necessary expansion fields for img_url\n    media.fields = 'url' # img_url\n  )\n) \n\nAlright, we successfully requested data. Now, it becomes time to parse it to something useful. The content() function will to that.\n\nparsed_request <- request %>% content('parsed')\nparsed_request\n\n$data\n$data[[1]]\n$data[[1]]$attachments\n$data[[1]]$attachments$media_keys\n$data[[1]]$attachments$media_keys[[1]]\n[1] \"3_1510533145334104067\"\n\n\n\n$data[[1]]$id\n[1] \"1510533315262042112\"\n\n$data[[1]]$author_id\n[1] \"1070306701\"\n\n$data[[1]]$created_at\n[1] \"2022-04-03T08:23:01.000Z\"\n\n$data[[1]]$text\n[1] \"#30DayChartChallenge #Day3 - Topic: historical\\n\\nBack to the Shakespeare data! Hamlet is is longest play, the comedies tend to be shorter.\\n\\nTool: #rstats\\nData: kaggle users LiamLarsen, aodhan\\nColor-Scale: MetBrewer\\nFonts: Niconne, Noto Sans (+Mono)\\nCode: https://t.co/iXAbniQDCb https://t.co/JCNrYH9uP4\"\n\n\n\n$includes\n$includes$media\n$includes$media[[1]]\n$includes$media[[1]]$media_key\n[1] \"3_1510533145334104067\"\n\n$includes$media[[1]]$type\n[1] \"photo\"\n\n$includes$media[[1]]$url\n[1] \"https://pbs.twimg.com/media/FPZ95H0XwAMHA8q.jpg\"\n\n\n\n$includes$users\n$includes$users[[1]]\n$includes$users[[1]]$id\n[1] \"1070306701\"\n\n$includes$users[[1]]$name\n[1] \"Christian Gebhard\"\n\n$includes$users[[1]]$username\n[1] \"c_gebhard\"\n\n\n\n\nExtract tweet data from what the API gives us and download images\nWe have seen that parsed_request is basically a large list that contains everything we requested from the API. Unfortunately, it is a highly nested list, so we have to do some work to extract the parts we actually want. pluck() from the purrr package is our best friend on this one. Here’s all the information we extract from the parsed_request.\n\nlibrary(purrr) # for pluck and map functions\n# Extract necessary information from list-like structure\ntweet_text <- parsed_request %>% \n  pluck(\"data\", 1, 'text') \ntweet_text\n\n[1] \"#30DayChartChallenge #Day3 - Topic: historical\\n\\nBack to the Shakespeare data! Hamlet is is longest play, the comedies tend to be shorter.\\n\\nTool: #rstats\\nData: kaggle users LiamLarsen, aodhan\\nColor-Scale: MetBrewer\\nFonts: Niconne, Noto Sans (+Mono)\\nCode: https://t.co/iXAbniQDCb https://t.co/JCNrYH9uP4\"\n\ntweet_user <-  parsed_request %>% \n  pluck(\"includes\", 'users', 1, 'username')\ntweet_user\n\n[1] \"c_gebhard\"\n\n# We will use the tweet date and time as part of unique file names\n# Replace white spaces and colons (:) for proper file names\ntweet_date <- parsed_request %>% \n  pluck(\"data\", 1, 'created_at') %>% \n  lubridate::as_datetime() %>% \n  str_replace(' ', '_') %>% \n  str_replace_all(':', '')\ntweet_date\n\n[1] \"2022-04-03_082301\"\n\nimg_urls <- parsed_request %>% \n  pluck(\"includes\", 'media') %>% \n  bind_rows() %>% # bind_rows for multiple pictures, i.e. multiple URLS\n  filter(type == 'photo') %>% \n  pull(url)\nimg_urls\n\n[1] \"https://pbs.twimg.com/media/FPZ95H0XwAMHA8q.jpg\"\n\n\nNext, download all the images via the img_urls and download.file(). We will use walk2() to download all files (in case there are multiple images/URLs) and save the files into PNGs that are named using the unique tweet_date IDs. Remember to set mode = 'wb' in download.file(). I am not really sure why but without it you will save poor quality images.\n\n# Download image - set mode otherwise download is blurry\nimg_names <- paste('tweet', tweet_user, tweet_date, seq_along(img_urls), sep = \"_\")\nwalk2(img_urls, img_names, ~download.file(.x, paste0(.y, '.png'), mode = 'wb'))\n\nSo let’s do a quick recap of what we have done so far. We\n\nAssembled an API request\nParsed the return of the request\nCherrypicked the information that we want from the resulting list\nUsed the image URLs to download and save the files to our working directory.\n\nLet’s cherish this mile stone with a dedicated function.\n\nrequest_twitter_data <- function(tweet_url, bearer_token) {\n  # Extract tweet id by regex\n  tweet_id <- tweet_url %>% str_match(\"status/([0-9]+)\") %>% .[, 2]\n  auth <- paste(\"Bearer\", bearer_token) # API needs format \"Bearer <my_token>\"\n  API_url <- 'https://api.twitter.com/2/tweets'\n  \n  # Make request to API and parse to list\n  parsed_request <- GET(\n    API_url, \n    add_headers(Authorization = auth), \n    query = list(\n      ids = tweet_id, \n      tweet.fields = 'created_at', # time stamp\n      expansions='attachments.media_keys,author_id', \n      # necessary expansion fields for img_url\n      media.fields = 'url' # img_url\n    )\n  ) %>% content('parsed')\n  \n  # Extract necassary information from list-like structure\n  tweet_text <- parsed_request %>% \n    pluck(\"data\", 1, 'text') \n  \n  tweet_user <-  parsed_request %>% \n    pluck(\"includes\", 'users', 1, 'username')\n  \n  # Make file name unique through time-date combination\n  # Replace white spaces and colons (:) for proper file names\n  tweet_date <- parsed_request %>% \n    pluck(\"data\", 1, 'created_at') %>% \n    lubridate::as_datetime() %>% \n    str_replace(' ', '_') %>% \n    str_replace_all(':', '')\n  \n  img_urls <- parsed_request %>% \n    pluck(\"includes\", 'media') %>% \n    bind_rows() %>% \n    filter(type == 'photo') %>% \n    pull(url)\n  \n  # Download image - set mode otherwise download is blurry\n  img_names <- paste('tweet', tweet_user, tweet_date, seq_along(img_urls), sep = \"_\")\n  walk2(img_urls, img_names, ~download.file(.x, paste0(.y, '.png'), mode = 'wb'))\n  \n  # Return list with information\n  list(\n    url = tweet_url,\n    text = tweet_text,\n    user = tweet_user,\n    file_names = paste0(img_names, '.png'),\n    file_paths = paste0(getwd(), '/', img_names, '.png')\n  )\n}\n\n\n\nFill out Markdown template using extracted information and images\nWe have our images and the original tweet now. Thanks to our previous function, we can save all of the information in a list.\n\nrequest <- request_twitter_data(tweet_url, bearer_token)\n\nSo, let’s bring all that information into a Markdown file. Here is the template.md file that I have created for this joyous occasion.\n\nlibrary(readr) # for reading and writing files from/to disk\ncat(read_file('template.md'))\n\n#dataviz #twitter\n\n![[insert_img_name_here]]\n\n### Original Tweet\n\ninsert_text_here\n\nOriginal: insert_URL_here\n\n### Original Mail\n\ninsert_mail_here\n\n\nAs you can see, I started the Markdown template with two tags #dataviz and #twitter. This helps me to search for a specific dataviz faster. Also, I have already written out the Markdown syntax for image imports ![[...]] and added a placeholder insert_img_name_here. This one will be replaced by the file path to the image. Similarly, other placeholders like insert_text_here and insert_mail_here allow me to save the tweet and the mail content into my note taking system too.\nTo do so, I will need a function that replaces all the placeholders. First, I created a helper function that changes the image import placeholder properly, when there are multiple images.\n\nmd_import_strings <- function(file_names) {\n  paste0('![[', file_names, ']]', collapse = '\\n') \n}\n\nThen, I created a function that takes the request list that we got from calling our own request_twitter_data() function and iteratively uses str_replace_all(). This iteration is done with reduce2() which will replace all placeholders in template.md .\n\nlibrary(tibble) # for easier readable tribble creation\n# Replace the placeholders in the template\n# We change original mail place holder later on\nreplace_template_placeholder <- function(template_name, request) {\n  # Create a dictionary for what to replace in template\n  replace_dict <- tribble(\n    ~template, ~replacement,\n    '\\\\!\\\\[\\\\[insert_img_name_here\\\\]\\\\]', md_import_strings(request$file_names),\n    'insert_text_here', request$text %>% str_replace_all('#', '(#)'),\n    'insert_URL_here', request$url\n  )\n  \n  # Iteratively apply str_replace_all and keep only final result\n  reduce2(\n    .x = replace_dict$template, \n    .y = replace_dict$replacement,\n    .f = str_replace_all,\n    .init =  read_lines(template_name) \n  ) %>% \n    paste0(collapse = '\\n') # Collaps lines into a single string\n}\n\nreplace_template_placeholder('template.md', request) %>% cat()\n\n#dataviz #twitter\n\n![[tweet_c_gebhard_2022-04-03_082301_1.png]]\n\n### Original Tweet\n\n(#)30DayChartChallenge (#)Day3 - Topic: historical\n\nBack to the Shakespeare data! Hamlet is is longest play, the comedies tend to be shorter.\n\nTool: (#)rstats\nData: kaggle users LiamLarsen, aodhan\nColor-Scale: MetBrewer\nFonts: Niconne, Noto Sans (+Mono)\nCode: https://t.co/iXAbniQDCb https://t.co/JCNrYH9uP4\n\nOriginal: https://twitter.com/c_gebhard/status/1510533315262042112\n\n### Original Mail\n\ninsert_mail_here\n\n\nAs you can see, my replace_template_placeholder() function also replaces the typical # from Twitter with (#). This is just a precaution to avoid wrong interpretation of these lines as headings in Markdown. Also, the original mail has not been inserted yet because we have no mail yet. But soooon. Finally, we need to write the replaced strings to a file. I got some helpers for that right here.\n\nwrite_replaced_text <- function(replaced_text, request) {\n  file_name <- request$file_name[1] %>% str_replace('_1.png', '.md')\n  write_lines(replaced_text, file_name)\n  paste0(getwd(), '/', file_name) \n}\nreplaced_template <- replace_template_placeholder('template.md', request) %>%\n  write_replaced_text(request)\n\n\n\nShuffle files around on your file system\nAwesome! We created new image files and a new Markdown note in our working directory. Now, we have to move them to our Obsidian vault. This is the place where I collect all my Markdown notes for use in Obsidian. In my case, I will need to move the Markdown note to the vault directory and the images to a subdirectory within this vault. This is because I changed settings in Obsidian that makes sure that all attachments, e.g. images, are saved in a separate subdirectory.\nHere’s the function I created to get that job done. The function uses the request list again because it contains the file paths of the images. Here, vault_location and attachments_dir are the file paths to my Obsidian vault.\n\nlibrary(tidyr) # for unnesting\nmove_files <- function(request, replaced_template, vault_location, attachments_dir) {\n  # Create from-to dictionary with file paths in each column\n  move_dict <- tribble(\n    ~from, ~to,\n    request$file_path, paste0(vault_location, '/', attachments_dir),\n    replaced_template, vault_location\n  ) %>% \n    unnest(cols = 'from')\n  # Copy files from current working directory to destination\n  move_dict %>% pwalk(file.copy, overwrite = T)\n  # Delete files in current working directory\n  walk(move_dict$from, file.remove)\n}"
  },
  {
    "objectID": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#how-to-extract-url-and-other-stuff-from-mail",
    "href": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#how-to-extract-url-and-other-stuff-from-mail",
    "title": "How to collect dataviz from Twitter into your note-taking system",
    "section": "How to extract URL and other stuff from mail",
    "text": "How to extract URL and other stuff from mail\nLet’s take a quick breather and recap. We have written functions that\n\ntake a tweet URL\nhussle the Twitter API to give us all its data\ndownload the images and tweet text\nsave everything to a new Markdown note based on a template\ncan move the note plus images to the location of our note-taking hub\n\nNot to brag but that is kind of cool. But let’s not rest here. We still have to get some work done. After all, we want our workflow to be email-based. So, let’s access our mails using R. Then, we can extract a Twitter URL and apply our previous functions. Also, this lets us finally replace the insert_mail_here placeholder in our Markdown note.\n\nPostman gives you access\nI have created a dummy mail account at gmail. Using the mRpostman package, we can establish a connection to our mail inbox. After the connection is established, we can filter for all new emails that are sent from our list of allowed_senders.\n\nlibrary(mRpostman) # for email communication\nimap_mail <- 'imaps://imap.gmail.com' # mail client\n# Establish connection to imap server\ncon <- configure_imap(\n  url = imap_mail,\n  user = user_mail,\n  password = password_mail\n)\n\n# Switch to Inbox\ncon$select_folder('Inbox') \n\n# Extract mails that are from the list of allowed senders\nmails <- allowed_senders %>% \n  map(~con$search_string(expr = ., where = 'FROM')) %>% \n  unlist() %>% \n  na.omit() %>% # Remove NAs if no mail from a sender\n  as.numeric() # avoids attributes\n\n\n\nGrab URLs from mail\nIf mails is not empty, i.e. if there are new mails, then we need to extract the tweet URLs from them. Unfortunately, depending on where you sent your email from, the mail text can be encoded.\nFor example, I send most of the tweets via the share button on Twitter using my Android smartphone. And for some reason, my Android mail client encodes the mails in something called base64. But sending a tweet URL from Thunderbird on my computer works without any encoding. Here are two example mails I have sent to my dummy mail account.\n\nif (!is_empty(mails)) mail_bodys <- mails %>% con$fetch_text()\ncat(mail_bodys[[1]])\ncat(mail_bodys[[2]])\n\nAs you can see, the mail sent from my computer is legible but the other one is gibberish. Thankfully, Allan Cameron helped me out on Stackoverflow to decode the mail. To decode the mail, the trick was to extract the parts between base64 and ----.\nThere are two such texts in the encoded mail. Surprisingly, the first one decoded to a text without line breaks. This is why we take the second encoded part and decode it. However, this will give us an HTML text with all kinds of tags like <div> and what not. Therefore, we use html_read() and html_text2() from the rvest package to handle that. All of this is summarized in this helper function.\n\ndecode_encoded_mails <- function(encoded_mails) {\n  # Ressource: https://stackoverflow.com/questions/71772972/translate-encoding-of-android-mail-in-r\n  # Find location in each encoded string where actual text starts\n  start_encoded <- encoded_mails %>% \n    str_locate_all('base64\\r\\n\\r\\n') %>% \n    map(~pluck(., 4) + 1) %>% \n    unlist()\n  \n  # Find location in each encoded string where actual text starts\n  end_encoded <- encoded_mails %>% \n    str_locate_all('----') %>% \n    map(~pluck(., 3) - 1)%>% \n    unlist()\n  \n  # Use str_sub() to extract encoded text\n  encoded_text <- tibble(\n    string = unlist(encoded_mails), \n    start = start_encoded, \n    end = end_encoded\n  ) %>% \n    pmap(str_sub) \n  \n  # Decode: base64 -> raw -> char -> html -> text\n  encoded_text %>% \n    map(base64enc::base64decode) %>% \n    map(rawToChar) %>% \n    map(rvest::read_html) %>% \n    map(rvest::html_text2)\n}\n\nI feel like this is the most hacky part of this blog post. Unfortunately, your milage may vary here. If your phone or whatever you use encodes the mails differently, then you may have to adjust the function. But I hope that I have explained enough details and concepts for you to manage that if it comes to this.\nRecall that I send both plain mails from Thunderbird and encoded mails from Android. Therefore, here is another helper that decoded mails if neccessary from both types in one swoop.\n\ndecode_all_mails <- function(mail_bodys) {\n  # Decode in case mail is base64 decoded\n  is_encoded <- str_detect(mail_bodys, 'Content-Transfer-Encoding')\n  encoded_mails <- mail_bodys[is_encoded]\n  plain_mails <- mail_bodys[!is_encoded]\n  decoded_mails <- encoded_mails %>% decode_encoded_mails()\n  c(decoded_mails, plain_mails)\n}\n\nThe remaining part of the code should be familiar:\n\nUse decode_all_mails() for decoding\nGrab URLs with str_extract()\nUse request_twitter_data() with our URLs\nReplace placeholders with replace_template_placeholder()\nThis time, replace mail placeholders too with another str_replace() iteration\nMove files with move_files()\n\nThe only new thing is that we use our postman connection to move the processed mails into a new directory (which I called “Processed”) on the email server. This way, the inbox is empty again or filled only with mails from unauthorized senders.\n\nif (!is_empty(mails)) {\n  # Grab mail texts and URLs\n  mail_bodys <- mails %>% con$fetch_text() %>% decode_all_mails\n  urls <- mail_bodys %>% str_extract('https.*')\n  \n  # Remove mails from vector in case s.th. goes wrong \n  # and urls cannot be detected\n  mail_bodys <- mail_bodys[!is.na(urls)]\n  mails <- mails[!is.na(urls)]\n  urls <- urls[!is.na(urls)]\n  \n  # For each url request twitter data\n  requests <- map(urls, request_twitter_data, bearer_token = bearer_token)\n  \n  # Use requested twitter data to insert texts into Markdown template \n  # and write to current working directory\n  replaced_templates_wo_mails <- \n    map(requests, replace_template_placeholder, template = 'template.md') \n  \n  # Now that we have mails, replace that placeholder too\n  replaced_templates <- replaced_templates_wo_mails %>% \n    map2(mail_bodys, ~str_replace(.x, 'insert_mail_here' ,.y)) %>% \n    map2(requests, ~write_replaced_text(.x, .y))\n  \n  # Move markdown files and extracted pngs to correct place on HDD\n  walk2(\n    requests, \n    replaced_templates, \n    move_files, \n    vault_location = vault_location, \n    attachments_dir = attachments_dir\n  )\n  \n  # Move emails on imap server to Processed directory\n  con$move_msg(mails, to_folder = 'Processed')\n}"
  },
  {
    "objectID": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#last-step-execute-r-script-automatically",
    "href": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#last-step-execute-r-script-automatically",
    "title": "How to collect dataviz from Twitter into your note-taking system",
    "section": "Last Step: Execute R script automatically",
    "text": "Last Step: Execute R script automatically\nAlright, alright, alright. We made it. We have successfully\n\nextracted URLs from mails,\ncreated new notes and\nmoved them to their designated place\n\nThe only thing that is left to do is execute this script automatically. Again, if you don’t want to assemble the R script yourself using the code chunks in this blog post, check out this GitHub gist.\nOn Windows, you can write a VBS script that will execute the R script. Window’s task scheduler is easily set up to run that VBS script regularly, say every hour. For completeness’ sake let me give you an example VBS script. But beware that I have no frikkin clue how VBS scripts work beyond this simple call.\nSet wshshell = WScript.CreateObject (\"wscript.shell\")\nwshshell.run \"\"\"C:\\Program Files\\R\\R-4.0.5\\bin\\Rscript.exe\"\" \"\"D:\\Local R Projects\\Playground\\TwitterTracking\\my_twitter_script.R\"\"\", 6, True\nset wshshell = nothing\nThe idea of this script is to call Rscript.exe and give it the location of the R script that we want to execute. Of course, you will need to adjust the paths to your file system. Notice that there are super many double quotes in this script. This is somewhat dumb but it’s the only way I could find to make file paths with white spaces work (see StackOverflow).\nOn Ubuntu (and probably other Unix-based systems), I am sure that every Unix user knows that there is CronTab to schedule regular tasks. On Mac, I am sure there is something. But instead of wandering even further from my expertise, I will refer to your internet search skills."
  },
  {
    "objectID": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#mind-the-possibilities",
    "href": "posts/09_get_twitter_posts_into_your_notetaking_system/09_get_twitter_posts_into_your_notetaking_system.html#mind-the-possibilities",
    "title": "How to collect dataviz from Twitter into your note-taking system",
    "section": "Mind the possibilities",
    "text": "Mind the possibilities\nWe made it! We connected to Twitter’s API and our dummy email to get data viz (what’s the plural here? viz, vizz, vizzes, vizzeses?) into our note-taking system. Honestly, I think that was quite an endeavor. But now we can use the same ideas for all kind of other applications! From the top of my head I can think of more scenarios where similar solutions should be manageable. Here are two ideas.\n\nTake notes on the fly using emails and automatically incorporate the emails into your note-taking system.\nTake a photo from a book/text you’re reading and send it to another dummy mail. Run a script that puts the photo and the mail directly into your vault.\n\nSo, enjoy the possibilities! If you liked this blog post, then consider following me on Twitter and/or subscribing to my RSS feed. Until next time!"
  },
  {
    "objectID": "posts/09_get_twitter_posts_into_your_notetaking_system/template.html",
    "href": "posts/09_get_twitter_posts_into_your_notetaking_system/template.html",
    "title": "Albert Rapp",
    "section": "",
    "text": "[[insert_img_name_here]]\n\nOriginal Tweet\ninsert_text_here\nOriginal: insert_URL_here\n\n\nOriginal Mail\ninsert_mail_here"
  },
  {
    "objectID": "posts/09_get_twitter_posts_into_your_notetaking_system/tweet_c_gebhard_2022-04-03_082301.html",
    "href": "posts/09_get_twitter_posts_into_your_notetaking_system/tweet_c_gebhard_2022-04-03_082301.html",
    "title": "Albert Rapp",
    "section": "",
    "text": "[[tweet_c_gebhard_2022-04-03_082301_1.png]]\n\nOriginal Tweet\n(#)30DayChartChallenge (#)Day3 - Topic: historical\nBack to the Shakespeare data! Hamlet is is longest play, the comedies tend to be shorter.\nTool: (#)rstats Data: kaggle users LiamLarsen, aodhan Color-Scale: MetBrewer Fonts: Niconne, Noto Sans (+Mono) Code: https://t.co/iXAbniQDCb https://t.co/JCNrYH9uP4\nOriginal: https://twitter.com/c_gebhard/status/1510533315262042112\n\n\nOriginal Mail\ninsert_mail_here"
  },
  {
    "objectID": "posts/07_janitor_showcase/07_janitor_showcase.html",
    "href": "posts/07_janitor_showcase/07_janitor_showcase.html",
    "title": "Showcasing the janitor package",
    "section": "",
    "text": "The janitor package contains only a little number of functions but nevertheless it is surprisingly convenient. I never really fully appreciated its functionality until I took a look into the documentation. Of course, other packages can achieve the same thing too but janitor makes a lot of tasks easy. Thus, here is a little showcase. If you prefer a video version, you can find this blog post on YouTube."
  },
  {
    "objectID": "posts/07_janitor_showcase/07_janitor_showcase.html#clean-column-names",
    "href": "posts/07_janitor_showcase/07_janitor_showcase.html#clean-column-names",
    "title": "Showcasing the janitor package",
    "section": "Clean column names",
    "text": "Clean column names\nAs everyone working with data knows, data sets rarely come in a clean format. Often, the necessary cleaning process already starts with the column names. Here, take this data set from TidyTuesday, week 41.\n\nnurses <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-05/nurses.csv')\nnames(nurses)\n\n [1] \"State\"                                          \n [2] \"Year\"                                           \n [3] \"Total Employed RN\"                              \n [4] \"Employed Standard Error (%)\"                    \n [5] \"Hourly Wage Avg\"                                \n [6] \"Hourly Wage Median\"                             \n [7] \"Annual Salary Avg\"                              \n [8] \"Annual Salary Median\"                           \n [9] \"Wage/Salary standard error (%)\"                 \n[10] \"Hourly 10th Percentile\"                         \n[11] \"Hourly 25th Percentile\"                         \n[12] \"Hourly 75th Percentile\"                         \n[13] \"Hourly 90th Percentile\"                         \n[14] \"Annual 10th Percentile\"                         \n[15] \"Annual 25th Percentile\"                         \n[16] \"Annual 75th Percentile\"                         \n[17] \"Annual 90th Percentile\"                         \n[18] \"Location Quotient\"                              \n[19] \"Total Employed (National)_Aggregate\"            \n[20] \"Total Employed (Healthcare, National)_Aggregate\"\n[21] \"Total Employed (Healthcare, State)_Aggregate\"   \n[22] \"Yearly Total Employed (State)_Aggregate\"        \n\n\nThese column names are intuitively easy to understand but not necessarily easy to process by code as there are white spaces and other special characters. Therefore, I accompany most data input by clean_names() from the janitor package.\n\nlibrary(janitor)\nlibrary(dplyr) # load for pipe %>%  and later wrangling\nnames(nurses %>% clean_names)\n\n [1] \"state\"                                       \n [2] \"year\"                                        \n [3] \"total_employed_rn\"                           \n [4] \"employed_standard_error_percent\"             \n [5] \"hourly_wage_avg\"                             \n [6] \"hourly_wage_median\"                          \n [7] \"annual_salary_avg\"                           \n [8] \"annual_salary_median\"                        \n [9] \"wage_salary_standard_error_percent\"          \n[10] \"hourly_10th_percentile\"                      \n[11] \"hourly_25th_percentile\"                      \n[12] \"hourly_75th_percentile\"                      \n[13] \"hourly_90th_percentile\"                      \n[14] \"annual_10th_percentile\"                      \n[15] \"annual_25th_percentile\"                      \n[16] \"annual_75th_percentile\"                      \n[17] \"annual_90th_percentile\"                      \n[18] \"location_quotient\"                           \n[19] \"total_employed_national_aggregate\"           \n[20] \"total_employed_healthcare_national_aggregate\"\n[21] \"total_employed_healthcare_state_aggregate\"   \n[22] \"yearly_total_employed_state_aggregate\"       \n\n\nDid you see what happened? White spaces were converted to _ and parantheses were removed. Even the % signs were converted to percent. Now, these labels are easy to understand AND process by code. This does not mean that you are finished cleaning but at least now the columns are more accessible."
  },
  {
    "objectID": "posts/07_janitor_showcase/07_janitor_showcase.html#remove-empty-and-or-constant-columns-and-rows",
    "href": "posts/07_janitor_showcase/07_janitor_showcase.html#remove-empty-and-or-constant-columns-and-rows",
    "title": "Showcasing the janitor package",
    "section": "Remove empty and or constant columns and rows",
    "text": "Remove empty and or constant columns and rows\nData sets come with empty or superfluous rows or columns are not a rare sighting. This is especially true if you work with Excel files because there will be a lot of empty cells. Take a look at the dirty Excel data set from janitor’s GitHub page. It looks like this when you open it with Excel.\n\n\n\n\n\nTaking a look just at this picture we may notice a couple of things.\n\nFirst, Jason Bourne is teaching at a school. I guess being a trained assassin qualifies him to teach physical education. Also - and this is just a hunch - undercover work likely earned him his “Theater” certification.\nSecond, the header above the actual table will be annoying, so we must skip the first line when we read the data set.\nThird, the column names are not ideal but we know how to deal with that by now.\nFourth, there are empty rows and columns we can get rid of.\nFifth, there is a column that contains only ‘YES’. Therefore it contains no information at all and can be removed.\n\nSo, let us read and clean the data. The janitor package will help us with remove_empty() and remove_constant().\n\nxl_file <- readxl::read_excel('dirty_data.xlsx', skip = 1) %>% \n  clean_names() %>%\n  remove_empty() %>% \n  remove_constant()\nxl_file\n\n# A tibble: 12 × 9\n   first_name   last_n…¹ emplo…² subject hire_…³ perce…⁴ full_…⁵ certi…⁶ certi…⁷\n   <chr>        <chr>    <chr>   <chr>     <dbl>   <dbl> <chr>   <chr>   <chr>  \n 1 Jason        Bourne   Teacher PE        39690    0.75 Yes     Physic… Theater\n 2 Jason        Bourne   Teacher Drafti…   43479    0.25 Yes     Physic… Theater\n 3 Alicia       Keys     Teacher Music     37118    1    Yes     Instr.… Vocal …\n 4 Ada          Lovelace Teacher <NA>      38572    1    Yes     PENDING Comput…\n 5 Desus        Nice     Admini… Dean      42791    1    Yes     PENDING <NA>   \n 6 Chien-Shiung Wu       Teacher Physics   11037    0.5  Yes     Scienc… Physics\n 7 Chien-Shiung Wu       Teacher Chemis…   11037    0.5  Yes     Scienc… Physics\n 8 James        Joyce    Teacher English   36423    0.5  No      <NA>    Englis…\n 9 Hedy         Lamarr   Teacher Science   27919    0.5  No      PENDING <NA>   \n10 Carlos       Boozer   Coach   Basket…   42221   NA    No      Physic… <NA>   \n11 Young        Boozer   Coach   <NA>      34700   NA    No      <NA>    Politi…\n12 Micheal      Larsen   Teacher English   40071    0.8  No      Vocal … English\n# … with abbreviated variable names ¹​last_name, ²​employee_status, ³​hire_date,\n#   ⁴​percent_allocated, ⁵​full_time, ⁶​certification_9, ⁷​certification_10\n\n\nHere, remove_empty() defaulted to remove, both, rows and colums. If we wish, we can change that by setting e.g. which = 'rows'.\nNow, we may also want to see the hire_data in a sensible format. For example, in this dirty data set, Jason Bourne was hired on 39690. Luckily, our janitor can make sense of it all.\n\nxl_file %>% \n  mutate(hire_date = excel_numeric_to_date(hire_date))\n\n# A tibble: 12 × 9\n   first_name last_…¹ emplo…² subject hire_date  perce…³ full_…⁴ certi…⁵ certi…⁶\n   <chr>      <chr>   <chr>   <chr>   <date>       <dbl> <chr>   <chr>   <chr>  \n 1 Jason      Bourne  Teacher PE      2008-08-30    0.75 Yes     Physic… Theater\n 2 Jason      Bourne  Teacher Drafti… 2019-01-14    0.25 Yes     Physic… Theater\n 3 Alicia     Keys    Teacher Music   2001-08-15    1    Yes     Instr.… Vocal …\n 4 Ada        Lovela… Teacher <NA>    2005-08-08    1    Yes     PENDING Comput…\n 5 Desus      Nice    Admini… Dean    2017-02-25    1    Yes     PENDING <NA>   \n 6 Chien-Shi… Wu      Teacher Physics 1930-03-20    0.5  Yes     Scienc… Physics\n 7 Chien-Shi… Wu      Teacher Chemis… 1930-03-20    0.5  Yes     Scienc… Physics\n 8 James      Joyce   Teacher English 1999-09-20    0.5  No      <NA>    Englis…\n 9 Hedy       Lamarr  Teacher Science 1976-06-08    0.5  No      PENDING <NA>   \n10 Carlos     Boozer  Coach   Basket… 2015-08-05   NA    No      Physic… <NA>   \n11 Young      Boozer  Coach   <NA>    1995-01-01   NA    No      <NA>    Politi…\n12 Micheal    Larsen  Teacher English 2009-09-15    0.8  No      Vocal … English\n# … with abbreviated variable names ¹​last_name, ²​employee_status,\n#   ³​percent_allocated, ⁴​full_time, ⁵​certification_9, ⁶​certification_10"
  },
  {
    "objectID": "posts/07_janitor_showcase/07_janitor_showcase.html#rounding",
    "href": "posts/07_janitor_showcase/07_janitor_showcase.html#rounding",
    "title": "Showcasing the janitor package",
    "section": "Rounding",
    "text": "Rounding\nTo my surprise shock, R uses some unexpected rounding rule. In my world, whenever a number ends in .5, standard rounding would round up. Apparently, R uses something called banker’s rounding that in these cases rounds towards the next even number. Take a look.\n\nround(seq(0.5, 4.5, 1))\n\n[1] 0 2 2 4 4\n\n\nI would expect that the rounded vector contains the integers from one to five. Thankfully, janitor offers a convenient rounding function.\n\nround_half_up(seq(0.5, 4.5, 1))\n\n[1] 1 2 3 4 5\n\n\nOk, so that gives us a new function for rounding towards integers. But what is really convenient is that janitor can round_to_fractions.\n\nround_to_fraction(seq(0.5, 2.0, 0.13), denominator = 4)\n\n [1] 0.50 0.75 0.75 1.00 1.00 1.25 1.25 1.50 1.50 1.75 1.75 2.00\n\n\nHere, I rounded the numbers to the next quarters (denominator = 4) but of course any fraction is possible. You can now live the dream of rounding towards arbitrary fractions."
  },
  {
    "objectID": "posts/07_janitor_showcase/07_janitor_showcase.html#find-matches-in-multiple-characteristics",
    "href": "posts/07_janitor_showcase/07_janitor_showcase.html#find-matches-in-multiple-characteristics",
    "title": "Showcasing the janitor package",
    "section": "Find matches in multiple characteristics",
    "text": "Find matches in multiple characteristics\nIn my opinion, the get_dupes() function is really powerful. It allows us to find “similar” observations in a data set based on certain characteristics. For example, the starwars data set from dplyr contains a lot of information on characters from the Star Wars movies. Possibly, we want to find out which characters are similar w.r.t. to certain traits.\n\nstarwars %>% \n  get_dupes(eye_color, hair_color, skin_color, sex, homeworld) %>% \n  select(1:8)\n\n# A tibble: 7 × 8\n  eye_color hair_color skin_color sex    homeworld dupe_count name        height\n  <chr>     <chr>      <chr>      <chr>  <chr>          <int> <chr>        <int>\n1 blue      black      yellow     female Mirial             2 Luminara U…    170\n2 blue      black      yellow     female Mirial             2 Barriss Of…    166\n3 blue      blond      fair       male   Tatooine           2 Luke Skywa…    172\n4 blue      blond      fair       male   Tatooine           2 Anakin Sky…    188\n5 brown     brown      light      female Naboo              3 Cordé          157\n6 brown     brown      light      female Naboo              3 Dormé          165\n7 brown     brown      light      female Naboo              3 Padmé Amid…    165\n\n\nSo, Luke and Anakin Skywalker are similar to one another. Who would have thought that. Sadly, I don’t enough about Star Wars to know whether the other matches are similarly “surprising”. In any case, the point here is that we can easily find matches according to arbitrarily many characteristics. Conveniently, these characteristics are the first columns of the new output and we get a dupe_count.\nAlright, this concludes our little showcase. In the janitor package, there is another set of tabyl() functions. These are meant to improve base R’s table() functions. Since I rarely use that function I did not include it but if you use table() frequently, then you should definitely check out tabyl()."
  },
  {
    "objectID": "posts/03_writing_versatile_fcts/03_writing_versatile_fcts.html",
    "href": "posts/03_writing_versatile_fcts/03_writing_versatile_fcts.html",
    "title": "Writing Versatile Functions with R",
    "section": "",
    "text": "This week, I had to deal with two very similar tasks on two very similar but not identical data sets that required me to write a function that is versatile enough to deal with both data sets despite their subtle differences. The differences that had to be accounted for mainly related to using functions in the two cases that relied on differently many arguments. Also, some of the column names were different which meant that I could not hard-code the column names into the function I was creating.\nConsequently, I had to use a few non-standard concepts (at least not standard to me) that enabled me to create the function which did everything I asked it to do. Since these concepts seemed interesting to me, I decided to implement a small example resulting in this blog post. Actually, I was even motivated to create a video for this blog post. You can find it on YouTube."
  },
  {
    "objectID": "posts/03_writing_versatile_fcts/03_writing_versatile_fcts.html#what-we-want-to-achieve",
    "href": "posts/03_writing_versatile_fcts/03_writing_versatile_fcts.html#what-we-want-to-achieve",
    "title": "Writing Versatile Functions with R",
    "section": "What We Want To Achieve",
    "text": "What We Want To Achieve\nThe aim of this example is to write a function that can create two tibbles that are conceptually similar but do not necessarily use the same column names or compute the existing columns in the same way. For this blog post, I have already set up two dummy data sets like that so that we can see what we want to do.\nLet’s take a look at these data sets I creatively called dat_A and dat_B.\n\nlibrary(tidyverse)\ndat_A %>% head(3)\n\n# A tibble: 3 × 3\n     mu sigma dat             \n  <dbl> <dbl> <list>          \n1    -1   1   <tibble [5 × 2]>\n2    -1   1.5 <tibble [5 × 2]>\n3    -1   2   <tibble [5 × 2]>\n\ndat_B %>% head(3)\n\n# A tibble: 3 × 2\n  lambda dat             \n   <dbl> <list>          \n1    0.5 <tibble [5 × 2]>\n2    0.7 <tibble [5 × 2]>\n3    0.9 <tibble [5 × 2]>\n\n\nAs you can see, each tibble contains a column dat. This column consists of tibbles with multiple summarized stochastic processes which were simulated using parameters that are given by the remaining columns of dat_A and dat_B.\nYou probably have already noticed that the stochastic processes must have been simulated using differently many parameters since tibble A contains additional columns mu and sigma whereas tibble B can offer only one additional column lambda. However, even if differently many and differently named parameters are used, the logic of the generating function needs to be the same:\n\nTake parameters.\nSimulate stochastic processes with these parameters.\nSummarize processes\n\nThus, in step 1 the generating function which we want to code, needs to be versatile enough to handle different argument names and amounts. Next, let’s see what the dat column has in store for us.\n\ndat_A %>% pluck(\"dat\", 1) %>% head(3)\n\n# A tibble: 3 × 2\n      n proc_mean\n  <int>     <dbl>\n1     1    -1.01 \n2     2    -0.958\n3     3    -0.968\n\ndat_B %>% pluck(\"dat\", 1) %>% head(3)\n\n# A tibble: 3 × 2\n      n proc_variance\n  <int>         <dbl>\n1     1          5.27\n2     2          2.66\n3     3          3.08\n\n\nFirst of all, notice that I accessed the first tibble in the dat column using the super neat pluck() function. In my opinion, this function is preferable to the clunky base R usage of $ and [[, e.g. like dat_A$dat[[1]].\nAs you can see, the tibbles that are saved in dat contain columns n and proc_mean resp. proc_variance. As hinted at before, each row is supposed to represent a summary of the n-th realization of a stochastic process.\nHowever, notice that the summary statistics in use are not the same! The different column names proc_mean and proc_variance indicate that in tibble A the sample mean was used whereas tibble B contains sample variances. Again, our function that generates tib_A and tib_B should be flexible enough to create differently named and differently computed columns."
  },
  {
    "objectID": "posts/03_writing_versatile_fcts/03_writing_versatile_fcts.html#helpful-concepts",
    "href": "posts/03_writing_versatile_fcts/03_writing_versatile_fcts.html#helpful-concepts",
    "title": "Writing Versatile Functions with R",
    "section": "Helpful Concepts",
    "text": "Helpful Concepts\nNow that we know what we want to create, let us begin by learning how to handle differently many arguments and their varying names.\n\ndot-dot-dot\nFor these kinds of purposes, R offers the ...-operator (pronounced dot-dot-dot). Basically, it serves as a placeholder for everything you do not want to evaluate immediately.\nFor instance, have you ever wondered how dplyr’s select() function is able to select the correct column?1 If you’re thinking “No, but what’s so special about this?”, then you may want to notice that it is actually not that simple to define your own select() function even with the help of the dplyr function.\nThis is because defining an appropriate function to select two columns from, say, the iris data set cannot be done like this:\n\nmy_select <- function(x, y) {select(iris, x, y)}\n\nNow, if you want to use the function the same way you would use dplyr::select(), i.e. simply passing, say, Sepal.Width, Sepal.Length (notice no \"\") to your new function, it would look like this\n\nmy_select(Sepal.Width, Sepal.Length)\n#> Error: object 'Sepal.Width' not found\n\nThis error appears because at some point, R will try to evaluate the arguments as variables from your current environment. But of course this variable is not present in your environment and only present within the iris data set. Therefore, what dplyr::select() accomplishes is that it lets R know to evaluate the input argument only later on, i.e. when the variable from the data set is “available”.\nThis is where ... comes into play. It is not by chance that select() only has arguments .data and .... Here, select() uses that everything which is thrown into ..., will be passed along to be evaluated later. This can save our my_select() function, too.\n\nmy_select <- function(...) {select(iris, ...)}\nmy_select(Sepal.Width, Sepal.Length) %>% head(3)\n\n  Sepal.Width Sepal.Length\n1         3.5          5.1\n2         3.0          4.9\n3         3.2          4.7\n\n\nWorks like a charm! This will help us to define a function that is flexible enough for our purposes. Before we start with that, let us learn about another ingredient we will use.\n\n\ncurly-curly\nIf we were to only select a single column from iris using our my_select() function, we could have also written the function using {{ }} (pronounced curly-curly). It operators similar to ... in the sense that it allows for later evaluation but applies this concept to specific variable. Check out how that can be used here.\n\nmy_select <- function(x) {select(iris, {{x}})}\nmy_select(Sepal.Width) %>% head(3)\n\n  Sepal.Width\n1         3.5\n2         3.0\n3         3.2\n\n\nWhat’s more the curly-curly variables - curly-curlied variables (?) - can also be used later on for stuff like naming a new column. For example, let us modify our previous function to demonstrate how that can be used.\n\nselect_and_add <- function(x, y) {\n  select(iris, {{x}}) %>% \n    mutate({{y}} := 5) \n  # 5 can be replaced by some meaningful calculation\n}\nselect_and_add(\"Sepal.Width\", \"variable_y\") %>% head(3)\n\n  Sepal.Width variable_y\n1         3.5          5\n2         3.0          5\n3         3.2          5\n\n\nMind the colon! Here, if you want to use y as column name later on you cannot use the standard mutate() syntax but have to use := instead.\n\n\nFunctional Programming\nOne last thing that we will use, is the fact that R supports functional programming. Thus, we can use functions as arguments of other functions. For instance, take a look at this super simple, yet somewhat useless wrapper function for illustration purposes.\n\nmy_simulate <- function(n, func) {\n  func(n)\n}\nset.seed(564)\nmy_simulate(5, rnorm)\n\n[1]  0.4605501 -0.7750968 -0.7159321  0.6882645 -2.0544591\n\n\nAs you just witnessed, I simply passed rnorm (without a call using ()) to my_simulate as the func argument such that rnorm is used whenever func is called. In our use case, this functionality can be used to simulate different stochastic processes (that may depend on different parameters)."
  },
  {
    "objectID": "posts/03_writing_versatile_fcts/03_writing_versatile_fcts.html#the-implementation",
    "href": "posts/03_writing_versatile_fcts/03_writing_versatile_fcts.html#the-implementation",
    "title": "Writing Versatile Functions with R",
    "section": "The Implementation",
    "text": "The Implementation\nAlright, we have assembled everything we need in order to create our simulate_and_summarize_proc() function. In this example, the simulation of the stochastic processes will consist of simply calling rnorm() or rexp() but, of course, these functions can be substituted with arbitrarily complex simulation functions.\nWe will use n_simus as the amount of realizations that are supposed to be simulated and each realization will be of length TMax. Further, we will use ... to handle an arbitrary amount of parameters that are supposed to be passed to simulation_func. So, let’s implement the simulation part first (detailed explanations below).\n\nsimulate_and_summarize_proc <- \n  function(..., TMax, n_simus, simulation_func) {\n    argslist <- list(n = TMax, ...) %>% \n      map(~rep(., n_simus))\n    \n    tibble(\n      t = list(1:TMax),\n      n = 1:n_simus,\n      value = pmap(argslist, simulation_func)\n    ) \n  }\nset.seed(457)\nsimulate_and_summarize_proc(\n  mean = 1,\n  sd = 2,\n  TMax = 200, \n  n_simus = 3, \n  simulation_func = rnorm # arguments -> n, mean, sd\n) \n\n# A tibble: 3 × 3\n  t               n value      \n  <list>      <int> <list>     \n1 <int [200]>     1 <dbl [200]>\n2 <int [200]>     2 <dbl [200]>\n3 <int [200]>     3 <dbl [200]>\n\n\nAs you can see, this created three (simple) stochastic processes of length 200 using the parameters mean = 1 and sd = 2. We can validate that the correct parameters were used once we implement the summary functions.\nFirst, let us address the tricky part in this function. In order to pass a list of arguments to pmap() that are then used with simulation_func, we first need to rearrange the lists a bit. After the first step, by simply putting everything from ... into the list we have a list like this:\n\nlist(n = 100, mean = 1, sd = 2) %>% str()\n\nList of 3\n $ n   : num 100\n $ mean: num 1\n $ sd  : num 2\n\n\nHowever, we will need to have each variable in the list repeated n_simus time in order to simulate more than one realization. Thus, we use map() to replicate:\n\nlist(n = 200, mean = 1, sd = 2) %>% \n  map(~rep(., 3)) %>% \n  str()\n\nList of 3\n $ n   : num [1:3] 200 200 200\n $ mean: num [1:3] 1 1 1\n $ sd  : num [1:3] 2 2 2\n\n\nNote that calling rep() without map() does not cause an error but does not deliver the appropriate format:\n\nlist(n = 100, mean = 1, sd = 2) %>% \n  rep(3) %>% \n  str()\n\nList of 9\n $ n   : num 100\n $ mean: num 1\n $ sd  : num 2\n $ n   : num 100\n $ mean: num 1\n $ sd  : num 2\n $ n   : num 100\n $ mean: num 1\n $ sd  : num 2\n\n\nNext, let us take the current output and implement the summary. To do so, we will add another variables summary_name and summary_func to the function in order to choose a column name resp. a summary statistic.\n\nsimulate_and_summarize_proc <- \n  function(..., TMax, n_simus, simulation_func, summary_name, summary_func) {\n    argslist <- list(n = TMax, ...) %>% \n      map(~rep(., n_simus))\n    \n    tibble(\n      t = list(1:TMax),\n      n = 1:n_simus,\n      value = pmap(argslist, simulation_func)\n    ) %>% # this part is added\n      unnest(c(t, value)) %>% \n      group_by(n) %>%\n      summarise({{summary_name}} := summary_func(value))\n  }\nset.seed(457)\nsimulate_and_summarize_proc(\n  mean = 1,\n  sd = 2,\n  TMax = 200, \n  n_simus = 5, \n  simulation_func = rnorm, \n  summary_name = \"mega_awesome_mean\", \n  summary_func = mean\n) \n\n# A tibble: 5 × 2\n      n mega_awesome_mean\n  <int>             <dbl>\n1     1             0.955\n2     2             0.932\n3     3             0.987\n4     4             1.07 \n5     5             1.15 \n\n\nFinally, we can use our super versatile function in combination with map() to create dat_A and dat_B.\n\ndat_A <- \n  expand_grid(\n    mu = seq(-1, 1, 0.25),\n    sigma = seq(1, 3, 0.5)\n  ) %>% \n  mutate(dat = map2(\n    mu, sigma, \n    ~simulate_and_summarize_proc(\n      mean = .x, \n      sd = .y, \n      TMax = 200, \n      n_simus = 3, \n      simulation_func = rnorm, \n      summary_name = \"proc_mean\", \n      summary_func = mean\n    )\n  ))\n  \n\ndat_B <- \n  expand_grid(\n    lambda = seq(0.5, 1.5, 0.2)\n  ) %>% \n  mutate(dat = map(\n    lambda, \n    ~simulate_and_summarize_proc(\n      rate = .,\n      TMax = 200, \n      n_simus = 3, \n      simulation_func = rexp, \n      summary_name = \"proc_variance\", \n      summary_func = var\n    )\n  ))"
  },
  {
    "objectID": "posts/03_writing_versatile_fcts/03_writing_versatile_fcts.html#conclusion",
    "href": "posts/03_writing_versatile_fcts/03_writing_versatile_fcts.html#conclusion",
    "title": "Writing Versatile Functions with R",
    "section": "Conclusion",
    "text": "Conclusion\nSo, we have seen that we can combine {{ }}, ... and functional programming to create highly versatile functions. Of course, as always one might be tempted to say that one could have just programmed two different functions for our particular example.\nHowever, this would cause a lot of code duplication because a lot of steps are essentially the same which is hard to debug and maintain. Also, creating numerous functions does not scale well if we need to cover way more than two cases.\nWith that being said, I hope that you found this blog post helpful and if so, feel free to hit the comments or push the applause button below. See you next time."
  },
  {
    "objectID": "posts/02_luck_vs_skill/02_luck_vs_skill.html",
    "href": "posts/02_luck_vs_skill/02_luck_vs_skill.html",
    "title": "Is Success Luck or Hard Work?",
    "section": "",
    "text": "Recently, I decided to try out a few stretches in an effort to stay in shape during long stretches of working from home and not leaving the house. Also, to distract me from my inflexible body I thought I would watch a video on YouTube simultaneously and as luck would have it, I saw an interesting video on Veritasium’s YouTube Channel called “Is Success Luck or Hard Work”. Personally, I agree with a lot of things being said in that video and I recommend that you check out the video if you want to get a perspective on the role of luck compared to skill (or keep on reading for my own take on this topic).\nBut more importantly, in the video Derek Muller - the guy behind Veritasium - describes a simulation he ran in order to hint at whether luck played a role in the selection of 11 out of 18300 applicants in 2017 for the NASA astronaut training program. The underlying model that is simulated in the video is described as assuming that astronauts are selected mostly based on their skill. However, 5% of the decision is also based on luck.\nThis sounds a little bit vague, so Muller elaborates: First, each applicant is assigned a random skill score (out of 100) and a luck score (out of 100). Then, the weighted sum of the two scores result in an applicant’s overall score (the weights being of course 95% for skill and 5% for luck). Finally, the top 11 applicants according to this overall score will then go on to become astronauts.\nNow, based on a thousand runs of this simulation what Veritasium finds is that it was the very lucky applicants who were selected. More precisely, the average luck score of the picked applicants was 94.7. Similarly, on average out of the 11 picked astronauts only 1.6 applicants would have been the same had the selection process been based on skill alone.\nSo, as I was watching this video, I noticed two things. One, I am really inflexible and I need to stretch more and two, this simulation sounds pretty cool and I bet I could recreate this simulation quite easily. Thus, an idea for a new blog post was born."
  },
  {
    "objectID": "posts/02_luck_vs_skill/02_luck_vs_skill.html#the-original-approach",
    "href": "posts/02_luck_vs_skill/02_luck_vs_skill.html#the-original-approach",
    "title": "Is Success Luck or Hard Work?",
    "section": "The Original Approach",
    "text": "The Original Approach\nLater on, I want to tweak the above approach a bit but for now let us simulate the process as described above. First, we will need a function to generate the applicants’ scores. Here, I want to generate the luck score according to a uniform distribution on \\((0, 100)\\) because I assume that we are all lucky in a similar way regardless of what the position we apply for is.\nHowever, I think it is conceivable that for highly specialized jobs (such as astronauts) only really skilled applicants show up whereas jobs that fall more in the “jack of all trades” category may attract applicants from all kinds of skill ranges. This will, of course, affect the distribution of skill scores and I wonder if this has a significant effect on the overall results. Therefore, I will make sure that the score generating function has the ability to use different skill distributions. Finally, let us add an option to change the skill-luck ration which we will first set to its default value of 5%.1\n\nlibrary(tidyverse)\nsimulate_applicants <- function(n, dist, luck_ratio = 0.05) {\n  tibble(\n    skill = dist(n),\n    luck = runif(n, min = 0, max = 100),\n    overall = (1 - luck_ratio) * skill + luck_ratio * luck\n  )\n}\nset.seed(123)\nsimulate_applicants(5, function(x) rnorm(x, 50, 1))\n\n# A tibble: 5 × 3\n  skill  luck overall\n  <dbl> <dbl>   <dbl>\n1  49.4  95.7    51.8\n2  49.8  45.3    49.5\n3  51.6  67.8    52.4\n4  50.1  57.3    50.4\n5  50.1  10.3    48.1\n\n\nObviously, each column represents the respective score for each applicant. Regarding the skill scores I propose a couple of different distributions:\n\nUniform distribution: We assume that applicants come from all kinds of skill ranges and all skill levels are equally likely.\nNormal distribution: We assume that most people fall within a medium skill range which we model by a normal distribution with mean 50 and standard deviation \\(50/4\\) so that for our 18000 astronauts chances are very slim that one of them falls outside the range (due to the empirical rule of the normal distribution).\nHigh specialization: To cover the scenario of only highly skilled applicants, let us use a beta distribution \\(X \\sim \\text{Beta}(1.2, 10)\\) and use the transformed variable \\(100(1-X)\\) as skill distribution.2\nLow Specialization: Similarly, let us use \\(100X\\) where \\(X \\sim \\text{Beta}(1.2, 10)\\) to simulate a scenario in which mostly applicants with a low skill score occur.\n\nThe corresponding functions that realize the distributions for us are given as follows:\n\nnormal_skills <- function(n) {\n  rnorm(n, mean = 50, sd = 50 / 4) %>% \n    # Make sure that score stays in bounds\n    pmax(0) %>% \n    pmin(100)\n}\n\nuniform_skills <- function(n) {\n  runif(n, min = 0, max = 100)\n}\n\nhigh_skills <- function(n) {\n  100 * (1 - rbeta(n, 1.2, 10))\n}\n\nlow_skills <- function(n) {\n  100 * rbeta(n, 1.2, 10)\n}\n\nTo illustrate the high and low skill distribution, let us take a look at the densities of the beta distributions in question.\n\n\n\n\n\nNow, let us write a function that runs a single iteration of the selection process and marks applicants as either selected or not. We will denote the number of applicants to be picked via m. In our astronaut example it holds that \\(m = 11\\).\n\npick_applicants <- function(n, dist, m = 11) {\n  applicants <- simulate_applicants(n, dist)\n  \n  applicants %>% \n    arrange(desc(overall)) %>% \n    mutate(selected = c(rep('yes', m), rep('no', n - m)))\n}\n\nn <- 18300\npick_applicants(n, normal_skills)\n\n# A tibble: 18,300 × 4\n   skill  luck overall selected\n   <dbl> <dbl>   <dbl> <chr>   \n 1  98.1 11.3     93.8 yes     \n 2  97.4  5.56    92.8 yes     \n 3  93.1 72.9     92.1 yes     \n 4  93.2 60.5     91.6 yes     \n 5  90.5 94.4     90.7 yes     \n 6  94.3 15.4     90.4 yes     \n 7  89.1 88.6     89.1 yes     \n 8  89.4 76.1     88.7 yes     \n 9  87.7 83.8     87.5 yes     \n10  88.0 75.2     87.3 yes     \n# … with 18,290 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nUsing this function, we can easily simulate the whole application process a couple of times Monte Carlo style.\n\nset.seed(123)\nN <- 1000\nsimus <- expand_grid(\n  dist = c(\"uniform_skills\", \"normal_skills\", \"low_skills\", \"high_skills\"),\n  simuID = 1:N\n) %>%\n  mutate(applicants = map(dist, ~pick_applicants(n, get(.)))) %>% \n  unnest(applicants) %>% \n  filter(selected == 'yes')\n\nHaving created a tibble simus that contains information on the skill and luck scores of success candidates for each simulation run, we can visualize the distribution of the luck scores of the successful candidates using boxplots. The idea behind that is that if luck is important, then the boxplots should differ from that of a standard uniform distribution on \\((0, 100)\\).\n\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(gtable)\navgs <- simus %>% \n  group_by(dist) %>% \n  summarize(avg_luck = mean(luck)) %>% \n  arrange(desc(avg_luck)) %>% \n  mutate(\n    avg_luck = scales::comma(avg_luck, accuracy = .01),\n    dist = case_when(\n      dist == \"high_skills\" ~ \"High skills\",\n      dist == \"low_skills\" ~ \"Low skills\",\n      dist == \"uniform_skills\" ~ \"Uniform skills\",\n      dist == \"normal_skills\" ~ \"Normal skills\"\n    )\n  )\n\n\ncolorScale <- glue::glue(\"dodgerblue{1:4}\")\navgTable <- tableGrob(\n  avgs, \n  rows = NULL, \n  cols = NULL,\n  theme = ttheme_minimal(\n     core=list(fg_params = list(col = colorScale, fontface = 2))\n  )\n) \n\nsimus %>%  \n  mutate(dist = fct_reorder(dist, luck)) %>% \n  ggplot(aes(y = dist, x = luck, col = dist)) +\n  geom_boxplot(show.legend = F) +\n  theme_classic() +\n  labs(\n    x = \"Luck\", \n    y = element_blank(),\n    title = \"Luck Distribution Among Successful Applicants\",\n    subtitle = \"(Average luck score depicted in the table)\"\n  ) +\n  annotation_custom(avgTable, xmin = 20,  xmax = 60, ymin = 2, ymax = 5) +\n  scale_y_discrete(breaks = NULL) +\n  scale_color_manual(values = rev(colorScale)) + \n  theme(axis.line.y = element_blank(), \n        plot.subtitle = element_text(size = 10)) \n\n\n\n\nHere, we added a table of the average luck score of the successful applicants to the boxplots that summarize the distribution of their luck scores. As it turns out, we can clearly see differences between the luck distribution of the successful candidates across the different skill distributions. Interestingly, the uniform skill distribution comes quite close to the average value Veritasium finds in his video, so I guess we can assume that he probably used that skill distribution.\nI would say that the key takeaway of this picture is that the more specialized your area of expertise is, the luckier you have to be if you have many similarly skilled competitors. In a way, this makes sense. If you and your competition is basically at the top of the game and there is not much room to differentiate candidates w.r.t. skill, then luck may just be the deciding factor.\nInterestingly, the same holds true when skills are uniformly distributed among the whole range. Further, when the skill distribution of you and your competition looks more normally distributed or if the skill scores of all applicants are rather low, then you do not need to be extremely lucky (at least not as much as before). But still, you have to be more lucky than the average applicant (recall that luck is modeled via a uniform distribution here with mean 50).\nSo, in a sense this might mean that if chances are good that there are a couple of applicants who are as good as it gets, i.e. at the maximum of the skill range (which is the case for the high and uniform skills), then the successful candidates are indeed really lucky. Similarly, if chances are low that some applicants are the best of the best (normal and low skills), then successful applicants are luckier than the average Joe but in a less extreme way than in the previous example. In total, it looks like luck plays a role in either case.\nFinally, to break this scenario down to something more realistic with a higher available spots to applicants ratio I ran the same simulation but with only one available position and 100 applications.3 The results look similar but as expected if their are less applicants for one position, then luck plays a lesser but still important role."
  },
  {
    "objectID": "posts/02_luck_vs_skill/02_luck_vs_skill.html#overcoming-threshold-approach",
    "href": "posts/02_luck_vs_skill/02_luck_vs_skill.html#overcoming-threshold-approach",
    "title": "Is Success Luck or Hard Work?",
    "section": "Overcoming Threshold Approach",
    "text": "Overcoming Threshold Approach\nThere is this old joke where a recruiter in an HR department gets a large stack of applications for a specific position within the company. The recruiter immediately begins to work on the applications by simply taking half of the applications and throwing them into the trash because he “does not hire unlucky people”.\nThis may be an extreme action but then again I have heard that some companies immediately reject an application if they see only a single typo in the documents. To me, this is similar to what the recruiter from the joke is doing because a typo can happen by accident to even the most careful person regardless of their skill. So, let us use this as a way to construct another hypothetical scenario.\nIn this scenario, an application goes through two stages. In the first stage, the application is either is moved to the second stage or is rejected due to some arbitrary small reason. By this we mean some small event based on luck which we will model here with our luck score from above. In the second stage, everything is judged purely on skill alone, i.e. the person with the highest skill score gets the job.\nLet us write a new function for this second hypothetical scenario. Notice that this new function ranks applicants according to their skill score, i.e. the most skilled applicant is ranked as 1, the second most skilled applicant is ranked as 2 and so on.\n\npick_applicants2 <- function(n, dist, m = 11, luck_thresh = 50) {\n  applicants <- simulate_applicants(n, dist)\n  \n  applicants <- applicants %>%\n    mutate(skill_rank = min_rank(desc(skill))) %>% \n    filter(luck > luck_thresh)\n  \n  applicants %>% \n    arrange(desc(skill)) %>% \n    mutate(selected = c(rep('yes', m), rep('no', nrow(applicants) - m)))\n}\n\npick_applicants2(n, high_skills)\n\n# A tibble: 49 × 5\n   skill  luck overall skill_rank selected\n   <dbl> <dbl>   <dbl>      <int> <chr>   \n 1  99.9  87.4    99.3          1 yes     \n 2  99.1  54.6    96.9          3 yes     \n 3  98.8  54.5    96.6          5 yes     \n 4  98.8  97.5    98.7          6 yes     \n 5  98.2  90.6    97.8         10 yes     \n 6  97.6  60.8    95.8         12 yes     \n 7  97.0  87.0    96.5         13 yes     \n 8  96.8  71.9    95.5         14 yes     \n 9  96.7  97.0    96.8         16 yes     \n10  96.6  55.1    94.5         18 yes     \n# … with 39 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAfterwards, we can run a similar simulation as before. However, this time we will take a look at the distribution of the ranks of the successful candidates. Here are the results for our astronaut scenario, i.e. 18300 applicants with 11 open positions.\n\n\n\n\n\n\n\n\nAs you can see, the distribution of the skill ranks of the successful applicants do not vary much across the skill distributions. Also, it looks like the average ranks are simply the amount of positions plus 1. This is somewhat unsurprising since we throw out 50% of the applicants randomly in the first stage of the process.\nNevertheless, this reinforces the idea that not necessarily the most skilled person gets hired to do the job. What is even more surprising to me is the fact that in the current scenario there is still a chance of around 8% that a successful applicant has a skill rank above 22 (recall that there are 11 open positions). Now, running the same analysis for 100 applicants for 1 job again yields similar results.\n\n\n\n\n\nIn these 1000 simulations, the chances that a successful applicant has a skill score of 5 or above are similarly high but depend on the skill distribution.\n\n\n# A tibble: 4 × 3\n# Groups:   dist [4]\n  dist               n prop \n  <chr>          <int> <chr>\n1 Normal skills     82 8.2% \n2 Uniform skills    69 6.9% \n3 High skills       65 6.5% \n4 Low skills        58 5.8%"
  },
  {
    "objectID": "posts/02_luck_vs_skill/02_luck_vs_skill.html#summary",
    "href": "posts/02_luck_vs_skill/02_luck_vs_skill.html#summary",
    "title": "Is Success Luck or Hard Work?",
    "section": "Summary",
    "text": "Summary\nSo, in our simulations we have seen that luck plays a role in whether an applicant gets a job. Further, this actually depended on the skill distribution of the applicants and the impact of luck was most strongly pronounced in the cases when there was a good chance that highly skilled people are present among the applicants. Probably, this is the most relevant scenario anyway since we all like to believe that we are skilled but are still somewhat more proficient than other skilled people.\nOf course, in terms of real world implications, this simulation can only give anecdotal evidence towards the hypothesis that luck plays a large role in success. Also, we made a lot of assumptions in our simulations that might be debatable. But, personally, I like to believe that luck cannot be neglected in the end and even the most skilled person may want to be grateful when he gets accepted for a job. If you wish to share your opinion on that, feel free to leave a comment. Finally, if you thought this post was interesting, I would also appreciate it if you clicked the applause button (just so that it feels less like I am talking into a void)."
  },
  {
    "objectID": "posts/04_sonntagsfrage/04_sonntagsfrage.html",
    "href": "posts/04_sonntagsfrage/04_sonntagsfrage.html",
    "title": "Did German Voters Become More Impulsive?",
    "section": "",
    "text": "A bit more than two weeks ago, Germany held a federal election and, naturally, this is always reason for a lot of discussions and subjective truths. One subjective truth I encountered myself related to how fast the party CDU/CSU was able to collect and lose votes according to polls right before the election.\nAccording to the Allensbach Institute, a private polling institute based in Allensbach, Baden-Württemberg, on July 20th, approximately three months before the official election, the CDU/CSU could get 31.5% of the votes1. Almost four weeks later on the 19th of August, the Allensbach institute forecast only 27.5% for the CDU/CSU.\nAt that time, I had the subjective feeling that it was quite common to assume that the CDU/CSU is on a steep downward spiral. In the end, the CDU/CSU was able to slow its downward course and got 24.1% of the votes in the election. While this is still an abysmal outcome for this party, I was surprised that it was not worse.\nA “similar” surprising tale can be told for other parties too. For instance, the party SPD was gaining a lot of steam in the last three months of the election campaign and the party DIE GRÜNE, once surprisingly popular during the election, lost a lot of votes towards the end as well.\nAll of these ups and downs left a feeling of rapid change for some. For example, last weekend I had an interesting discussion about whether voters no longer cast their votes according to “belief” but are much more influenced by the spur of the moment and flip-flop back and forth between parties depending on who is making the headlines at that time. Consequently, I decided that this might be something worth looking at with data.\nThus, this blog post tries to look at historic data from election polls to see if this year’s change before the election is indeed something unprecedented. If this is so, then that might support that voters become more impulsive. So, this is why I scraped election polls2 since 1998 from the Allensbach institute and the Kantar (Emnid) institute whose election polls can be found publicly here3."
  },
  {
    "objectID": "posts/04_sonntagsfrage/04_sonntagsfrage.html#data-warning",
    "href": "posts/04_sonntagsfrage/04_sonntagsfrage.html#data-warning",
    "title": "Did German Voters Become More Impulsive?",
    "section": "Data Warning",
    "text": "Data Warning\nI believe it is worth pointing out that the polling results will have to be taken with a grain of salt. Especially the fact that people might judge their current preference differently in non-election years compared to election years has to be taken into account. Obviously, I suspect that the polling institutes considered this as part of their forecast but nevertheless it cannot hurt to mention potential caveats."
  },
  {
    "objectID": "posts/04_sonntagsfrage/04_sonntagsfrage.html#popularity-over-time",
    "href": "posts/04_sonntagsfrage/04_sonntagsfrage.html#popularity-over-time",
    "title": "Did German Voters Become More Impulsive?",
    "section": "Popularity Over Time",
    "text": "Popularity Over Time\nThis being said, let’s take a look at the six (currently) largest parties and their popularity over time. Here, I will only look at the data from the Allensbach institute as these are already quite a lot of data points and the picture might get messy otherwise. As this is an election-focused blog post, I took the liberty of labeling only the election years on the x-axis4.\n\n\n\n\n\n\n\n\n\n\n\nSome recent trends are detectable but in this particular figure, I don’t see anything that points to an increased volatility in recent times."
  },
  {
    "objectID": "posts/04_sonntagsfrage/04_sonntagsfrage.html#what-happens-close-to-an-election",
    "href": "posts/04_sonntagsfrage/04_sonntagsfrage.html#what-happens-close-to-an-election",
    "title": "Did German Voters Become More Impulsive?",
    "section": "What Happens Close to an Election?",
    "text": "What Happens Close to an Election?\nInstead of looking at the overall fluctuations, we could look at the last three months before an election. Since an election takes places in September, in the next plot I have depicted only the polling results in the months July, August and September in an election year. To make trends more visible, I have added a regression line for each party and each election.\n\n\n\n\n\n\n\n\nInterestingly, the most recent election seems to have had more volatile last three months compared to previous elections. Indeed, this could indicate more impulsive voteing behavior but I am not entirely convinced yet."
  },
  {
    "objectID": "posts/04_sonntagsfrage/04_sonntagsfrage.html#three-month-volatility",
    "href": "posts/04_sonntagsfrage/04_sonntagsfrage.html#three-month-volatility",
    "title": "Did German Voters Become More Impulsive?",
    "section": "Three-Month Volatility",
    "text": "Three-Month Volatility\nTo see if the change in the last three months of the most recent election is truly something out of the ordinary, we need context. In order to get this context, let us consult our historical data again and compute the average share of votes for each party in every quarter of every year. Then, hopefully, these computed mean percentages represent the mood of the majority of people in a given quarter and we can see how much these means change from quarter to quarter.\n\n\n\n\n\nTaking the fluctuations over time into account, the quarterly change right before this year’s election looks less extreme. In fact, most of the parties have had more extreme or similar changes on a quarterly basis in the past.\nClearly, the fact that we aggregate the share of votes over a period of three months could potentially obscure fluctuations. But as the next plot shows, if we do the same thing but aggregate on a monthly basis, then the overall impression of the new monthly plot is the same as with the original quarterly plot."
  },
  {
    "objectID": "posts/04_sonntagsfrage/04_sonntagsfrage.html#coefficient-of-variation",
    "href": "posts/04_sonntagsfrage/04_sonntagsfrage.html#coefficient-of-variation",
    "title": "Did German Voters Become More Impulsive?",
    "section": "Coefficient of Variation",
    "text": "Coefficient of Variation\nBefore we try to make sense of what all that we have seen could mean in terms of voters’ impulsiveness, let us take one more stab at trying to measure the volatility. This time, let us compute the coefficient of variation (CV)5 of the mean weekly share of votes for each quarter and each party and display this over time.\n\n\n\n\n\nExcept for the AfD and SPD, no profound trend in the CV can be detected as most regression lines appear to have a slope that is close to zero. Further, the AfD’s decrease in its monthly CV might be explained by the fact that it is a comparatively new party which means that might not have had a solid voter base in the beginning.\nSince we are interested in more recent voter behavior, let us try to take only the data since the beginning of 2013 into account. This should give us an impression about the three most recent elections.\n\n\n\n\n\nOverall, as the confidence bands of the regression lines indicate that the real slope of the lines might as well be close to zero, I find it hard to argue either way about a more impulsive voting behavior."
  },
  {
    "objectID": "posts/04_sonntagsfrage/04_sonntagsfrage.html#conclusion",
    "href": "posts/04_sonntagsfrage/04_sonntagsfrage.html#conclusion",
    "title": "Did German Voters Become More Impulsive?",
    "section": "Conclusion",
    "text": "Conclusion\nSo, we have observed something odd here. In previous elections, a party’s changes in popularity in the last three months were not as profound as the last quarter’s changes that we witnessed this year. Overall, however, the degree of how much changed right before the election is nothing that could not be witnessed in the past at other non-election times.\nIf I had to guess, I would say that this might indicate that over the last 20 years, people’s willingness to vote for a different party remained somewhat similar. But it appears like the moment, when voters eventually decide for a party, has been moved closer to the election itself. Thus, one might argue that it has become harder to pinpoint which party one feels most connected to and this leads to an indecision right until the end. This indecision could indeed lead to a flipping back and forth between two or three parties one feels similarly connected to. But this, I would argue, is a sign of people lacking a strong connection to one single party and not one of impulsiveness.\nWhat do you think? Is there more that we can extract from this kind of data? As I am quite new to this type of analysis, I am always glad about suggestions about how to improve. If you want to share your ideas, feel free to send me an e-mail or leave a message in the comment section. As always, if you liked this blog post, I would appreciate a hit on the applause button below."
  },
  {
    "objectID": "posts/01_visualize_kernel_density/01_visualize_kernel_density.html",
    "href": "posts/01_visualize_kernel_density/01_visualize_kernel_density.html",
    "title": "Animating kernel density estimators",
    "section": "",
    "text": "For my first post on this blog I decided to create an animation using the animation package. To give this animation some purpose let me demonstrate how kernel density estimators work with the help of an animation.\nIn general, kernel density estimators are, as is kind of obvious by the name, used to estimate the underlying density of a random sample. For instance, imagine that we have a sample drawn from an some unknown distribution.\nThen, assuming that we do not actually know that the current sample was drawn from an exponential distribution, we might want to estimate the density and see if the estimate fits to some well-known parametric family of distributions. With the help of ggplot() and geom_density() this is straightforward.\nThe underlying procedure to generate the plot it to use a kernel density estimator \\(\\hat{f}_h\\) in order to estimate the true underlying density \\(f\\) by using the formula\n\\[\n\\hat{f}_h(x) = \\frac{1}{nh} \\sum_{k = 1}^{n} K\\Big(\\frac{x - x_k}{h}\\Big)\n\\]\nfor all \\(x \\in \\mathbb{R}\\) where \\(n\\) is the sample lengh and \\(h > 0\\) is a smoothing parameter that needs to be chosen and \\(K\\) is a “suitable” function. Usually, this parameter \\(h\\) is called bandwidth and \\(K\\) is called a kernel function which is often to be the density of a probability distribution."
  },
  {
    "objectID": "posts/01_visualize_kernel_density/01_visualize_kernel_density.html#the-bandwidth",
    "href": "posts/01_visualize_kernel_density/01_visualize_kernel_density.html#the-bandwidth",
    "title": "Animating kernel density estimators",
    "section": "The Bandwidth",
    "text": "The Bandwidth\nIn geom_density(), the default kernel function is the Gaussian density and the bandwidth can be tweaked through the bw argument.\n\nh <- 1\nggplot() +\n  geom_density(aes(x = sample), bw = h) +\n  annotate(\n    \"label\", \n    x = 0.5, \n    y = 0.1, \n    label = glue::glue(\"bw = {h}\")\n  )\n\n\n\n\nOf course, I could now create multiple plots and change the value if h each time to demonstrate the effect of the bandwidth but the point of this post was to create an animation. So let’s do that instead.\nNevertheless, to create an animation, we need to be able to create multiple plots. Therefore, let us use the previous code and wrap a function depending on h around that. This function will be our plot generator depending on the bandwidth.\n\nplot_gen <- function(h) {\n  g <- ggplot() +\n    geom_density(aes(x = sample), bw = h) +\n    annotate(\n      \"label\", \n      x = 0.5, \n      y = 0.1, \n      label = glue::glue(\"bw = {h}\")\n    )\n  print(g) \n  # For the animation we need this to be printed\n}\n\nNow that we have that, define a function that creates all the plots we want to see in our animation, i.e. we create the animation frame by frame. This function can then be passed to saveGIF() from the animation package which then renders the animation for us. Creating a gif in R is as simple as that.\n\nall_the_plots <- function() {\n  map(seq(0.05, 0.5, 0.05), plot_gen)\n}\nlibrary(animation)\nsaveGIF(all_the_plots())\n\n[1] TRUE\n\n\n\n\n\n\n\nAs you can see, the bandwidth really is a smoothing parameter. Of course, too much smoothing may not yield great results so the parameter needs to be chosen with care but let us not worry about this in this blog post."
  },
  {
    "objectID": "posts/01_visualize_kernel_density/01_visualize_kernel_density.html#the-actual-estimation-procedure",
    "href": "posts/01_visualize_kernel_density/01_visualize_kernel_density.html#the-actual-estimation-procedure",
    "title": "Animating kernel density estimators",
    "section": "The Actual Estimation Procedure",
    "text": "The Actual Estimation Procedure\nLet us create another animation to visualize how kernel density estimation works on a more basic level, i.e. . In order to do so, notice that if the kernel \\(K\\) is a continuous density function of a random variable \\(X\\) (e.g. a Gaussian random variable), then the density function of the random variable \\((X + x_0)h\\) where \\(x_0 \\in \\mathbb{R}\\) and \\(h > 0\\) is given by \\(K((X - x_0)/h)/h.\\)\nConsequently, in the case of standard Gaussian random variables \\(X\\), the kernel density estimator is nothing but the average of the densities of \\(n\\) Gaussian random variables with individual means \\(x_k\\), \\(k = 1, \\ldots, n,\\) and common standard deviation \\(h.\\)\nTherefore, for a given sample you can run a kernel density estimation by taking the following steps:\n\nCheck where each data point \\(x_k\\) is located on the x-axis\nFor each data point \\(x_k\\) draw a Gaussian density \\(f_k\\) with standard deviation \\(h\\) and mean \\(x_k\\)\nFor each \\(x \\in \\mathbb{R}\\) check what are the values of \\(f_k\\), \\(k = 1, \\ldots, n,\\) at \\(x\\) and average these.\n\nSo to create a visualization of the kernel density estimation principle, we simple create a function that plots each of those steps for us. Finally, we execute all of these functions and send them to saveGIF().\nWe begin by computing the data we need to create the plots later on, i.e. we simulate a sample and compute the values of the densities.\n\ncompute_density <- function(x_0, h, K = dnorm) {\n  xx <- seq(-6, 6, 0.001) * h\n  tibble(\n    x = xx,\n    density = K((x - x_0) / h) / h\n  )\n}\n\nset.seed(123)\n# For the sake of demonstration we use a \n# small uniformly distributed sample here\nx_sample <- runif(5, -5, 5)\nh <- 1\n\n\ntib <- tibble(\n  k = seq_along(x_sample),\n  density = map(x_sample, compute_density, h = h)\n) %>%\n  unnest(density)\n\nThen, it becomes time for our first step, i.e. check where the sample values are located.\n\ndraw_axis <- function(x_sample, tib) {\n  labs <- glue::glue(\"$x_{seq_along(x_sample)}$\")\n  labs <- latex2exp::TeX(labs)\n  p <- ggplot(data = NULL, aes(x = x_sample)) +\n    theme_minimal() +\n    theme(\n      axis.line.x = element_line(),\n      panel.grid = element_blank(),\n      axis.ticks = element_line(size = 1),\n      axis.text = element_text(size = 14)\n    ) +\n    scale_x_continuous(\n      limits = c(min(tib$x), max(tib$x)),\n      breaks = sort(x_sample),\n      minor_breaks = NULL,\n      labels = labs\n    ) +\n    scale_y_continuous(\n      breaks = NULL, \n      limits = c(0, max(tib$density) + 0.025)\n    ) +\n    labs(x = element_blank())\n  p\n}\n\n\np <- draw_axis(x_sample, tib)\np\n\n\n\n\nOnce we have that, we draw the kernels.\n\ndraw_kernel <- function(p, tib) {\n  p <- p +\n    geom_line(data = tib, aes(x, density, group = k), size = 1) +\n    geom_segment(\n      aes(\n        x = x_sample,\n        xend = x_sample,\n        y = 0,\n        yend = dnorm(0)\n      ),\n      linetype = 2\n    ) +\n    labs(y = element_blank())\n  p\n}\ndraw_kernel(p, tib)\n\n\n\n\nNext, average the densities at an arbitrary position \\(x_0.\\) If we can do that, then we can iterate through different values of \\(x_0.\\)\n\nplot_until_x0 <- function(tib, x0) {\n  labs <- glue::glue(\"$x_{seq_along(x_sample)}$\")\n  labs <- latex2exp::TeX(labs)\n  tib_x0 <- tib %>%\n    filter(x <= x0) %>%\n    group_by(x) %>%\n    summarise(est = mean(density), .groups = \"drop\")\n\n\n  anim_col <- 'firebrick3'\n  g <- ggplot() +\n    geom_line(\n      data = tib,\n      aes(x, density, group = k),\n      alpha = 0.5,\n      size = 1\n    ) +\n    geom_point(\n      data = filter(tib, x == x0),\n      aes(x, density),\n      #col = anim_col,\n      alpha = 0.75,\n      size = 3\n    ) +\n    geom_vline(xintercept = x0, col = anim_col, alpha = 0.5) +\n    geom_point(\n      data = slice_tail(tib_x0, n = 1),\n      aes(x, est),\n      col = anim_col,\n      size = 3\n    ) +\n    geom_line(\n      data = tib_x0,\n      aes(x, est),\n      col = anim_col,\n      size = 1\n    ) +\n    theme_classic() +\n    theme(\n      axis.line.x = element_line(),\n      axis.line.y = element_blank(),\n      panel.grid = element_blank(),\n      axis.ticks = element_line(size = 1),\n      axis.text = element_text(size = 14)\n    ) +\n    scale_x_continuous(\n      limits = c(min(tib$x), max(tib$x)),\n      breaks = sort(x_sample),\n      minor_breaks = NULL,\n      labels = labs\n    ) +\n    scale_y_continuous(\n      breaks = NULL, \n      limits = c(0, max(tib$density) + 0.025)\n    ) +\n    labs(x = element_blank(), y = element_blank())\n\n  print(g)\n\n}\n\n\n\nx0 <- (0)\nplot_until_x0(tib, x0)\n\n\n\n\nLast but not least, we may want to display the estimated density without the underlying kernels.\n\nfinal_plot <- function(tib) {\n  labs <- glue::glue(\"$x_{seq_along(x_sample)}$\")\n  labs <- latex2exp::TeX(labs)\n  tib_x0 <- tib %>%\n    group_by(x) %>%\n    summarise(est = mean(density), .groups = \"drop\")\n\n\n  anim_col <- 'firebrick3'\n  g <- tib_x0 %>%\n    ggplot() +\n    geom_line(\n      aes(x, est),\n      col = anim_col,\n      size = 1\n    ) +\n    theme_classic() +\n    theme(\n      axis.line.x = element_line(),\n      axis.line.y = element_blank(),\n      panel.grid = element_blank(),\n      axis.ticks = element_line(size = 1),\n      axis.text = element_text(size = 14)\n    ) +\n    scale_x_continuous(\n      limits = c(min(tib$x), max(tib$x)),\n      breaks = sort(x_sample),\n      minor_breaks = NULL,\n      labels = labs\n    ) +\n    scale_y_continuous(\n      breaks = NULL, \n      limits = c(0, max(tib$density) + 0.025)\n    ) +\n    labs(x = element_blank(), y = element_blank())\n\n  print(g)\n}\nfinal_plot(tib)\n\n\n\n\nFinally, we have all the ingredients to create the animation by collecting all of these functions in a wrapper function and using it in conjunction with saveGIF().\n\ngif <- function(x_sample, tib) {\n  p <- draw_axis(x_sample, tib)\n  map(1:3, ~print(p))\n  p <- draw_kernel(p, tib)\n  map(1:5, ~print(p))\n  map(seq(min(tib$x), max(tib$x), 0.5), ~plot_until_x0(tib, .))\n  map(1:15, ~final_plot(tib))\n}\n\nsaveGIF(gif(x_sample, tib),\n        interval = 0.4, # animation speed\n        ani.width = 720,\n        ani.height = 405,\n        movie.name = \"kernelAnimation.gif\")\n\n\n\n\n\n\nThus, we have created a short animation that illustrates the kernel density estimation procedure. Probably, there is some room for improving the animation by fine tuning the plots, tweaking with the animation speed or the number of frames. For now, though, let us leave everything as it is.\nBut feel free to let me know what could be improved in the comments. Similarly, if you want to leave any other form of feedback, feel free to roam the comment section too. Finally, if you enjoyed what you have seen here but do not want to bother writing a comment, you may simply hit the applause button instead."
  },
  {
    "objectID": "posts/12_embedding_shiny_app/12_embedding_shiny_app.html",
    "href": "posts/12_embedding_shiny_app/12_embedding_shiny_app.html",
    "title": "How to embed a Shiny app into your blog posts",
    "section": "",
    "text": "Today’s a short blog post. It’s mainly for sharing a cool trick I just learned.\nHere’s a simple template to incorporate your Shiny app into an HTML file. For instance, you can incorporate your shiny app into your blog post like I do here. Simply exchange the src argument by your Shiny app’s URL and then you’re good to go. Here, I use the app that I have shown you a couple of months ago.\n<iframe src=\"https://rappa.shinyapps.io/interactive-ggplot\" data-external=\"1\" width=\"925px\" height=\"800px\">\n</iframe>\nFrom what I could tell, this is same code that knitr::include_app() drops. But including the iframe manually let’s you adjust the width and height of your frame. Beware that you will have to choose the dimensions large enough for your Shiny app.\nUPDATE: Originally, I had demonstrated the above code chunk here. But that causes unnecessary traffic on my shinyapps.io account, so I removed the demo."
  },
  {
    "objectID": "posts/ggplot2-tips/05_ggforce_examples/05_ggforce_examples.html",
    "href": "posts/ggplot2-tips/05_ggforce_examples/05_ggforce_examples.html",
    "title": "A couple of visualizations from ggforce",
    "section": "",
    "text": "It is almost the beginning of a new year and I have decided to finish off this year with a quick blog post. Also, friends were shaming me that I have been slacking off on this blog lately. Therefore, let’s get started right away. We’ll keep things simple and look at a few cool plots from the ggforce package. Of course, we have already glimpsed at this package in the previous installment of this ggplot2-tips series."
  },
  {
    "objectID": "posts/ggplot2-tips/05_ggforce_examples/05_ggforce_examples.html#mark-point-plots",
    "href": "posts/ggplot2-tips/05_ggforce_examples/05_ggforce_examples.html#mark-point-plots",
    "title": "A couple of visualizations from ggforce",
    "section": "Mark Point Plots",
    "text": "Mark Point Plots\nLet us first take a look at the penguins data set from the palmerpenguins package. Same as last time, this will be the dummy data set we use for plots but of course any other data set would be fine too.\n\nlibrary(dplyr)\nlibrary(ggplot2)\ntheme_set(theme_light())\ndat <- palmerpenguins::penguins %>% \n  filter(!is.na(sex))\np <- dat %>% \n  ggplot(aes(bill_length_mm, flipper_length_mm, col = species)) +\n  geom_point()\np\n\n\n\n\nVisually, we can see that the points are strongly grouped by species which makes sense as these kind of measurements often define a species. With help from ggforce we can visually emphasize this grouping by drawing rectangles or ellipses around the groups.\n\nlibrary(ggforce)\nrect_plot <- p + \n  geom_mark_rect(size = 1)\nellipse_plot <- p + \n  geom_mark_ellipse(aes(fill = species), alpha = 0.25)\n\nlibrary(patchwork) # see last ggplot2-tips post\nrect_plot / ellipse_plot\n\n\n\n\nThere is also a geom_mark_hull() function that requires the concaveman package to be installed. Using this function, we can draw a hull around the points.\n\np +\n  geom_mark_hull(size = 1, concavity = 3)\n\n\n\n\nBeware though that this hull is “redrawn at draw time”, so your hull may look different when you zoom into the plot. Also, let me point out that geom_mark_hull() has an argument concavity that allows you to make the hull “more wiggly”."
  },
  {
    "objectID": "posts/ggplot2-tips/05_ggforce_examples/05_ggforce_examples.html#alluvial-plots",
    "href": "posts/ggplot2-tips/05_ggforce_examples/05_ggforce_examples.html#alluvial-plots",
    "title": "A couple of visualizations from ggforce",
    "section": "Alluvial Plots",
    "text": "Alluvial Plots\nWith ggforce you can easily draw so-called alluvial plots. Originally, these are used to visualize a “stream over time” as for instance shown on Wikipedia. But the same visualization can be used to visualize “composition of groups” like so.\n\n\n\n\n\nFrom this plot, it is clear that unsurprisingly most of high weight penguins are male. What is maybe more surprising is that all Chinstrap penguins live on Dream. Obviously, the first layer in this alluvial plot is sort of redundant as the color already codes the sex but for accessibility it is often encouraged to use some form of double encoding (e.g. different shape AND color for groups). Thus, I find it practical and somewhat convenient to add this first layer.\nCreating this plot requires a couple of steps but ggforce has useful functions that make our life easier. More precisely we will need to\n\ncount occurences in each subgroup and convert this in a suitable format for later plotting. gather_set_data() will help us doing that.\ndraw lines between subgroups with geom_parallel_sets()\ndraw boxes to identify subgroups with geom_parallel_sets_axes()\nlabel the boxes with geom_parallel_sets_labels\n\nThe first step is processed as follows\n\nreshaped_dat <- dat %>% \n  mutate(\n    mass_group = factor(\n      cut_number(body_mass_g, 3),\n      labels = c(\"high\", \"medium\", \"low\")\n    )\n  ) %>%\n  count(species, island, sex, mass_group) %>% \n  gather_set_data(x = 1:4)\n\nThis simply counts the occurences in each subgroup and then adds three columns x, y and id based on the subgroup labels. These three new columns are necessary for generating the plot which is done as follows\n\nreshaped_dat %>% \n  ggplot(aes(\n    x = x, \n    split = y, \n    id = id, \n    value = n\n  )) +\n  geom_parallel_sets(aes(fill = sex), alpha = 0.5) +\n  geom_parallel_sets_axes(axis.width = 0.2) +\n  geom_parallel_sets_labels(colour = 'white', size = 4)\n\n\n\n\nHere, value is the counts of the subgroups. Also, notice that the splits on the x-axis is not in the same order as in my original plot. The order can be easily changed by converting x to a factor whose levels have the desired ordering. The complete code is\n\nreshaped_dat %>% \n  ggplot(aes(\n    x = factor(x, c(\"sex\", \"species\", \"island\", \"mass_group\")), \n    split = y, \n    id = id, \n    value = n\n  )) +\n  geom_parallel_sets(aes(fill = sex), alpha = 0.5) +\n  geom_parallel_sets_axes(axis.width = 0.2) +\n  geom_parallel_sets_labels(colour = 'white', size = 4) +\n  labs(x = element_blank()) + \n  scale_y_continuous(breaks = NULL) +\n  theme(text = element_text(size = 12)) +\n  scale_fill_brewer(palette = 'Set1')"
  },
  {
    "objectID": "posts/ggplot2-tips/05_ggforce_examples/05_ggforce_examples.html#voronoi-diagrams",
    "href": "posts/ggplot2-tips/05_ggforce_examples/05_ggforce_examples.html#voronoi-diagrams",
    "title": "A couple of visualizations from ggforce",
    "section": "Voronoi Diagrams",
    "text": "Voronoi Diagrams\nNext, let us explore Voronoi diagrams. These are constructed from a set of “center points” which are used to form polygons such that these fill the whole plane and each polygons consists of the points that are closest to a polygon’s center point. If you found this somewhat confusing, then you are in luck because Wikipedia has a super neat animation that illustrates this concept.\nUsing bill and flipper lengths to define the center points’ x- and y-coordinates, we can create a Voronoi diagram via geom_voronoi_tile() and geom_voronoi_segment() as follows.\n\ndat %>% \n  ggplot(aes(bill_length_mm, flipper_length_mm, group = 1)) +\n  geom_voronoi_tile(aes(fill = species)) +\n  geom_voronoi_segment() +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_void()\n\n\n\n\nHere, the lines between polygons are shown due to geom_voronoi_segment() and if we wish to get rid of the lines we can simply remove this layer. Also, let us ignore possible applications of Voronoi diagrams1 for a bit. What I really wanted to demonstrate is a small bit of Rtistry I found on Twitter and found really cool.\nWith a couple of random numbers and a bit of coloring one can create some visually appealing graphics (at least I like to think so). First, let’s take a look at only a few random numbers\n\nset.seed(23479)\nN <- 25\ntibble(x = runif(N), y = runif(N)) %>% \n  ggplot(aes(x, y)) +\n  geom_voronoi_tile(aes(fill = y)) +\n  scale_fill_viridis_c(option = 'A') +\n  theme_void() + \n  theme(legend.position = 'none')\n\n\n\n\nNot so super impressive but using many random numbers a “smoother” picture will be created.\n\nset.seed(23479)\nN <- 1000\ntibble(x = runif(N), y = runif(N)) %>% \n  ggplot(aes(x, y)) +\n  geom_voronoi_tile(aes(fill = y)) +\n  scale_fill_viridis_c(option = 'A') +\n  theme_void() + \n  theme(legend.position = 'none')\n\n\n\n\nOf course, arranging the center points differently and using other colors leads to very different pictures.\n\nset.seed(23479)\nN <- 1000\ntibble(x = runif(N, -1, 1), y = sqrt(abs(x) + runif(N))) %>% \n  ggplot(aes(x, y)) +\n  geom_voronoi_tile(aes(fill = y)) +\n  scale_fill_viridis_c(option = 'E') +\n  theme_void() + \n  theme(legend.position = 'none')"
  },
  {
    "objectID": "posts/ggplot2-tips/05_ggforce_examples/05_ggforce_examples.html#sina-plots",
    "href": "posts/ggplot2-tips/05_ggforce_examples/05_ggforce_examples.html#sina-plots",
    "title": "A couple of visualizations from ggforce",
    "section": "Sina Plots",
    "text": "Sina Plots\nComing back to less artistic plots, consider the following violin plots from the ggplot2 package.\n\ndat %>% \n  ggplot(aes(x = species, y = body_mass_g)) +\n  geom_violin(fill = \"grey80\")\n\n\n\n\nCompared with common boxplots, these kind of plots show the distribution of the data more explicitly with density estimates (rotated by 90 degrees and mirrored for symmetry). This gets rid of the intrinsic problem of boxplots, i.e. only showing quantiles. Sometimes though, we want to see the quantiles as well. In these instances, an additional boxplot is plotted within the violin plots like so.\n\ndat %>% \n  ggplot(aes(x = species, y = body_mass_g)) +\n  geom_violin(fill = \"grey80\") +\n  geom_boxplot(width = 0.25)\n\n\n\n\nHowever, even with both of these plots combined we still don’t know how many points are in this data set. To make that information available in the visualizations, so-called sina plots fill the area of violin plots with jittered data points instead of depicting the estimated density directly.\n\ndat %>% \n  ggplot(aes(x = species, y = body_mass_g)) +\n  geom_sina()\n\n\n\n\nIf a data set is large, then the points will display the same contour as the violin plot. In any case, the violin plot can be plotted beneath the points as well for better visibility.\n\ndat %>% \n  ggplot(aes(x = species, y = body_mass_g)) +\n  geom_violin(fill = \"grey80\") +\n  geom_sina()\n\n\n\n\nThis way, we can see both the distribution AND the number of data points in a single plot. Of course, there are more ways to display the distribution of data and ggdist is just the right package to do that job. I will show you that particular package in the next installment of the ggplot2-tips series.\nAnd that concludes our small demonstration of a few ggforce functions. For more functions check out ggforce’s website. For sure, there is more cool stuff like Bezier curves and facet zooms to explore.\nFinally, here is an overview of all the cool visuals we have created. Let me know what you think in the comments or simply hit the applause button below if you liked the content."
  },
  {
    "objectID": "posts/ggplot2-tips/06_learning_by_thieving/06_learning_by_thieving.html",
    "href": "posts/ggplot2-tips/06_learning_by_thieving/06_learning_by_thieving.html",
    "title": "ggplot-tips: Learning by Thieving",
    "section": "",
    "text": "TidyTuesday, the weekly social data project that brings together R users, is a great way to connect to the R community and learn to wrangle and visualize data. But more importantly, it is a superb chance to learn new data visualization skills by doing thieving. Let me elaborate.\nEach week, you get a chance to work with a new data set and create a (hopefully) nice visualization1. Afterwards, you can share visualizations with the world on twitter using #tidyTuesday. Of course, being the curious person that you are, you check out contributions from other fellow R users. And more often than not, you will see really cool visualizations and wish that you could do something like that too. And you can!\nUsually, people share their code together with their viz. Consequently, you are only one ctrl-C away from stepping up your dataviz game. Do I mean that you should take the entire code and brand that as your own work? Of course not! But you can maybe ctrl-C aspects of the code and reuse it for something you have been wanting to do for a long time. Let’s make this specific. Last week, I found this gem by Georgios Karamanis.\n\n\nTransphobic hate crimes in Sweden for this week's Bring Your Own Data #TidyTuesday.The inspiration was a plot made by @thomasoide for this Axios article: https://t.co/zMrnr9tszGSource: @myndigheten_bracode: https://t.co/HSCew2zrUg #Rstats #dataviz pic.twitter.com/IVQ1wTBZmt\n\n— Georgios Karamanis (@geokaramanis) January 5, 2022\n\n\nWhat intrigued me were the bars with criss-cross lines. Now, clearly I want to be able to do that too. Luckily, the tweet also contains a link to the corresponding GitHub repository. Et voilà, a quick glance at the code reveals the use of a so-called ggpattern package and a quick ctrl-C of the package name combined with a internet search leads me to the package’s documentation.\nThere, I find out that it is quite easy to get bars with different patterns2 using geom_col_pattern(). For example, these code snippets are taken straight from the documentation (more ctrl-Cs). For more, check out the documentation.\n\nlibrary(ggplot2)\nlibrary(ggpattern)\nlibrary(patchwork)\n\ndf <- data.frame(level = c(\"a\", \"b\", \"c\", 'd'), outcome = c(2.3, 1.9, 3.2, 1))\n\nstripes <- ggplot(df) +\n  geom_col_pattern(\n    aes(level, outcome, pattern_fill = level), \n    pattern = 'stripe',\n    fill    = 'white',\n    colour  = 'black'\n  ) +\n  theme_bw(18) +\n  theme(legend.position = 'none') \n\nkittens <- ggplot(df) +\n  geom_col_pattern(\n    aes(level, outcome, fill = level), \n    pattern = 'placeholder',\n    pattern_type = 'kitten',\n    pattern_size = 3\n  ) +\n  theme_bw(18) +\n  theme(legend.position = 'none') \n\nstripes + kittens\n\n\n\n\nThere you go. So, now I “can do” bars with different patterns. “Hold on, it is not like you are totally an expert now. How does any of that help?”, you might think. And, clearly you are right. Having emulated something I saw online, does not make me exactly into an visual artist but now I am equipped with one more tool to try out come next TidyTuesday.\nRepeat that often enough and soon you have acquired a lot of tools to use in diverse settings. Eventually, the lines between “I copied what I found online” and “This is a trick I like to do frequently” blur. In the end, repeated practice and learning from others is what makes you into an expert. And sometimes that “learning from others” part is as simple as strolling through GitHub repositories on the lookout for your next great coup.\n\n\n\n\nFootnotes\n\n\nHonestly, it does not really matter if your visualization is looking “nice”. I have ended up sharing a bunch of, say, average at best visualizations. (Exhibit A, Exhibit B). The point is too keep showing up and trying. In fact, even the visualizations I am not totally proud of contain elements which I have spent a lot of time working on. This practice has often ended up helping me in unexpected situations.↩︎\nI know, I know. The tweet was using geom_rect_pattern(). Not exactly the same but the principles are.↩︎"
  },
  {
    "objectID": "posts/ggplot2-tips/01_a_few_ggplot_tips/01_a_few_ggplot_tips.html",
    "href": "posts/ggplot2-tips/01_a_few_ggplot_tips/01_a_few_ggplot_tips.html",
    "title": "Beginning a ggplot2 Series: Logarithmize Your Scales",
    "section": "",
    "text": "It is not that long ago when I first encountered ggplot2 and decided to learn how to use it1. By no means do I think that I have sufficiently mastered this package yet but as time has passed I have certainly picked up a few tips on my journey to get better at creating more meaningful visualizations. So, in order to remind myself of and share what I learned, I decided to create a sort of series containing tips that enhanced my visualization skills.\nHowever, this is not supposed to be an intro to ggplot2 though. I have already done that and you can find it in the data exploration chapter of my “Yet Again: R + Data Science” lecture notes (see YARDS). Currently, I plan to make each installment of the series somewhat short to keep it simple and all further posts in this series will be collected under the ggplot2-tips series tag which you can also access from this blog’s main page. So, without further ado, let us begin."
  },
  {
    "objectID": "posts/ggplot2-tips/01_a_few_ggplot_tips/01_a_few_ggplot_tips.html#using-log-transforms",
    "href": "posts/ggplot2-tips/01_a_few_ggplot_tips/01_a_few_ggplot_tips.html#using-log-transforms",
    "title": "Beginning a ggplot2 Series: Logarithmize Your Scales",
    "section": "Using log-transforms",
    "text": "Using log-transforms\nOften, one finds variables in a data set that resemble heavy-tailed distributions and you can detect it by a simple histogram in a lot of cases. For instance, take a look at the variable sale_price of the ames dataset from the modeldata package. This variable contains the sale price of 2930 properties in Ames, Iowa and its histogram looks like this:\n\nlibrary(tidyverse)\nlibrary(modeldata)\ndata(ames)\n# I like to clean names s.t. no capital letters are used in the variable names\names <- ames %>% janitor::clean_names()\n\names %>%\n  ggplot(aes(x = sale_price)) +\n  geom_histogram()\n\n\n\n\nAs you can see, the distribution looks skewed in the sense that most of the sale prices fall within one range but there are also sale prices that are comparatively high. In effect, the histogram depicts a “long tail” and the highly priced sales are not that easily discernible in the histogram as the column heights may become really small and there may be large “gaps” between columns as seen above.\nOne common way to deal with this is to apply a logarithm to the variable of interest. It does not really matter which logarithm you use but since we like to work in a decimal system, a logarithm with base 10 is often used. Let’s see how this changes the picture.\n\names %>%\n  ggplot(aes(x = log(sale_price, 10))) +\n  geom_histogram()\n\n\n\n\nAdmittedly, we have a gap in the histogram on the left hand side now but overall the histogram looks way less skewed. In fact, this somewhat resembles what a histogram of a normally distributed random variable could look like. This is nice because Gaussian variables are something which a lot of statistical techniques can work with best.\nThus, working with a logarithmized variable might be helpful in the long run. Note that sometimes a variable benefits from being logarithmized but also contains values that are zero. To apply the logarithm anyway, often one then offsets the variable by shifting the variable by 1.\nUnfortunately, it may be nice that logarithmized variables are beneficial for statistical techniques and that the histograms are less skewed but the way we achieved that in the above example let’s the audience of the visualization somewhat clueless as to what the actual sale prices were. Sure, if in doubt, one could simply use a calculator to compute \\(10^{4}\\) and \\(10^{6}\\) to get a feeling for the range of the sale prices but of course no one will want to do that. This brings me to my next point."
  },
  {
    "objectID": "posts/ggplot2-tips/01_a_few_ggplot_tips/01_a_few_ggplot_tips.html#use-scale__log10",
    "href": "posts/ggplot2-tips/01_a_few_ggplot_tips/01_a_few_ggplot_tips.html#use-scale__log10",
    "title": "Beginning a ggplot2 Series: Logarithmize Your Scales",
    "section": "Use scale_*_log10()",
    "text": "Use scale_*_log10()\nHonestly, I don’t know why but for a long time I have logarithmized data for visualization purposes as above because using scale_x_log10() felt somewhat frightening because I did not understand what was going on there. Take a look what happens if I add this particular layer to our initial plot instead of logarithmizing manually.\n\names %>%\n  ggplot(aes(x = sale_price)) +\n  geom_histogram() +\n  scale_x_log10() \n\n\n\n\nNotice that the overall impression of the picture is the same as with the manually logarithmized plot. However, the x-axis is now logarithmized as opposed to being linear. So, manual logarithmization of the variable leads to just that: A transformation of the data but the axis in the plot remains linear which is why the labels on that x-axis showed values that needed to be retransformed to its original values.\nIn contrast, using scale_x_log10() leaves the data as it is but transforms the x-axis. In this case, this new axis is used for binning and counting to compute the histogram. Therefore, we can easily see that the majority of the sale prices lie between 100,000 and 300,000. Of course, things would be even simpler if the axis labels were not given in scientific notation. Luckily, we can easily change that."
  },
  {
    "objectID": "posts/ggplot2-tips/01_a_few_ggplot_tips/01_a_few_ggplot_tips.html#adjust-labels-using-the-scales-package",
    "href": "posts/ggplot2-tips/01_a_few_ggplot_tips/01_a_few_ggplot_tips.html#adjust-labels-using-the-scales-package",
    "title": "Beginning a ggplot2 Series: Logarithmize Your Scales",
    "section": "Adjust labels using the scales package",
    "text": "Adjust labels using the scales package\nAs its name says, the scales package works really well in conjunction with the scale_* layers from ggplot2. In fact, this can make it somewhat comfortable to quickly adjust axis labels by simply passing a function (mind the ()) from the scales package to the scale_* layer’s argument labels. Here, we may simply use label_number() to get rid of the scientific notation.\n\names %>%\n  ggplot(aes(x = sale_price)) +\n  geom_histogram() +\n  scale_x_log10(labels = scales::label_number()) \n\n\n\n\nEven better, scales has a lot of functions that are useful for specific units such as dollar or week, month, year (in case you are working with time data whose labels can be a special kind of pain).\n\names %>%\n  ggplot(aes(x = sale_price)) +\n  geom_histogram() +\n  scale_x_log10(labels = scales::label_dollar()) \n\n\n\n\nOf course, the same thing works not only for the x-axis scale but for all kinds of other scales too. For instance, if you want to plot the same histogram but oriented vertically, you can simply change the x-aesthetic to be the y-aesthetic which means that you will need to adjust the y scale then.\n\names %>%\n  ggplot(aes(y = sale_price)) +\n  geom_histogram() +\n  scale_y_log10(labels = scales::label_dollar()) \n\n\n\n\nIn retrospect, it is really easy to adjust the axis with scale_* layers and the scales package and I really do not know why I hesitated in the past to use these functions. I guess adding another layer to the plot felt somewhat harder and slower than brute-forcing my way through the transformation. But believe me, in the long run this takes up way more of your time (especially if you want to interpret your plot later on).\nI hope that you enjoyed this post and if you did, feel free to hit the applause button below. In any case, I look forward to see you in the next installment of this series."
  },
  {
    "objectID": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html",
    "href": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html",
    "title": "4 Ways to use colors in ggplot more efficiently",
    "section": "",
    "text": "When creating a plot I frequently catch myself using way too many colors. Thus, I have to remind myself often to keep things simple. Usually, this makes a data visualization way more effective.\nLuckily, I found a neat datawrapper blogpost by Lisa Charlotte Muth that shows us how to reduce the use of colors.\nBut as I was reading the blog post, I found myself wondering how some of the mentioned principles could be implemented in ggplot. Naturally, I began experimenting and created a few example plots using fewer colors. This post will show you how you can do that too."
  },
  {
    "objectID": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html#preliminaries",
    "href": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html#preliminaries",
    "title": "4 Ways to use colors in ggplot more efficiently",
    "section": "Preliminaries",
    "text": "Preliminaries\nFor completeness’ sake, let me mention the basic settings I will use for all visualizations. Honestly, if you have no idea what happens in the following code chunk, just skip it. More or less, this chunk makes sure that all plots are using theme_minimal() plus a small number of tweaks. These tweaks are\n\nThe use of the Fira Sans font with help from the showtext package.\nThe plot titles are aligned to the left, have some spacing around them and are colored using a color from the Okabe Ito color palette. Ever since I read Fundamentals of Data Visualization by Claus Wilke, I am fond of this color palette as I find the colors nice and apparently it is also color-blind safe.\n\n\nlibrary(tidyverse)\nlibrary(showtext)\nfont_add_google(\"Fira Sans\", \"firasans\")\nshowtext_auto()\n\ntheme_customs <- theme(\n  text = element_text(family = 'firasans', size = 16),\n  plot.title.position = 'plot',\n  plot.title = element_text(\n    face = 'bold', \n    colour = thematic::okabe_ito(8)[6],\n    margin = margin(t = 2, r = 0, b = 7, l = 0, unit = \"mm\")\n  ),\n)\n\ntheme_set(theme_minimal() + theme_customs)"
  },
  {
    "objectID": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html#show-shades-not-hues",
    "href": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html#show-shades-not-hues",
    "title": "4 Ways to use colors in ggplot more efficiently",
    "section": "Show shades, not hues",
    "text": "Show shades, not hues\nAlright, enough with the preliminaries. Let’s count how many different car classes are represented in the mpg dataset from the ggplot2 package. I am sure you have seen the data already when you read this ggplot post. So, no further comment on this data set.\n\nmpg %>% \n  ggplot(aes(x = year, fill = class)) +\n  geom_bar()\n\n\n\n\nUgh, this is a colorful mess and sort of reminds me of the gnome rainbow puking gif. Let’s reduce the color load by sticking to only three colors. To differentiate between classes we will make some colors more transparent.\nThus, we need to create a new variable in our data set that lumps the classes into three groups (for the colors).\n\n# Group classes into three groups (to reduce colors to 3)\ndat <- mpg %>% \n  mutate(\n    year = factor(year),\n    class_group = case_when(\n      class %in% c('2seater', 'compact', 'midsize') ~ \"grp1\",\n      class == 'minivan' ~ \"grp2\",\n      T ~ \"grp3\"\n    )\n  )\n\nNow that this is done, we can map fill to our new class_group variable and the regular class variable to alpha.\n\nshades_plt <- dat %>% \n  ggplot(aes(x = year, fill = class_group, alpha = class)) +\n  geom_bar() +\n  labs(\n    x = 'Year',\n    y = 'Counts',\n    alpha = 'Class',\n    title = 'Show shades, not hues'\n  )\nshades_plt \n\n\n\n\nFor better control of the visuals let us manually create and assign colors and the transparency levels.\n\n# Color-blind safe colors\ncolors <-  thematic::okabe_ito(3)\n# Possible levels of transparency (one for each class)\nalpha_max <- 1\nalpha_min <- 0.7\nalpha_vals <- c(\n  seq(alpha_max, alpha_min, length.out = 4), \n  seq(alpha_min, alpha_max, length.out = 4)[-1]\n)\nalpha_vals\n\n[1] 1.0 0.9 0.8 0.7 0.8 0.9 1.0\n\n# Tweak previous plot\nshades_plt <- shades_plt +\n  scale_fill_manual(values = colors) +\n  scale_alpha_manual(values = alpha_vals)\nshades_plt\n\n\n\n\nNext, let us consolidate the two legends into one. This can be done via guides(). Here, the fill guide will be set to guide_none() to get rid of the class_group legend.\nAlso, the alpha guide needs to be manually overwritten via override.aes in guide_legend() using the color codes that we saved in the vector colors. This way, the alpha legend will also depict the colors instead of only the transparency level.\n\nshades_plt <- shades_plt +\n  guides(\n    fill = guide_none(),\n    alpha = guide_legend(\n      override.aes = list(fill = colors[c(1, 1, 1, 2, 3, 3, 3)]\n      )\n    )\n  ) \nshades_plt"
  },
  {
    "objectID": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html#group-categories-together-by-color-but-keep-showing-them",
    "href": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html#group-categories-together-by-color-but-keep-showing-them",
    "title": "4 Ways to use colors in ggplot more efficiently",
    "section": "Group categories together by color, but keep showing them",
    "text": "Group categories together by color, but keep showing them\nSo, this already looks better. However, adjacent colored blocks now “merge” into each other. This can make it hard to differentiate between classes.\nTo overcome this issue, add lines between blocks. Luckily, this is spectacularly easy and done by setting the color aesthetic in geom_bar() to white. Here’s the complete code.\n\ndat %>% \n  ggplot(aes(x = year, fill = class_group, alpha = class)) +\n  geom_bar(col = 'white') + # Add lines for distinction\n  scale_fill_manual(values = colors) +\n  scale_alpha_manual(values = alpha_vals) +\n  guides(\n    fill = guide_none(),\n    alpha = guide_legend(override.aes = list(fill = colors[c(1, 1, 1, 2, 3, 3, 3)]))\n  ) +\n  labs(\n    x = 'Year',\n    y = 'Counts',\n    alpha = 'Class',\n    title = 'Group categories together by color, \\nbut keep showing them'\n  )"
  },
  {
    "objectID": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html#emphasize-just-one-or-a-few-categories",
    "href": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html#emphasize-just-one-or-a-few-categories",
    "title": "4 Ways to use colors in ggplot more efficiently",
    "section": "Emphasize just one or a few categories",
    "text": "Emphasize just one or a few categories\nNext, let us switch tracks and look at some other kind of data. At Our World in Data you can find a lot of interesting data sets. One of these contains survey information on who Americans spend their time with (in average minutes per day by age). If you download this data set, you can create a plot like this.\n\n# Some data wrangling\ntime_data <- read_csv(\"time-spent-with-relationships-by-age-us.csv\") %>% \n  rename_with(\n    ~c('Entitity', 'Code', 'Age', 'alone', 'friends', 'children', 'parents', \n       'partner', 'coworkers')\n  ) %>% \n  pivot_longer(\n    cols = alone:coworkers, \n    names_to = 'person',\n    values_to = 'minutes'\n  ) %>% \n  janitor::clean_names() %>% \n  filter(age <= 80) \n\n# Color-blind safe colors\ncolors <- thematic::okabe_ito(8)[-6]\n\n# Line plot\np <- time_data %>% \n  ggplot(aes(x = age, y = minutes, col = person)) +\n  geom_line(size = 1.5) +\n  scale_color_manual(values = colors) +\n  coord_cartesian(xlim = c(15, 81), expand = F) +\n  scale_y_continuous(minor_breaks = NULL) +\n  labs(x = 'Age (in years)', y = 'Minutes', col = 'Time spent')\np\n\n\n\n\nOnce again, we created a plot with loads of color. If this were an interactive plot where we can focus on one line at a time, this would not necessarily be a problem. However, as it is, this is a rather messy spaghetti plot and extracting meaning from it is hard.\nBut if we know what story we want to tell, then we can save this plot by emphasizing only the important parts. This is where the gghighlight package shines. It works by adding a gghighlight() layer to an existing plot with conditions for filtering. All data points that do not fulfill these conditions are greyed out.\n\nlibrary(gghighlight)\nalone_plt <- p + \n  gghighlight(person == 'alone', use_direct_label = F) +\n  labs(title = 'Emphasize just one or a few categories')\nalone_plt\n\n\n\n\nFinally, we are only one text annotation away from telling a story.\n\nalone_plt +\n  annotate(\n    'text',\n    x = 15,\n    y = 455,\n    label = 'We spend a lot of time alone...',\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    size = 7\n  )\n\n\n\n\nOf course, a data set may contain multiple stories that may also need multiple highlights. No problem. With gghighlight() we can combine as many conditions as we like.\n\nage_40_plt <- p + \n  gghighlight(\n    person %in% c('alone', 'children'), \n    age >= 38, \n    use_direct_label = F\n  ) +\n  geom_segment(x = 38, xend = 38, y = -Inf, yend = 300, linetype = 2, col = 'grey20') +\n  labs(title = 'Emphasize just one or a few categories') \n\nage_40_plt +\n  annotate(\n    'text',\n    x = 15,\n    y = 403,\n    label = 'Around the age of 40, we spend \\nless time with children and \\nmore time alone.',\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    lineheight = 0.85,\n    size = 5.5\n  )"
  },
  {
    "objectID": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html#label-directly",
    "href": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html#label-directly",
    "title": "4 Ways to use colors in ggplot more efficiently",
    "section": "Label directly",
    "text": "Label directly\nIn all previous plots, we displayed a legend at the side of the plot. However, this requires quite a large amount of space which we can save by direct labeling (either with annotate() for a single label or geom_text() for multiple labels).\n\nalone_plt +\n  annotate(\n    'text',\n    x = 15,\n    y = 455,\n    label = 'We spend a lot of time alone...',\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    size = 7\n  ) +\n  annotate(\n    'text', \n    x = 70, \n    y = 420, \n    label = 'alone',\n    hjust = 0,\n    vjust = 0,\n    size = 7,\n    family = 'firasans',\n    color = colors[1]\n  ) +\n  labs(title = 'Label directly') +\n  theme(legend.position = 'none')\n\n\n\n\nThis way, we save a lot of space and can give the remaining part of the plot more room. Also, this saves the reader some cognitive effort because one does not have to switch back and forth between legend and actual plot.\nIn this particular case, there is another option for direct labelling. Notice how close the word ‘alone’ from the original text annotation is to the highlighted line anyway. Therefore, we may as well save us one additional annotation and colorize a single word in the orginal annotation.\nTo do so, the ggtext package and a bit of HTML magic will help us. Basically, what we need is to change the annotation from text geom to richtext geom and create a string that contains the HTML-code for colored text. Here that is <span style = 'color:#E69F00;'>...</span>.\n\nlibrary(ggtext)\ncolor_alone <- glue::glue(\n  \"We spend a lot of time <span style = 'color:{colors[1]};'>alone</span>...\"\n)\ncolor_alone\n\nWe spend a lot of time <span style = 'color:#E69F00;'>alone</span>...\n\nalone_plt +\n  labs(title = 'Label directly') +\n  annotate(\n    'richtext',\n    x = 15,\n    y = 400,\n    label = color_alone,\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    size = 7,\n    label.color = NA\n  ) +\n  theme(legend.position = 'none')\n\n\n\n\nNaturally, we can do this for our second highlighted plot as well. In this case, the colored key words are not adjacent to the actual lines.\n\nage_40_text <- glue::glue(\n  \"Around the age of 40, we spent less <br> time with \n  <span style = 'color:{colors[2]};'>children</span> \n  and more <br> time <span style = 'color:{colors[1]};'>alone</span>.\"\n)\n\nage_40_plt +\n  labs(title = 'Label directly') +\n  annotate(\n    'richtext',\n    x = 15,\n    y = 400,\n    label = age_40_text,\n    hjust = 0,\n    vjust = 0,\n    family = 'firasans',\n    lineheight = 1.25,\n    size = 5.5,\n    label.color = NA\n  ) +\n  theme(legend.position = 'none')\n\n\n\n\nConsequently, the reader may have to go back and forth between text and lines again but still we used our space more efficiently. So, I will let this count as direct labeling.\nFinally, let us come full circle and return to our initial bar plot. This one could also use some direct labels. Normally, I would simply add a geom_text() layer together with position_stack() to the initial plot as described here.\nBut for some magical reason, this did not align the labels properly and it was driving me crazy. Therefore, I counted the car classes and computed the label positions manually.\n\nmanual_counts <- mpg %>% \n  count(year, class) %>% \n  mutate(\n    year = factor(year),\n    class_group = case_when(\n      class %in% c('2seater', 'compact', 'midsize') ~ \"grp1\",\n      class == 'minivan' ~ \"grp2\",\n      T ~ \"grp3\"\n    )\n  ) \n\nlabels <- manual_counts %>% \n  mutate(class = factor(class)) %>%  \n  group_by(year) %>% \n  arrange(year, desc(class)) %>% \n  mutate(\n    csum = cumsum(n), \n    n = (lag(csum, default = 0) + csum) / 2\n  )\n\nBut once this small detour is overcome, we can label the plot in the same manner as before. Unfortunately, the 2seater class is so small that the label wouldn’t fit into the box. Therefore, I decided to plot the label on top.\n\nmanual_counts %>% \n  ggplot(aes(x = year, y = n, fill = class_group)) +\n  geom_col(aes(alpha = class), col = 'white') +\n  scale_fill_manual(values = colors) +\n  scale_alpha_manual(values = alpha_vals) +\n  labs(\n    x = 'Year',\n    y = 'Counts',\n    alpha = 'Class',\n    title = 'Label directly'\n  ) +\n  # Add all but one label\n  geom_text(\n    data = labels %>% filter(class != '2seater'),\n    aes(label = class), \n    col = 'white',\n    family = 'firasans',\n    size = 5,\n    fontface = 'bold'\n  ) +\n  # Add 2seater label\n  geom_text(\n    data = labels %>% filter(class == '2seater'),\n    aes(y = n + 3, label = class), \n    col = 'black',\n    family = 'firasans',\n    size = 5,\n    fontface = 'bold'\n  ) +\n  theme(legend.position = 'none')"
  },
  {
    "objectID": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html#closing-remarks",
    "href": "posts/ggplot2-tips/07_four_ways_colors_more_efficiently/07_four_ways_colors_more_efficiently.html#closing-remarks",
    "title": "4 Ways to use colors in ggplot more efficiently",
    "section": "Closing remarks",
    "text": "Closing remarks\nThe blog post that inspired this post contains a few more tips like using other indicators than color and you should definitely check it out. Also, Lisa Muth apparently writes a book on colors in data visualizations and documents her thoughts here. If you look for more content on colors, this might be a fountain of information.\nAs for using patterns instead of colors, I recently wrote a blog post that leverages the ggpattern package to do just that. Check it out here. And as always, if you don’t want to miss new blog post, either follow me on Twitter or via my RSS feed."
  },
  {
    "objectID": "posts/ggplot2-tips/11_rounded_rectangles/11_rounded_rectangles.html",
    "href": "posts/ggplot2-tips/11_rounded_rectangles/11_rounded_rectangles.html",
    "title": "Storytelling in ggplot using rounded rectangles",
    "section": "",
    "text": "A standard ggplot output can rarely convey a powerful message. For effective data visualization you need to customize your plot. A couple of weeks ago, I showed you how.\nIn this blog post, I will rebuild another great data viz from scratch. If you have read my original blog post, then you won’t have to learn many new tricks. Most of the techniques that I use can be found there. This is also why I save explanations only for the parts that are new. This should keep this blog post a bit shorter. You’re welcome.\nNevertheless, in today’s installment of my ggplot2 series I will teach you something truly special. I will teach you how to create…*drum roll*…rounded rectangles. Sounds exciting, doesn’t it? Well, maybe not. But it looks great. Check out what we’ll build today.\nThis plot comes to you via another excellent entry of the storytelling with data (SWD) blog. To draw rectangles with rounded rectangles we can leverage the ggchicklet package. Though, for some mysterious reason, the geom_* that we need is hidden in that package. Therefore, we will have to dig it out. That’s the easy way to do it. And honestly, this is probably also the practical way to do it.\nHowever, every now and then I want to do things the hard way. So, my dear reader, this is why I will also show you how to go from rectangles to rounded rectangles the hard way. But only after showing you the easy way first, of course. Only then, in the second part of this blog post, will I take the sadistically-inclined among you on a tour to the world of grobs.\nGrobs, you say? Is that an instrument? No, Patrick, it is an graphical object. Under the hood, we can transform a ggplot to a list of graphical objects. And with a few hacks, we can adjust that list. This way, the list will contain not rectGrobs but roundrectGrobs. Then, we can put everything back together, close the hood and enjoy our round rectangles. Now, enough intro, let’s go."
  },
  {
    "objectID": "posts/ggplot2-tips/11_rounded_rectangles/11_rounded_rectangles.html#basic-plot",
    "href": "posts/ggplot2-tips/11_rounded_rectangles/11_rounded_rectangles.html#basic-plot",
    "title": "Storytelling in ggplot using rounded rectangles",
    "section": "Basic plot",
    "text": "Basic plot\nFirst, let us recreate the “bad” plot that the above SWD blog post remodels. In the end, we will work on the remodeled data viz too. As always, though, there is something to be learnt from creating an ugly plot. So, here’s the beast that we will build.\n\n\n\n\n\nSource: Storytelling with data (SWD) blog\n\n\n\n\n\nRead data\nI didn’t find the underlying data and had to guess the values from the plot. Thus, I probably didn’t get the values exactly right. But for our purposes this should suffice. If you want, you can download the European csv-file that I created here.\n\nlibrary(tidyverse)\n# Use read_csv2 because it's an European file\ndat <- read_csv2('ratios.csv')\n\n\n\nCompute averages\nLet me point out that taking the average of the ratios may not necessarily give an appropriate result (in a statistical kind of sense). But, once again, this should not bother us as we only want to learn how to plot.\n\navgs <- dat %>% \n  pivot_longer(\n    cols = -1,\n    names_to = 'type',\n    values_to = 'ratio'\n  ) %>% \n  group_by(type) %>% \n  summarise(ratio = mean(ratio)) %>% \n  mutate(location = 'REGION AVERAGE')\navgs\n\n# A tibble: 3 × 3\n  type               ratio location      \n  <chr>              <dbl> <chr>         \n1 inventory_turnover  9.78 REGION AVERAGE\n2 store_lower         7.11 REGION AVERAGE\n3 store_upper        12.1  REGION AVERAGE\n\n### Combine with data \ndat_longer <- dat %>% \n  pivot_longer(\n    cols = -1,\n    names_to = 'type',\n    values_to = 'ratio'\n  ) \ndat_longer_with_avgs <- dat_longer %>% \n  bind_rows(avgs)\n\n\n\nCreate bars\n\n## Colors we will use throughout this blog post\ncolor_palette <- thematic::okabe_ito(8)\n\n# Make sure that bars are in the same order as in the data set\ndat_factored <- dat_longer %>% \n  mutate(location = factor(location, levels = dat$location)) \n\np <- dat_factored %>% \n  ggplot(aes(location, ratio)) +\n  geom_col(\n    data = filter(dat_factored, type == 'inventory_turnover'),\n    fill = color_palette[2]\n  ) +\n  theme_minimal()\np\n\n\n\n\n\n\nTurn labels and get rid of axis text\n\np <- p +\n  labs(x = element_blank(), y = element_blank()) +\n  theme(\n    axis.text.x = element_text(angle = 50, hjust = 1)\n  )\np\n\n\n\n\n\n\nRemove expansion to get x-labels closer to the bars\n\np <- p + coord_cartesian(ylim = c(0, 30), expand = F)\np\n\n\n\n\n\n\nRemove other grid lines\n\np <- p + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.major.y = element_line(colour = 'black', size = 0.75)\n  )\np\n\n\n\n\n\n\nFormat y-axis\n\np <- p +\n  scale_y_continuous(\n    breaks = seq(0, 30, 5),\n    labels = scales::label_comma(accuracy = 0.1)\n  )\np\n\n\n\n\n\n\nAdd points\n\np <- p +\n  geom_point(\n    data = filter(dat_factored, type == 'store_lower'),\n    col = color_palette[1],\n    size = 3\n  ) +\n  geom_point(\n    data = filter(dat_factored, type == 'store_upper'),\n    col = color_palette[3],\n    size = 3\n  ) \np\n\n\n\n\n\n\nAdd average lines\n\np <- p +\n  geom_hline(\n    yintercept = avgs[[3, 'ratio']], \n    size = 2.5, \n    col = color_palette[3]\n  ) +\n  geom_hline(\n    yintercept = avgs[[2, 'ratio']], \n    size = 2.5, \n    col = color_palette[1]\n  )\np\n\n\n\n\n\n\nAdd text labels\n\np +\n  geom_text(\n    data = filter(dat_factored, type == 'inventory_turnover'),\n    aes(label = scales::comma(ratio, accuarcy = 0.1)),\n    nudge_y = 0.8,\n    size = 2.5\n  )"
  },
  {
    "objectID": "posts/ggplot2-tips/11_rounded_rectangles/11_rounded_rectangles.html#improved-plot",
    "href": "posts/ggplot2-tips/11_rounded_rectangles/11_rounded_rectangles.html#improved-plot",
    "title": "Storytelling in ggplot using rounded rectangles",
    "section": "Improved plot",
    "text": "Improved plot\nNow, let us begin building the improved plot. First, let us get the long labels onto the y-axis and use regular rectangles before we worry about the rounded rectangles.\n\nFlip axes and use rectangles to show upper and lower bounds.\nUnfortunately, geom_rect() does not work as intended.\n\ndat_with_avgs <- dat_longer_with_avgs %>% \n  pivot_wider(\n    names_from = 'type',\n    values_from = 'ratio'\n  ) \n\ndat_with_avgs %>% \n  ggplot() +\n  geom_rect(\n    aes(\n      xmin = store_lower, \n      xmax = store_upper, \n      ymin = location, \n      ymax = location\n    )\n  )\n\n\n\n\nInstead, let us create a new numeric column containing a location’s rank based on its inventory_turnover. This is done with row_number(). While we’re at it, let us create a new tibble that also contains information on the colors each geom will use. Then, we can map to these new columns in ggplot and make sure that the values are used as is by setting scale_*_identity(). This is one convenient way to control the aesthetics of each geom without functional programming. With the image from above in mind, we know that our final plot will need\n\ndifferent col, fill and size values in geom_point()\ndifferent fill and alpha values in geom_rect()\n\nHere’s what this tibble looks like.\n\nbar_height <- 0.4 \nno_highlight_col <- 'grey70'\naverage_highlight_col <- 'grey40'\nbelow_highlight <- color_palette[2]\n\nsorted_dat <- dat_with_avgs %>% \n  mutate(num = row_number(inventory_turnover)) %>% \n  # Sort so that everything is in order of rank\n  # Important for text labels later on\n  arrange(desc(num)) %>% \n  mutate(\n    rect_color = case_when(\n      inventory_turnover < store_lower ~ below_highlight,\n      location == 'REGION AVERAGE' ~ average_highlight_col,\n      T ~ no_highlight_col\n    ),\n    rect_alpha = if_else(\n      inventory_turnover < store_lower,\n      0.5,\n      1\n    ),\n    point_color = if_else(\n      inventory_turnover < store_lower,\n      below_highlight,\n      'black'\n    ),\n    point_fill = if_else(\n      inventory_turnover < store_lower,\n      below_highlight,\n      'white'\n    ),\n    point_size = if_else(\n      inventory_turnover < store_lower,\n      3,\n      2\n    )\n  )\nsorted_dat\n\n# A tibble: 24 × 10\n   locat…¹ inven…² store…³ store…⁴   num rect_…⁵ rect_…⁶ point…⁷ point…⁸ point…⁹\n   <chr>     <dbl>   <dbl>   <dbl> <int> <chr>     <dbl> <chr>   <chr>     <dbl>\n 1 Castle…    14.7    24.3    20      24 #009E73     0.5 #009E73 #009E73       3\n 2 Wellsv…    13.6     7.6     2.5    23 grey70      1   black   white         2\n 3 Basin …    12.7     8       4.5    22 grey70      1   black   white         2\n 4 Atlant…    12.3    12.8     7.8    21 grey70      1   black   white         2\n 5 Neverl…    12.1    18      13.4    20 #009E73     0.5 #009E73 #009E73       3\n 6 Bluffi…    11.7     4.2     3      19 grey70      1   black   white         2\n 7 Bikini…    11.6    12.5     7.8    18 grey70      1   black   white         2\n 8 Metrop…    11.3    24      11.3    17 grey70      1   black   white         2\n 9 Hill V…    11      22       7.5    16 grey70      1   black   white         2\n10 Venusv…    10.4    15.5    12.3    15 #009E73     0.5 #009E73 #009E73       3\n# … with 14 more rows, and abbreviated variable names ¹​location,\n#   ²​inventory_turnover, ³​store_upper, ⁴​store_lower, ⁵​rect_color, ⁶​rect_alpha,\n#   ⁷​point_color, ⁸​point_fill, ⁹​point_size\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow, we can create our plot. Notice that I set shape = 21 in geom_point() to use both the fill and col aesthetic.\n\nsorted_dat %>% \n  ggplot() +\n  geom_rect(\n    aes(\n      xmin = store_lower, \n      xmax = store_upper, \n      ymin = num - bar_height, \n      ymax = num + bar_height, \n      fill = rect_color,\n      alpha = rect_alpha\n    ),\n  ) +\n  geom_point(\n    aes(\n      x = inventory_turnover,\n      y = num,\n      fill = point_fill,\n      col = point_color,\n      size = point_size\n    ),\n    shape = 21,\n    stroke = 1\n  ) +\n  scale_fill_identity() +\n  scale_color_identity() +\n  scale_size_identity() +\n  scale_alpha_identity() +\n  theme_minimal()\n\n\n\n\n\n\nUse ggchicklet for rounded rectangles\nThe whole point of this blog post is to use rounded rectangles. So let’s do that. The ggchicklet package has a geom called geom_rrect(). It works just like geom_rect() but accepts another value r which is used to determine the radius of the rounded rectangles. Unfortunately, this geom is not an exported function of this package. This means that if you write ggchicklet:: (e.g. in RStudio) and press TAB you won’t see geom_rrect(). Thus, you have to access the internal function via ::: (three colons).\n\np <- sorted_dat %>% \n  ggplot() +\n  ggchicklet:::geom_rrect(\n    aes(\n      xmin = store_lower, \n      xmax = store_upper, \n      ymin = num - bar_height, \n      ymax = num + bar_height, \n      fill = rect_color,\n      alpha = rect_alpha\n    ),\n    # Use relative npc unit (values between 0 and 1)\n    # This ensures that radius is not too large for your canvas\n    r = unit(0.5, 'npc')\n  ) +\n  geom_point(\n    aes(\n      x = inventory_turnover,\n      y = num,\n      fill = point_fill,\n      col = point_color,\n      size = point_size\n    ),\n    shape = 21,\n    stroke = 1\n  ) +\n  scale_fill_identity() +\n  scale_color_identity() +\n  scale_size_identity() +\n  scale_alpha_identity() +\n  theme_minimal()\np\n\n\n\n\n\n\nRemove grid lines, move axis and add some text elements\nWe will set the y-axis labels manually later on. Otherwise, we cannot change its colors one-by-one. For now, let’s get rid of superfluous grid lines, move the x-axis and add a title.\nNotice that I draw the axis line manually with a segment annotation. This seems weird, I know. Unfortunately, it cannot be helped because I still need room for the y-axis labels. And if I do not plot the axis line manually, then the axis line will start all the way to the left. Make sure that you set clip = 'off' in coord_cartesian() for the annotation to be displayed.\n\ntitle_lab <- 'Review stores with turnover ratios that are below their\\nforecasted range'\ntitle_size <- 14\naxis_label_size <- 8\ntext_size <- 18\np <- p +\n  scale_x_continuous(\n    breaks = seq(0, 25, 5),\n    position = 'top'\n  ) +\n  coord_cartesian(\n    xlim = c(-5, 25), \n    ylim = c(0.75, 24.75),  \n    expand = F,\n    clip = 'off'\n  ) +\n  annotate(\n    'segment',\n    x = 0,\n    xend = 25,\n    y = 24.75,\n    yend = 24.75,\n    col = no_highlight_col,\n    size = 0.25\n  ) +\n  labs(\n    x = 'INVENTORY TURNOVER RATIO',\n    y = element_blank(),\n    title = title_lab\n  ) +\n  theme(\n    text = element_text(\n      size = text_size,\n      color = average_highlight_col\n    ),\n    plot.title.position = 'plot',\n    panel.grid = element_blank(),\n    axis.title.x = element_text(\n      size = axis_label_size,\n      hjust = 0.21,\n      color = no_highlight_col\n    ),\n    axis.text.x = element_text(\n      size = axis_label_size,\n      color = no_highlight_col\n    ),\n    axis.ticks.x = element_line(color = no_highlight_col, size = 0.25),\n    axis.text.y = element_blank(),\n    axis.line.x = element_blank()\n  )\np\n\n\n\n\n\n\nAdd y-axis labels\n\ny_axis_text_size <- 3\np +\n  geom_text(\n    aes(\n      x = 0,\n      y = num,\n      label = location,\n      col = no_highlight_col,\n      hjust = 1,\n      size = y_axis_text_size\n    )\n  )\n\n\n\n\n\n\nHighlight words\nLet us turn to text highlights. For that we will need ggtext. This will let us use geom_richtext() instead of geom_text(). Notice that I have note saved the last geom_text() modification in p. Otherwise, we would get two overlapping layers of text. You can highlight single words as demonstrated in my blog post about effective use of colors.\n\nlibrary(ggtext)\nsorted_dat_with_new_labels <- sorted_dat %>% \n  mutate(location_label = case_when(\n    inventory_turnover < store_lower ~ glue::glue(\n      '<span style = \"color:{below_highlight}\">**{location}**</span>'\n    ),\n    location == 'REGION AVERAGE' ~ glue::glue(\n      '<span style = \"color:{average_highlight_col}\">**{location}**</span>'\n    ),\n    T ~ location\n  ))\n\np <- p +\n  geom_richtext(\n    data = sorted_dat_with_new_labels,\n    aes(\n      x = 0,\n      y = num,\n      label = location_label,\n      col = no_highlight_col,\n      hjust = 1,\n      size = y_axis_text_size\n    ),\n    label.colour = NA,\n    fill = NA\n  )\np\n\n\n\n\nFantastic! Next, we only have to highlight words in our call to action. Make sure that plot.title in theme() is an element_markdown().\n\ntitle_lab_adjusted <- glue::glue(\n  \"Review stores with **turnover ratios** that are <span style = 'color:{below_highlight}'>below their</span><br><span style = 'color:#7fceb9;'>**forecasted range**</span>\"\n)\n\np +\n  labs(title = title_lab_adjusted) +\n  theme(\n    plot.title = element_markdown(),\n    panel.background = element_rect(color = NA, fill = 'white')\n  )\n\n\n\n\nThere you go. This concludes the easy way to draw rounded rectangles with ggplot2 and ggchicklet. Now, I am well aware that this is probably the moment when many readers will drop out. So, let me do some shameless self-promotion before everyone’s gone.\nIf you enjoyed this post, follow me on Twitter and/or subscribe to my RSS feed. For reaching out to me, feel free to hit the comments or send me a mail. I am always happy to see people commenting on my work."
  },
  {
    "objectID": "posts/ggplot2-tips/11_rounded_rectangles/11_rounded_rectangles.html#rounded-rectangles-with-grobs",
    "href": "posts/ggplot2-tips/11_rounded_rectangles/11_rounded_rectangles.html#rounded-rectangles-with-grobs",
    "title": "Storytelling in ggplot using rounded rectangles",
    "section": "Rounded rectangles with grobs",
    "text": "Rounded rectangles with grobs\nAlright, this is where the hacking begins. In this last part of the blog post, I will show you to how transform rectangles to rounded rectangles. In principle, you could then create our SWD plot using geom_rect() and transform the rectangles afterwards. No additional package needed.\n\nSimple example with one bar\nLet me demonstrate a quick hack when there is only one bar in the plot. Unfortunately, this does not work with more than one bar. Still, this should get you acquainted with grobs. First, create a simple dummy plot.\n\nlibrary(tidyverse)\np <- mpg %>%\n  filter(year == 2008) %>% \n  ggplot(aes(year)) +\n  geom_bar() +\n  theme_minimal()\np\n\n\n\n\nNext, we can turn this plot into a so-called TableGrob. From what I understand, it is a highly nested list that contains all the graphical objects (grobs) that are part of our plot p.\n\nl <- ggplotGrob(p)\nl\n\nTableGrob (12 x 9) \"layout\": 18 grobs\n    z         cells       name                                          grob\n1   0 ( 1-12, 1- 9) background       zeroGrob[plot.background..zeroGrob.831]\n2   5 ( 6- 6, 4- 4)     spacer                                zeroGrob[NULL]\n3   7 ( 7- 7, 4- 4)     axis-l           absoluteGrob[GRID.absoluteGrob.820]\n4   3 ( 8- 8, 4- 4)     spacer                                zeroGrob[NULL]\n5   6 ( 6- 6, 5- 5)     axis-t                                zeroGrob[NULL]\n6   1 ( 7- 7, 5- 5)      panel                      gTree[panel-1.gTree.814]\n7   9 ( 8- 8, 5- 5)     axis-b           absoluteGrob[GRID.absoluteGrob.817]\n8   4 ( 6- 6, 6- 6)     spacer                                zeroGrob[NULL]\n9   8 ( 7- 7, 6- 6)     axis-r                                zeroGrob[NULL]\n10  2 ( 8- 8, 6- 6)     spacer                                zeroGrob[NULL]\n11 10 ( 5- 5, 5- 5)     xlab-t                                zeroGrob[NULL]\n12 11 ( 9- 9, 5- 5)     xlab-b titleGrob[axis.title.x.bottom..titleGrob.823]\n13 12 ( 7- 7, 3- 3)     ylab-l   titleGrob[axis.title.y.left..titleGrob.826]\n14 13 ( 7- 7, 7- 7)     ylab-r                                zeroGrob[NULL]\n15 14 ( 4- 4, 5- 5)   subtitle         zeroGrob[plot.subtitle..zeroGrob.828]\n16 15 ( 3- 3, 5- 5)      title            zeroGrob[plot.title..zeroGrob.827]\n17 16 (10-10, 5- 5)    caption          zeroGrob[plot.caption..zeroGrob.830]\n18 17 ( 2- 2, 2- 2)        tag              zeroGrob[plot.tag..zeroGrob.829]\n\n\nIn this case, calling l gave us an overview of plot parts. We will want to change stuff in the panel. Thus, let us extract the grobs from the sixth list entry of l. As we have seen in the table, this will give us a gTree. That’s another nested list. And it contains an interesting sublist called children. That’s where the grobs of this gTree are stored.\n\ngrobs <- l$grobs[[6]]\ngrobs$children\n\n(gTree[grill.gTree.812], zeroGrob[NULL], rect[geom_rect.rect.800], zeroGrob[NULL], zeroGrob[panel.border..zeroGrob.801]) \n\n\nHere, the rect grob is what we want to access. Thus, let us take a look what we can find there.\n\n# str() helps us to unmask the complicated list structure\ngrobs$children[[3]] %>% str()\n\nList of 10\n $ x     : 'simpleUnit' num 0.0455native\n  ..- attr(*, \"unit\")= int 4\n $ y     : 'simpleUnit' num 0.955native\n  ..- attr(*, \"unit\")= int 4\n $ width : 'simpleUnit' num 0.909native\n  ..- attr(*, \"unit\")= int 4\n $ height: 'simpleUnit' num 0.909native\n  ..- attr(*, \"unit\")= int 4\n $ just  : chr [1:2] \"left\" \"top\"\n $ hjust : NULL\n $ vjust : NULL\n $ name  : chr \"geom_rect.rect.800\"\n $ gp    :List of 6\n  ..$ col     : logi NA\n  ..$ fill    : chr \"#595959FF\"\n  ..$ lwd     : num 1.42\n  ..$ lty     : num 1\n  ..$ linejoin: chr \"mitre\"\n  ..$ lineend : chr \"square\"\n  ..- attr(*, \"class\")= chr \"gpar\"\n $ vp    : NULL\n - attr(*, \"class\")= chr [1:3] \"rect\" \"grob\" \"gDesc\"\n\n\nThis is a grob. It can be build using grid::rectGrob(). Basically, what you see here is a specification of everything from x and y position to graphical properties (gp) of this rectangular grob.\nThere is also a function grid::roundrectGrob(). As you may have guessed, it builds the rounded rectangle grobs that we so desperately crave. From grid::roundrectGrob()’s documentation, we know that we will have to specify another variable r to determine the radius of the rounded rectangles. So, here’s what we could do now.\n\nExtract x, y, gp and so on from grobs$children[[3]].\nAdd another argument r.\nPass all of these arguments to grid::roundrectGrob()\nExchange grobs$children[[3]] with our newly built roundrectGrob\n\nThis is what we will have to do at some point. But in this simple plot, I want to show you a different hack. Did you notice the class attributes of grobs$children[[3]]? Somewhere in there it says - attr(*, \"class\")= chr [1:3] \"rect\" \"grob\" \"gDesc\". And we can access and change that information through attr().\n\nattr(grobs$children[3][[1]], 'class')\n\n[1] \"rect\"  \"grob\"  \"gDesc\"\n\n\nNow, a really basic hack is to\n\nchange the class attribute from rect to roundrect.\nstick another argument r into the list\nput everything back together as if nothing happened\n\n\n## Change class attribute of grobs$children[3][[1]] from rect to roundrect\ncurrent_attr <-  attr(grobs$children[3][[1]], 'class')\nnew_attr <- str_replace(current_attr, 'rect', 'roundrect')\nattr(grobs$children[3][[1]], 'class') <- new_attr\n\n# Add r argument for grid::roundrectGrob()\n# We need to add a \"unit\"\n# Here I use the relative unit npc\ngrobs$children[3][[1]]$r <- unit(0.5, 'npc')\n\n# Copy original list and change grobs in place\nl_new <- l\nl_new$grobs[[6]] <- grobs\n\n# Draw grobs via grid::grid.draw()\ngrid::grid.newpage()\ngrid::grid.draw(l_new)\n\n\n\n\n\n\nDealing with multiple bars\nThe previous hack works if we plot only one bar. However, if there are multiple x arguments, then grid::roundrectGrob() will error. It seems like that function is not vectorized. So, we will build the rounded rectangles ourselves with functional programming. First let’s take a look at the plot that we want to modify.\n\np <- mpg %>%\n  ggplot(aes(class, fill = class)) +\n  geom_bar() +\n  theme_minimal()\np\n\n\n\n\nNow, let’s find out what arguments grid::roundrectGrob() accepts and extract as many of these from grobs$children[3] as possible.\n\nl <- ggplotGrob(p)\ngrobs <- l$grobs[[6]]\n\n# What arguments does roundrectGrob need?\narg_names <- args(grid::roundrectGrob) %>% as.list() %>% names()\n# Somehow last one is NULL\narg_names <- arg_names[-length(arg_names)]\narg_names\n\n [1] \"x\"             \"y\"             \"width\"         \"height\"       \n [5] \"default.units\" \"r\"             \"just\"          \"name\"         \n [9] \"gp\"            \"vp\"           \n\n# Extract the arguments roundrectGrob needs from grobs$children[3]\nextracted_args <- map(arg_names, ~pluck(grobs$children[3], 1, .)) \nnames(extracted_args) <- arg_names\nextracted_args %>% str()\n\nList of 10\n $ x            : 'simpleUnit' num [1:7] 0.0208native 0.16native 0.299native 0.438native 0.576native ...\n  ..- attr(*, \"unit\")= int 4\n $ y            : 'simpleUnit' num [1:7] 0.119native 0.735native 0.647native 0.207native 0.529native ...\n  ..- attr(*, \"unit\")= int 4\n $ width        : 'simpleUnit' num [1:7] 0.125native 0.125native 0.125native 0.125native 0.125native 0.125native 0.125native\n  ..- attr(*, \"unit\")= int 4\n $ height       : 'simpleUnit' num [1:7] 0.0733native 0.689native 0.601native 0.161native 0.484native ...\n  ..- attr(*, \"unit\")= int 4\n $ default.units: NULL\n $ r            : NULL\n $ just         : chr [1:2] \"left\" \"top\"\n $ name         : chr \"geom_rect.rect.911\"\n $ gp           :List of 6\n  ..$ col     : logi [1:7] NA NA NA NA NA NA ...\n  ..$ fill    : chr [1:7] \"#F8766D\" \"#C49A00\" \"#53B400\" \"#00C094\" ...\n  ..$ lwd     : num [1:7] 1.42 1.42 1.42 1.42 1.42 ...\n  ..$ lty     : num [1:7] 1 1 1 1 1 1 1\n  ..$ linejoin: chr \"mitre\"\n  ..$ lineend : chr \"square\"\n  ..- attr(*, \"class\")= chr \"gpar\"\n $ vp           : NULL\n\n\nAs you can can see, in our vector extracted_args the components x, y and so on are vectors of length 7 (since we have 7 bars in p). As I said before, this works because it is a rectGrob. But, with a roundrectGrob this would cause errors.\nNext, let us make sure that we know how many rectangles we need to change. Also, we will need to specify the radius r, and the graphical parameters gp should always have the same amount of arguments.\n\n## How many rectangles are there?\nn_rects <- extracted_args$x %>% length()\n\n## Add radius r\nextracted_args$r <- unit(rep(0.25, n_rects), 'npc')\n\n## Make sure that all list components in gp have equally many values\nextracted_args$gp$linejoin <- rep(extracted_args$gp$linejoin, n_rects)\nextracted_args$gp$lineend <- rep(extracted_args$gp$lineend, n_rects)\n\nNow comes the tedious part. We have to split up extracted_args into multiple nested lists. Unfortunately, the purrr package does not provide a function that works the way we want. That’s because we need many custom steps here. For instance, for the columns x and y we have to always extract a single value out of extracted_args. But with the columns just and name we need to extract the whole vector. Also, we have to adjust the names to ensure that they are unique.\nIn this blog post, we will get the tedious stuff out of the way with the following helper functions. Feel free to ignore them, if you only care about the general idea.\n\n## Write function that does splitting for each rectangle\n## Found no suitable purrr function that works in my case\nextract_value <- function(list, arg, rect) {\n  x <- list[[arg]]\n  # name and just need do be treated different\n  # In all cases just pick the i-th entry of list[[arg]]\n  if (!(arg %in% c('name', 'just'))) return(x[rect])\n  \n  ## There is only one name, so extract that and modify id\n  if (arg == 'name') {\n    return(paste0(x,  rect))\n  }\n  \n  # 'just' is two part vector and should always be the same\n  if (arg == 'just') return(x)\n}\n\nsplit_my_list <- function(list, n_rects) {\n  combinations <- tibble(\n    rect = 1:n_rects,\n    arg = list(names(list))\n  ) %>% \n    unnest(cols = c(arg)) \n  \n  flattened_list <- combinations %>% \n    pmap(~extract_value(list, ..2, ..1))\n  \n  names(flattened_list) <- combinations$arg\n  split(flattened_list, combinations$rect)\n}\n\nFinally, we can split extracted_args into sub-lists. Each of these is then used to call grid::roundrectGrob() with do.call(). Then, we have to replace the same part in our list grobs as we did before. However, since we have multiple grobs now that need to be put into a single location. Therefore, we have to bundle the grobs into one object. This is done via grid::grobTree() and do.call().\n\nlist_of_arglists <-  split_my_list(extracted_args, n_rects)\nlist_of_grobs <- map(list_of_arglists, ~do.call(grid::roundrectGrob, .)) \n\n# Build new list of grobs by replacing one part in old list\ngrobs_new <- grobs\n\n# save one list argument into children[3]\ngrobs_new$children[3] <- do.call(grid::grobTree, list_of_grobs) %>% list()\nl_new <- l\nl_new$grobs[[6]] <- grobs_new\n\n# Draw Plot\ngrid::grid.newpage()\ngrid::grid.draw(l_new)"
  },
  {
    "objectID": "posts/ggplot2-tips/11_rounded_rectangles/11_rounded_rectangles.html#conclusion",
    "href": "posts/ggplot2-tips/11_rounded_rectangles/11_rounded_rectangles.html#conclusion",
    "title": "Storytelling in ggplot using rounded rectangles",
    "section": "Conclusion",
    "text": "Conclusion\nWoooow! Marvel at our glorious rounded rectangles! Thanks to our excellent programming skills we made it through the grob jungle. In practice, it is probably easier to use geom_chicklet(). But still, this was a somewhat fun exercise and helped to demystify grobs (at least to some extend).\nThat’s it for today. If you’ve made it this far, then you already know that you should follow me on Twitter and/or subscribe to my RSS feed. So, I expect you to be here next time. There’s no way out anymore. So long!"
  },
  {
    "objectID": "posts/ggplot2-tips/02_aesthetic_labels/02_aesthetic_labels.html",
    "href": "posts/ggplot2-tips/02_aesthetic_labels/02_aesthetic_labels.html",
    "title": "ggplot tips: Assigning Labels to an Aesthetic",
    "section": "",
    "text": "This blog post is part of a series I am creating where I collect tips I found useful when I first learned to work with ggplot2. All posts which are part of this series can be found here. In this post I want to deal with how to manually or automatically create labels for some aesthetic."
  },
  {
    "objectID": "posts/ggplot2-tips/02_aesthetic_labels/02_aesthetic_labels.html#manually-assigning-labels",
    "href": "posts/ggplot2-tips/02_aesthetic_labels/02_aesthetic_labels.html#manually-assigning-labels",
    "title": "ggplot tips: Assigning Labels to an Aesthetic",
    "section": "Manually Assigning Labels",
    "text": "Manually Assigning Labels\nAssigning labels by hand, e.g. via col = \"some label\", can be a great idea in some instances. For example, when you use two different smoothing methods, a hand-written label to differentiate between the two methods helps a lot. For instance, take a look the relationship between city mileage cty and highway mileage hwy of cars in the mpg data set from the ggplot2 package.\n\nlibrary(tidyverse)\nggplot(data = mpg, aes(hwy, cty)) +\n  geom_jitter(alpha = 0.5)\n\n\n\n\nIf one suspects a linear relationship between those two variables, one might want to use geom_smooth(method = 'lm') to check that hypothesis by drawing a straight line through the points. Similarly, one may be inclined to see what geom_smooth() would return if a linear model is not enforced. Adding both smoothing methods to the plot (and removing the confidence bands) yields:\n\nggplot(data = mpg, aes(hwy, cty)) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(se = F, size = 1.5) +\n  geom_smooth(method = 'lm', se = F, size = 1.5)\n\n\n\n\nObviously, differently colored lines should be used here to differentiate between the two smoothing methods. We have two approaches to do this. Either, we can manually assign a color (without using aes()):\n\nggplot(data = mpg, aes(hwy, cty)) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(se = F, size = 1.5, col = 'red') +\n  geom_smooth(method = 'lm', se = F, size = 1.5, col = 'blue')\n\n\n\n\nOr we can use aes() and assign labels instead and let ggplot2 handle the colors on its own.\n\nggplot(data = mpg, aes(hwy, cty)) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(aes(col = 'auto'), se = F, size = 1.5) +\n  geom_smooth(method = 'lm', aes(col = 'lm'), se = F, size = 1.5) +\n  labs(col = 'Smoothing')\n\n\n\n\nPersonally, I prefer the latter approach because it has a couple of small advantages\n\nA legend is automatically generated with the corresponding labels such that even without looking at the code it becomes more obvious how each line was generated. Also, creating labels for an aesthetic is kind of the point of this post.\nI do not have to bother about the specific color names. For me, this is something that could take up a lot of time if I want to change the appearance of the plot later on because I might spend way too much time on finding colors that “work” together. Here, if I want to change the colors, I could simply use a Brewer color palette and hope that the creators of that palette had good reasons to arrange the palette the way they did.\n\n\nggplot(data = mpg, aes(hwy, cty)) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(aes(col = 'auto'), se = F, size = 1.5) +\n  geom_smooth(method = 'lm', aes(col = 'lm'), se = F, size = 1.5) +\n  labs(col = 'Smoothing') +\n  scale_color_brewer(palette = 'Set1')"
  },
  {
    "objectID": "posts/ggplot2-tips/02_aesthetic_labels/02_aesthetic_labels.html#automatically-assigning-labels-via-pivoting",
    "href": "posts/ggplot2-tips/02_aesthetic_labels/02_aesthetic_labels.html#automatically-assigning-labels-via-pivoting",
    "title": "ggplot tips: Assigning Labels to an Aesthetic",
    "section": "Automatically Assigning Labels via Pivoting",
    "text": "Automatically Assigning Labels via Pivoting\nSometimes, manually coloring aspects of your data can also be a bad idea. Especially, if you find yourself using the exact same geom_* multiple times on different variables of a data set, you may want to think about using a different approach. One such approach can be to rearrange the data first. For example, take a look at the following two time series that were simulated and collected in a tibble as follows:\n\nset.seed(123)\nx1 <- rnorm(10)\nx2 <- rnorm(10)\ntib <- tibble(\n  t = seq_along(x1),\n  ts1 = cumsum(x1),\n  ts2 = cumsum(x2)\n)\ntib\n\n# A tibble: 10 × 3\n       t    ts1   ts2\n   <int>  <dbl> <dbl>\n 1     1 -0.560  1.22\n 2     2 -0.791  1.58\n 3     3  0.768  1.98\n 4     4  0.839  2.10\n 5     5  0.968  1.54\n 6     6  2.68   3.33\n 7     7  3.14   3.82\n 8     8  1.88   1.86\n 9     9  1.19   2.56\n10    10  0.746  2.09\n\n\nNow, it is possible to plot both times series using geom_line() and use different colors for each line. To do so, one might be tempted (as I often was when I first learned ggplot2) to write code similar to the one we wrote earlier:\n\ntib %>% \n  ggplot(aes(x = t)) +\n  geom_line(aes(y = ts1, col = \"Time series 1\")) +\n  geom_line(aes(y = ts2, col = \"Time series 2\"))\n\n\n\n\nHere, we basically used geom_line() twice for more or less the same plot but with only one aesthetic slightly changed. However, this may not be the best approach. This is especially true if we were to do this for, say, 100 time series as it would involve a lot of code duplication.\nInstead, let’s try to rearrange the data via pivot_longer() before even beginning to plot anything1. This way, we might even plot way more than 2 time series with only a single geom_line():\n\nset.seed(123)\n# Create multiple time series\ntib <- map_dfc(1:6, ~cumsum(rnorm(10))) %>% \n  rename_with(~glue::glue(\"label{1:6}\")) %>% \n  bind_cols(t = 1:10, .)\n\n# Pivot and plot\ntib %>% \n  pivot_longer(cols = -1, names_to = \"ts\") %>% \n  ggplot(aes(t, value, col = ts)) +\n  geom_line()\n\n\n\n\nAs you just saw, it is also possible to, if necessary, relabel the column names in bulk before rearranging the data in order to label the aesthetic the way we want."
  },
  {
    "objectID": "posts/ggplot2-tips/02_aesthetic_labels/02_aesthetic_labels.html#same-procedure-different-aesthetic",
    "href": "posts/ggplot2-tips/02_aesthetic_labels/02_aesthetic_labels.html#same-procedure-different-aesthetic",
    "title": "ggplot tips: Assigning Labels to an Aesthetic",
    "section": "Same Procedure, Different Aesthetic",
    "text": "Same Procedure, Different Aesthetic\nFor the sake of an additional example, let us use the same ideas but with geom_boxplot() instead of geom_line(). Therefore, we will generate a couple of “data sets” and plot a box plot for each one:\n\nset.seed(123)\nmap_dfc(1:6, rnorm, n = 100) %>% \n  rename_with(~(1:6)) %>%\n  pivot_longer(cols = everything(), names_to = \"ds\") %>% \n  ggplot(aes(col = ds, y = value)) +\n  geom_boxplot()\n\n\n\n\nHere, I have used col again but as I have recently come to realize, using fill instead of col creates the “prettier” box plots so let’s use that instead.\n\nset.seed(123)\nmap_dfc(1:6, rnorm, n = 100) %>% \n  rename_with(~(1:6)) %>%\n  pivot_longer(cols = everything(), names_to = \"ds\") %>% \n  ggplot(aes(fill = ds, y = value)) +\n  geom_boxplot()\n\n\n\n\nSo, as you just witnessed, what I have described so far does not only work with the color aesthetic. In fact, we can pretty much use the same approaches for all other aesthetics.\nThus, we have seen how to easily create labels for an aesthetic of our choice by either manually assigning labels or rearranging the data first in order to use the previous column names to assign labels automatically. Let me know what you think in the comments or if you liked this post, simply hit the applause button below."
  },
  {
    "objectID": "posts/ggplot2-tips/09_functional_programming_ggplot/09_functional_programming_ggplot.html",
    "href": "posts/ggplot2-tips/09_functional_programming_ggplot/09_functional_programming_ggplot.html",
    "title": "How to use functional programming for ggplot",
    "section": "",
    "text": "A couple of weeks back, I recreated an info graphic with ggplot2. The result and the whole story is embedded in this thread on Twitter:\nAside from the embarrasing typo in “What you should know…”, I picked up a useful technique for what do when I want aesthetics to vary within a geom. Sounds complicated? Let’s take a look at a couple of examples."
  },
  {
    "objectID": "posts/ggplot2-tips/09_functional_programming_ggplot/09_functional_programming_ggplot.html#how-do-i-manually-set-aesthetics-with-aes-and-scale__identity",
    "href": "posts/ggplot2-tips/09_functional_programming_ggplot/09_functional_programming_ggplot.html#how-do-i-manually-set-aesthetics-with-aes-and-scale__identity",
    "title": "How to use functional programming for ggplot",
    "section": "How do I manually set aesthetics with aes() and scale_*_identity()?",
    "text": "How do I manually set aesthetics with aes() and scale_*_identity()?\nThis one is the easy case when all geoms behave properly.\n\nlibrary(tidyverse)\ntheme_set(theme_minimal())\ntib <- tribble(\n  ~x, ~xend, ~y, ~yend, ~size_col,\n  0, 1, 0, 1, 1,\n  1, 2, 1, 1, 5\n)\n\ntib %>% \n  ggplot(aes(x = x, xend = xend, y = y, yend = yend, size = size_col)) +\n  geom_segment(col = 'dodgerblue4') +\n  scale_size_identity()\n\n\n\n\nNotice that\n\nthe sizes were determined in the size_col column of tib.\nsizes were mapped to the aesthethic via aes().\nthe scale_size_identity() layer makes sure that the sizes are not assigned by ggplot but taken as given (identity scale layers are available for other aesthetics as well)."
  },
  {
    "objectID": "posts/ggplot2-tips/09_functional_programming_ggplot/09_functional_programming_ggplot.html#how-do-i-manually-set-aesthetics-without-aes",
    "href": "posts/ggplot2-tips/09_functional_programming_ggplot/09_functional_programming_ggplot.html#how-do-i-manually-set-aesthetics-without-aes",
    "title": "How to use functional programming for ggplot",
    "section": "How do I manually set aesthetics without aes()?",
    "text": "How do I manually set aesthetics without aes()?\nThe last example used aes() to access size_col from tib. However, we then had to make sure that ggplot does not assign sizes based on unique values in size_col. Instead, sizes were supposed to be taken as is. This was the job of scale_size_identity(). Let’s make it work without it.\n\ntib %>% \n  ggplot(aes(x = x, xend = xend, y = y, yend = yend)) +\n  geom_segment(col = 'dodgerblue4', size = tib$size_col) \n\nThis will generate the exact same plot as before (which is why I suppressed the output). In this case, we mapped the sizes manually by assigning a vector of sizes to the size aesthetic within geom_segment() but outside aes().\nOf course, now we cannot simply write size = size_col because geom_segment() won’t know that variable. Before, aes() let ggplot know that we mean size_col from the data set tib. Now, we have to pass the vector by accessing it from tib ourself through tib$size_col."
  },
  {
    "objectID": "posts/ggplot2-tips/09_functional_programming_ggplot/09_functional_programming_ggplot.html#how-do-i-manually-set-aesthethics-when-the-previous-approaches-do-not-work",
    "href": "posts/ggplot2-tips/09_functional_programming_ggplot/09_functional_programming_ggplot.html#how-do-i-manually-set-aesthethics-when-the-previous-approaches-do-not-work",
    "title": "How to use functional programming for ggplot",
    "section": "How do I manually set aesthethics when the previous approaches do not work?",
    "text": "How do I manually set aesthethics when the previous approaches do not work?\nFinally, let’s switch from geom_segment() to geom_curve().\n\ntib %>% \n  ggplot(aes(x = x, xend = xend, y = y, yend = yend)) +\n  geom_curve(col = 'dodgerblue4', size = tib$size_col, curvature = 0.6) \n\n\n\n\nThis changes our straight lines from before to curved lines. What’s more, I can control how strong the curvature is supposed to be via curvature. But as it is right now, both of our differently-sized curves have the same level of curvature.\nMaybe, this ought to be different. Maybe, not all curves are made the same. Maybe, our visualization should reflect the diversity of all the curves out there in this gigantic world we inhabit. All curves are beautiful!\nLet’s make this happen as we did before.\n\ntib %>% \n  ggplot(aes(x = x, xend = xend, y = y, yend = yend)) +\n  geom_curve(\n    col = 'dodgerblue4', \n    size = tib$size_col, \n    curvature = c(-0.3, 0.6) # two curves, two different curvatures\n  ) \n\nError in if (curvature == 0) {: the condition has length > 1\n\n\n\n\n\nOh no! It seems as if geom_curve() expects the argument of curvature to be a single number. Maybe aes() then?\n\ntib %>% \n  ggplot(aes(x = x, xend = xend, y = y, yend = yend)) +\n  geom_curve(\n    aes(curvature = c(-0.3, 0.6)),\n    col = 'dodgerblue4', \n    size = tib$size_col \n  ) \n\n\n\n\nWell, at least this time we can see curves. Unfortunately, the warning let’s us know that curvature is an unknown aesthetic which will be ignored. As you can see, this results in the same curvature for both curves again.\nSo, it looks like we can only hope to set each curvature separately.\n\nggplot(mapping = aes(x = x, xend = xend, y = y, yend = yend)) +\n  geom_curve(\n    data = slice(tib, 1), # first row of tib\n    col = 'dodgerblue4', \n    size = tib$size_col[1], # one size only\n    curvature = -0.3\n  ) +\n  geom_curve(\n    data = slice(tib, 2), # second row of tib\n    col = 'dodgerblue4', \n    size = tib$size_col[2], # other size\n    curvature = 0.6\n  ) \n\n\n\n\nAlright, this time we got what we wanted. That’s something at least. Honestly, our “solution” is not scalable though. What if we want to draw hundreds of curves?\nIn fact, this is what slowed me down when I created the info graphic that started this blog post. The text boxes were not vectorized so I would have to place each text box manually. That’s a lot of text boxes and I was having none of that.\nSo, here is where functional programming stepped in. Let’s recreate what I did based on our curve example. First, we extend tib with another curvature column.\n\ntib <- tib %>% mutate(curvature = c(-0.3, 0.6))\ntib\n\n# A tibble: 2 × 6\n      x  xend     y  yend size_col curvature\n  <dbl> <dbl> <dbl> <dbl>    <dbl>     <dbl>\n1     0     1     0     1        1      -0.3\n2     1     2     1     1        5       0.6\n\n\nThen, we use pmap() to create a list of curve layers. If you have not used any functional programming before, checkout my YARDS lecture notes on that topic. Basically, what we will do is to apply the geom_curve() function to each row of the tib data. Via ~ (in front of the function) and ..1, ..2, etc. we can then say where to stick in the values from each of tib’s columns.\n\ncurve_layers <- tib %>% \n  pmap(~geom_curve(\n    mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4),\n    size = ..5,\n    curvature = ..6,\n    col = 'dodgerblue4'\n  ))\ncurve_layers\n\n[[1]]\nmapping: x = 0, y = 0, xend = 1, yend = 1 \ngeom_curve: arrow = NULL, arrow.fill = NULL, curvature = -0.3, angle = 90, ncp = 5, lineend = butt, na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\n[[2]]\nmapping: x = 1, y = 1, xend = 2, yend = 1 \ngeom_curve: arrow = NULL, arrow.fill = NULL, curvature = 0.6, angle = 90, ncp = 5, lineend = butt, na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\n\nHere, we have set the first column of tib (x) to the x-aesthetic within aes. Then, we proceeded similarly for all other columns. This resulted in a list of curve layers.\nThese are useless without a ggplot() head. So, let’s complete the plot.\n\nggplot() +\n  curve_layers \n\n\n\n\nDamn, these are some nice functionally created curves. Now, let’s put our new technique to a test. Can it handle arbitrarily many curves?\n\nn_curves <- 50\ncurve_layers <- tibble(\n    x = runif(n_curves),\n    xend = runif(n_curves),\n    y = runif(n_curves),\n    yend = runif(n_curves),\n    size = runif(n_curves, 0, 2), \n    curvature = runif(n_curves, -1, 1)\n  ) %>% \n  pmap(~geom_curve(\n    mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4),\n    size = ..5,\n    curvature = ..6,\n    col = 'dodgerblue4'\n  ))\nggplot() + curve_layers\n\n\n\n\nCongratulations! We have successfully created drawings of a toddler. And the even better news is that we can draw as many curves as we want.\nSurprisingly, before I started this blog post, I was not aware that you can simply add lists to ggplot() and it works. As you will see in the Twitter thread on top of this post, I initially thought that one had to combine the list with more functional programming like so.\n\ncombine_gg_elements <- function(...) {\n  Reduce(`+`, list(...))\n}\ncombine_gg_elements(ggplot(), curve_layers)\n\n\n\n\nThis was something I picked up from Hadley Wickham’s ggplot2 book but it seems that we don’t need that anymore (the combine function, the book is still a great ressource). But I leave this here for completeness’ sake. Once again, writing a blog post has taught me stuff I thought I already knew. If you want to watch me learn more stuff or want to learn more ggplot things yourself, feel free to subscribe to my RSS feed or follow me on Twitter."
  },
  {
    "objectID": "posts/ggplot2-tips/04_arranging_plots/04_arranging_plots.html",
    "href": "posts/ggplot2-tips/04_arranging_plots/04_arranging_plots.html",
    "title": "ggplot tips: Arranging plots",
    "section": "",
    "text": "In this week’s TidyTuesday, I noticed that I am frequently not using only ggplot2 to create plots. In fact, it has become essential to me to leverage the powers of other great additional packages that align well with ggplot2. Therefore, I decided to extend my ggplot2-tips series by introducing a few packages I use quite often.\nIn this post, I want to cover how to arrange multiple plots. In particular, I will talk about the fantastic patchwork package by Thomas Lin Pedersen which helps to arrange plots quite intuitively. Further, I want to take a glance at ggforce, another package written by the same author as patchwork, because it also has a neat function for arranging plots. However, ggforce can do way more and I will demonstrate that in another installment of this series. Also, if you like to watch and listen rather than read about how the two packages work, you can check out the corresponding video on YouTube.\nSo, let us begin by creating a data set we want to fiddle with for plotting purposes. For simplicity, let us use the penguins data (without missing values) from the palmerpenguins package."
  },
  {
    "objectID": "posts/ggplot2-tips/04_arranging_plots/04_arranging_plots.html#arrange-plots-via-patchwork",
    "href": "posts/ggplot2-tips/04_arranging_plots/04_arranging_plots.html#arrange-plots-via-patchwork",
    "title": "ggplot tips: Arranging plots",
    "section": "Arrange Plots via patchwork",
    "text": "Arrange Plots via patchwork\nOften, we want to show multiple plots that tell a story when looked at together. Using patchwork, we can easily compose a single plot consisting of subplots. This is done by using the simple symbols + resp. / to display plots next to resp. on top of each other.\nFor demonstration purposes, let us generate a few simple plots.\n\npoint_plot <- dat %>% \n  ggplot(aes(bill_length_mm, flipper_length_mm, fill = sex)) +\n  geom_jitter(size = 3, alpha = 0.5, shape = 21)\npoint_plot\n\n\n\npoint_plot2 <- dat %>% \n  ggplot(aes(bill_length_mm, bill_depth_mm, fill = sex)) +\n  geom_jitter(size = 3, alpha = 0.5, shape = 21)\npoint_plot2\n\n\n\n# plot_plot is obviously a fun name\nboxplot_plot <- dat %>% \n  ggplot(aes(x = body_mass_g, fill = sex)) +\n  geom_boxplot()\nboxplot_plot\n\n\n\n\nClearly, showing each plot separately is boring and may not tell a story convincingly. Possibly, here you may want to say that the length and depth measurements give no clear distinction between male and female penguins but the weight measurements offers a better distinguishabilty between sexes. Maybe, if we see all plots together, we can tell that story without boring the reader.\n\nlibrary(patchwork)\np <- (point_plot + point_plot2) / boxplot_plot\np\n\n\n\n\nSee how I have used + to put the point plots next to each other and / to plot the boxplots below the two point plots. Obviously, that was super easy and neat. But this simple arrangement leads to a doubling of the legends which is somewhat bothersome. However, this is no cause for concern. plot_layout() is there to collect those legends for you.\n\np + plot_layout(guides = \"collect\") \n\n\n\n\nOf course, this leaves you with two legends which is kind of superfluous. The easy way to get rid of this is to plot no legends for the boxplots.\n\nboxplot_plot <- boxplot_plot + guides(fill = \"none\")\np <- (point_plot + point_plot2) / boxplot_plot\np + plot_layout(guides = \"collect\")\n\n\n\n\nNow, what about legend positioning? Well, we already know how that usually works for a single plot (via theme() in case you forgot) and the good news is that the exact same thing works with patchwork as well. But beware to apply an additional theme() layer to the whole plot and not just to the last plot added to our composed plot. To make sure that happens, we have to add this layer via &.\n\np + plot_layout(guides = \"collect\") & theme(legend.position = \"top\")\n\n\n\n\nBy the same logic, we can make additional changes to the whole plot e.g. to change the color mapping.\n\np +\n  plot_layout(guides = \"collect\") & \n  theme(legend.position = \"top\") &\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\nNext, let us control the layout a bit more and annotate the plot with plot_annotation().\n\n(point_plot + point_plot2 + plot_layout(widths = c(0.7, 0.3))) / \n  boxplot_plot +\n  plot_layout(guides = \"collect\", heights = c(0.4, 0.6)) +\n  plot_annotation(\n    title = \"Look at that arrangement!\",\n    subtitle = \"Wow\",\n    caption = \"Olà.\",\n    tag_levels = \"A\",\n    tag_prefix = \"(\",\n    tag_suffix = \")\"\n  ) & \n  labs(fill = \"Penguin sex\") &\n  theme(legend.position = \"top\") &\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\nWe did quite a lot here, so let’s recap:\n\nWe changed the widths of the plots in the first row by passing a vector of relative widths to widths in plot_layout().\nSame thing with heights in plot_layout() to make the boxplots larger.\nRenamed legend label with the regular labs() function.\nAdded a title, subtitle, caption and tags to the whole plot with plot_annotation().\n\nAlso, if you want to have the tags to only label the upper and lower row, you may want to wrap the first row together by wrap_elements(). Think of this as creating a new single unit.\n\nwrapped_plots <- wrap_elements(\n  point_plot + point_plot2 + plot_layout(widths = c(0.7, 0.3))\n)\n(wrapped_plots) / \n  boxplot_plot +\n  plot_layout(guides = \"collect\", heights = c(0.4, 0.6)) +\n  plot_annotation(\n    title = \"Look at that arrangement!\",\n    subtitle = \"Wow\",\n    caption = \"Olà.\",\n    tag_levels = \"A\",\n    tag_prefix = \"(\",\n    tag_suffix = \")\"\n  ) & \n  theme(legend.position = \"top\") &\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\nNotice how the upper row reinstated the default colors and has two legends. This demonstrates how wrap_elements() made the plots “independent” from the overall theming via &, so to speak. On the bright side, there is no (C) tag anymore.\nUnsurprisingly, patchwork can do much more but for starters I think the previous examples will already get you quite far. They are you “80/20 leverage points”, if you will. But in order to add one more neat feature, let me finish our intro to patchwork by showing you how to create plots in plots via insets.\n\n# Tweak boxplots a bit for better visual fit to point_plot\nplt <- boxplot_plot + \n  theme_minimal() + \n  coord_flip() + \n  theme(plot.background = element_rect(fill = \"grey80\"))\n\npoint_plot +\n  coord_cartesian(xlim = c(25, 60)) +\n  inset_element(\n    plt, \n    left = 0.01, \n    right = 0.4,\n    top = 0.99, \n    bottom = 0.6\n  )"
  },
  {
    "objectID": "posts/ggplot2-tips/04_arranging_plots/04_arranging_plots.html#create-subplots-via-ggforce",
    "href": "posts/ggplot2-tips/04_arranging_plots/04_arranging_plots.html#create-subplots-via-ggforce",
    "title": "ggplot tips: Arranging plots",
    "section": "Create Subplots via ggforce",
    "text": "Create Subplots via ggforce\nI really enjoy arranging plots with patchwork because, to me, the syntax feels quite intuitive (mostly). However, as you probably noticed, I had to design each subplot and arrange them by hand. Clearly, if I want to use a grid-like arrangement to display each combination of two variables from a given set of variables, this may become tedious.\nLuckily, there is the ggforce package that has a neat faceting function to accomplish just that. As was already mentioned above, apart from that, the ggforce package offers even more cool stuff which we will look at in a future blog post.\nWith facet_matrix() it becomes quite easy to get a grid of subplots to display multiple combinations of two variables. For instance, take a look at this.\n\nlibrary(ggforce)\ndat %>% \n  ggplot(aes(x = .panel_x, y = .panel_y, fill = sex)) +\n  geom_point(alpha = 0.5, size = 2, shape = 21) +\n  facet_matrix(\n    vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g)\n  )\n\n\n\n\nNow, while this is not a particular beautiful plot, it gives us a quick overview of interesting variables which might be great for an exploratory analysis. Notice how we had to use .panel_x and .panel_y as placeholder for the individual variables. We could use the geom_auto*() functions to avoid typing that as they default to the correct values for x and y. Consequently, we could have written\n\ndat %>% \n  ggplot(aes(fill = sex)) +\n  geom_autopoint(alpha = 0.5, size = 2, shape = 21) +\n  facet_matrix(\n    vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g)\n  )\n\nWith a little bit of tweaking, we can make this plot more interesting. For example. it would be neat if we had density plots on the diagonal. No problem! Add another geom_autodensity() layer and make sure that facet_matrix() understands to map only this layer to the diagonal subplots.\n\ndat %>% \n  ggplot(aes(fill = sex)) +\n  geom_autopoint(alpha = 0.5, size = 2, shape = 21) + # Layer 1\n  geom_autodensity(alpha = 0.5, position = \"identity\") + # Layer 2\n  facet_matrix(\n    vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g),\n    layer.diag = 2\n  )\n\n\n\n\nSee how layer.diag = 2 maps the diagonal elements to the second line of geom_* code. Similarly, we can manipulate the content of the upper and lower triangle in this grid by changing layer.lower or layer.upper in facet_matrix(). Let’s add another layer to see that in action.\n\ndat %>% \n  ggplot(aes(fill = sex)) +\n  geom_autopoint(alpha = 0.5, size = 2, shape = 21) + # Layer 1\n  geom_autodensity(alpha = 0.75, position = \"identity\") + # Layer 2\n  geom_hex(aes(x = .panel_x, y = .panel_y), alpha = 0.75) + # Layer 3\n  facet_matrix(\n    vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g),\n    layer.diag = 2,\n    layer.lower = 3\n  )\n\n\n\n\nLast but not least, let me mention that we can also easily create what is called an “asymmetric grid” in ggforce by mapping rows and columns manually. This is great for having categorical variables on one axis and numerical variables on the other axis.\n\ndat %>% \n  ggplot() +\n  geom_boxplot(\n    aes(x = .panel_x, y = .panel_y, group = .panel_x)\n  ) +\n  facet_matrix(\n    cols = vars(sex, species), \n    rows = vars(bill_depth_mm:body_mass_g)\n  )\n\n\n\n\nBeware that geom_boxplot() is a bit tricky as it requires the group argument to be explicitly set. Furthermore, if you want to add another aesthetic, e.g. fill, you will have to set group via interaction().\n\ndat %>% \n  ggplot() +\n  geom_boxplot(\n    aes(\n      x = .panel_x, \n      y = .panel_y, \n      fill = island, \n      group = interaction(.panel_x, island)\n    )\n  ) +\n  facet_matrix(\n    cols = vars(sex, species), \n    rows = vars(bill_depth_mm:body_mass_g)\n  )\n\n\n\n\nThis concludes our short summary of possibilities to arrange plots. In the next post of this ggplot2-tips series we will take a closer look at ggforce. I hope you enjoyed today’s blog post and I look forward to “see” you at my next blog post. In the meantime, feel free to leave a comment or a click on the applause button below."
  },
  {
    "objectID": "posts/ggplot2-tips/03_position_adjustment/03_position_adjustment.html",
    "href": "posts/ggplot2-tips/03_position_adjustment/03_position_adjustment.html",
    "title": "ggplot tips: Using position_stack() for Individual Positioning",
    "section": "",
    "text": "For a long time I have wondered why some people would use position_stack() for position alignment instead of the simpler version position = \"stack\". Recently, though, I learned the purpose of the former approach when I tried to add data labels to a stacked bar chart for better legibility.\nFurther, I decided that this knowledge is a good addition to this ggplot2-tips series, so let’s see what position_stack() can do. To achieve this, let us create a small dummy data set.\n\nlibrary(tidyverse)\ndummy_dat <- tibble(\n  group = c(rep(\"A\", 3), rep(\"B\", 3)),\n  category = factor(\n    c(rep(c(\"low\", \"medium\", \"high\"), 2)), \n    levels = rev(c(\"low\", \"medium\", \"high\")),\n  ),\n  percent = c(0.41, 0.16, 1 - 0.41 - 0.16, 0.26, 1 - 0.26 - 0.36, 0.36)\n)\ndummy_dat\n\n# A tibble: 6 × 3\n  group category percent\n  <chr> <fct>      <dbl>\n1 A     low         0.41\n2 A     medium      0.16\n3 A     high        0.43\n4 B     low         0.26\n5 B     medium      0.38\n6 B     high        0.36\n\n\nNext, take a look at the corresponding stacked bar chart. Since we created a dataset that contains percentages, I took the liberty of appropriately transforming the y-axis via scale_y_continuous().\n\ndummy_dat %>% \n  ggplot(aes(x = group, y = percent, fill = category)) +\n  geom_col() +\n  scale_y_continuous(labels = scales::percent_format())\n\n\n\n\nI believe that this visualization could be improved by adding text labels to each part of the stacked bar chart in order for the reader to immediately detect how large each portion of the bars is. Let’s try this via simply converting the values to strings and adding geom_text() to the plot.\n\ndummy_dat %>% \n  mutate(percent_labels = scales::percent(percent)) %>% \n  ggplot(aes(x = group, y = percent, fill = category)) +\n  geom_col() +\n  geom_text(aes(label = percent_labels)) + \n  scale_y_continuous(labels = scales::percent_format())\n\n\n\n\nClearly, this did not work as intended because geom_text() uses position = \"identity\" by default which is why the y-position of the labels is simply determined by its value. Now, here is where I would usually change the positioning via position = \"stack\". However, the result this approach delivers is somewhat less than perfect.\n\ndummy_dat %>% \n  mutate(percent_labels = scales::percent(percent)) %>% \n  ggplot(aes(x = group, y = percent, fill = category)) +\n  geom_col() +\n  geom_text(aes(label = percent_labels), position = \"stack\") + \n  scale_y_continuous(labels = scales::percent_format())\n\n\n\n\nIdeally, I would like the labels to appear in the middle of each colored block. We could try to use vjust to move the labels which is not a great idea since every label will be moved by the same amount and the blocks are of different height. Similarly, we could compute the block middle points by hand and use that as separate y-aesthetic in geom_text().\nClearly, this involves a tedious additional computation and we should avoid this, if possible. This is precisely where position_stack() comes in. Conveniently, using position = position_stack() stacks the bars just like position = \"stack\" does but the function position_stack() has another argument vjust by which we can move the labels individually.\nHere, the possible values of vjust range from 0 (bottom of the designated height) to 1 (top of the designated height). Therefore, moving the labels to the middle of each bar is as easy as setting vjust = 0.5.\n\ndummy_dat %>% \n  mutate(percent_labels = scales::percent(percent)) %>% \n  ggplot(aes(x = group, y = percent, fill = category)) +\n  geom_col() +\n  geom_text(\n    aes(label = percent_labels), \n    position = position_stack(vjust = 0.5)\n  ) + \n  scale_y_continuous(labels = scales::percent_format())\n\n\n\n\nFinally, one may - and this is definitely a matter of taste - tweak this plot further by changing the color and text formatting. Personally, I like darker colors combined with a white, bold label. In this case, this would look like this.\n\ndummy_dat %>% \n  mutate(percent_labels = scales::percent(percent)) %>% \n  ggplot(aes(x = group, y = percent, fill = category)) +\n  geom_col() +\n  geom_text(\n    aes(label = percent_labels), \n    position = position_stack(vjust = 0.5),\n    col = \"white\",\n    fontface = \"bold\"\n  ) + \n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\nIn summary, we have seen that using position = position_stack() is a more powerful alternative to position = \"stack\" that allows individual positioning. Nevertheless, as long as the additional arguments of position_stack() are not needed I still find the latter version simpler."
  },
  {
    "objectID": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html",
    "href": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html",
    "title": "Recreating the Storytelling with Data look with ggplot",
    "section": "",
    "text": "So, I found a great video from Storytelling with Data (SWD). In this video, a data storyteller demonstrates how a dataviz that does not demonstrate a clear story can be improved. Let’s take a look at the dataviz but, first, here’s the data.\nThis data set contains a lot of accuracy and error rates from different (anonymous) warehouses. Additionally, there are “null rates”. These are likely related to data quality issues. Furthermore, this data set is apparently taken from a client the data storytellers helped. In any case, here is a ggplot2 recreation of the client’s initial plot. Note that the plot does not match exactly but it’s close enough to get the gist.\nAs it is right know, the plot shows data. But what is the message of this dataviz? To make the message more explicit, the plot is transformed during the course of the video. Take a look at what story the exact same data can tell.\nFrom reading the SWD book, I know that some of the techniques that were used in this picture can be used in many settings. Therefore, I decided to document the steps I took to recreate the dataviz with ggplot.\nI tried to make this documentation as accessible as possible. Consequently, if you are already quite familiar with how to customize a ggplot’s details, then some of the explanations or references may be superfluous. Feel free to skip them. That being said, let’s transform the plot."
  },
  {
    "objectID": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#flip-the-axes-for-long-names",
    "href": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#flip-the-axes-for-long-names",
    "title": "Recreating the Storytelling with Data look with ggplot",
    "section": "Flip the axes for long names",
    "text": "Flip the axes for long names\nAlthough it is not really an issue here, warehouses or other places might be more identifiable by a (long) name rather than an ID. To make sure that these names are legible, show them on the y-axes. When I first learned ggplot, there was the layer coord_flip() to do that job for us. Nowadays, though, you can often avoid coord_flip() because a lot of geoms already understand what you mean, when you map categorical data to the y-aesthetic. But make sure that ggplot will know that you mean categorical data (especially if the labels are numerical like here).\n\ncategorial_dat <- dat_long %>% \n  mutate(\n    id = as.character(id),\n  )\n\ncategorial_dat %>% \n  ggplot(aes(x = percent, y = id)) +\n  geom_col(\n    aes(group = factor(type, levels = c('error', 'null', 'accuracy'))),\n    col = 'white', # set color to distinguish bars better\n  )\n\n\n\n\nNotice that I used the group- instead of fill-aesthetic because I only need grouping. Also, it is always a good idea to avoid excessive use of colors. This will allow us to emphasize parts of our story with colors later on."
  },
  {
    "objectID": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#add-reference-points",
    "href": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#add-reference-points",
    "title": "Recreating the Storytelling with Data look with ggplot",
    "section": "Add reference points",
    "text": "Add reference points\nAnother good idea it to put your data into perspective. To do so, include a reference point. This can be a summary statistic like the average error rate. For more great demonstration of reference points you can also check out the evolution of a ggplot by Cédric Scherer.\n\naverages <- dat_long %>% \n  group_by(type) %>% \n  summarise(percent = mean(percent)) %>% \n  mutate(id = 'ALL') \n\ndat_with_summary <- categorial_dat %>% \n  bind_rows(averages)\n\ndat_with_summary %>% \n  ggplot(aes(x = percent, y = id)) +\n  geom_col(\n    aes(group = factor(type, levels = c('error', 'null', 'accuracy'))),\n    col = 'white',\n  )"
  },
  {
    "objectID": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#order-your-data",
    "href": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#order-your-data",
    "title": "Recreating the Storytelling with Data look with ggplot",
    "section": "Order your data",
    "text": "Order your data\nTo allow your reader to gain a quick overview, put your data into some form of sensible ordering. This eases the burden of having to make sense of what the visual shows. Also, notice that we already did part of that. See, with the order of the levels in the group aesthetic, we influenced the ordering of the stacked bars. Here, we made sure that important quantities start at the left resp. right edges.\nWhy is that helpful, you ask? Well, the bars that start on the left all start at the same reference point. Therefore comparisons are quite easy for these bars. The same holds true for the right edge. Consequently, it is best that we reserve these vip seats for the important data. Check out what happens if I were to put the accuracy in the middle.\n\ndat_with_summary %>% \n  ggplot(aes(x = percent, y = id)) +\n  geom_col(\n    aes(group = factor(type, levels = c('error', 'accuracy', 'null'))),\n    col = 'white',\n  )\n\n\n\n\nNow, we can’t really make out which warehouses have a higher accuracy. Given that the accuracy is likely something we care about, this is bad. But we can change the order even more. For instance, we can also order the bars by error rate. Here, fct_reorder() is our friend.\n\nordered_dat <- dat_with_summary %>% \n  mutate(\n    type = factor(type, levels = c('error', 'null', 'accuracy')),\n    id = fct_reorder(id, percent, .desc = T)\n  ) \n\nordered_dat %>% \n  ggplot(aes(x = percent, y = id)) +\n  geom_col(\n    aes(group = type),\n    col = 'white',\n  )"
  },
  {
    "objectID": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#highlight-your-story-points",
    "href": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#highlight-your-story-points",
    "title": "Recreating the Storytelling with Data look with ggplot",
    "section": "Highlight your story points",
    "text": "Highlight your story points\nNext, it’s time to highlight your story points. This can be done with the gghighlight as I have demonstrated in another blog post. Alternatively, we can set the colors manually. The latter approach gave me the best results in this case, so we’ll go with that. But I am still a big fan of gghighlight, so don’t discard its power just yet.\n\n# Set colors as variable for easy change later\nunhighlighted_color <- 'grey80'\nhighlighted_color <- '#E69F00'\navg_error <- 'black'\navg_rest <- 'grey40'\n\n# Compute new column with colors of each bar\ncolored_dat <- ordered_dat %>% \n  mutate(\n    custom_colors = case_when(\n      id == 'ALL' & type == 'error' ~ avg_error,\n      id == 'ALL' ~ avg_rest,\n      type == 'error' & percent > 0.1 ~ highlighted_color,\n      T ~  unhighlighted_color\n    )\n  )\n\np <- colored_dat %>% \n  ggplot(aes(x = percent, y = id)) +\n  geom_col(\n    aes(group = type),\n    col = 'white',\n    fill = colored_dat$custom_colors # Set colors manually\n  )\np\n\n\n\n\nNotice how your eyes are immediately drawn to the intended region. That’s the power of colors! Also, note that setting the colors manually like this worked because fill in geom_col() is vectorized. This is not always the case. In these instances, you may find that functional programming solves your problem."
  },
  {
    "objectID": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#remove-axes-expansion-and-allow-drawing-outside-of-grid",
    "href": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#remove-axes-expansion-and-allow-drawing-outside-of-grid",
    "title": "Recreating the Storytelling with Data look with ggplot",
    "section": "Remove axes expansion and allow drawing outside of grid",
    "text": "Remove axes expansion and allow drawing outside of grid\nDid you notice that there is still some clutter in the plot? Removing clutter from a plot is a central element of the SWD look. Personally, I like this approach. So, let’s get down to the essentials and remove what does not need to be there. In this case, there are still (faint) horizontal lines behind each bar. Furthermore, this causes the warehouse IDs to be slightly removed from the bars. We change that through formatting the coordinate system with coord_cartesian().\n\np <- p +\n  coord_cartesian(\n    xlim = c(0, 1), \n    ylim = c(0.5, 20.5), \n    expand = F, # removes white spaces at edge of plot\n    clip = 'off' # allows drawing outside of panel\n  )\np\n\n\n\n\nHere, we turned off the expansion to avoid wasting white space. Now, the IDs are at their designated place and we do not see lines from their names to the bars anymore. If you want even more power on the space expansion you can leave expand = T and modify the expansion for each axis with scale_*_continuous() and the expansion() function. Check out Christian Burkhart’s neat cheatsheet that teaches you everything you need to understand expansions.\nOn an unrelated note, you may wonder why I set clip = 'off'. This little secret will be revealed soon. For now, just know that it allows you to draw geoms outside the regular panel."
  },
  {
    "objectID": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#move-and-format-axes",
    "href": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#move-and-format-axes",
    "title": "Recreating the Storytelling with Data look with ggplot",
    "section": "Move and format axes",
    "text": "Move and format axes\nYou may have noticed that the x-axis in the finished plot is at the top of the panel rather than at the bottom. While that is unusual, it helps the reader to get straight to the point as the data is in view earlier. This assumes that the eyes of a typical dataviz reader will first look at the top left corner and then zigzag downwards.\nIn ggplot2, moving the axes and setting the break points happens in a scale layer. It is here where we use the scales::percent() function to transform the axes labels. Additionally, changing labels happens in labs() and the remaining axes and text changes happen in theme().\n\nunhighlighed_col_darker <- 'grey60'\np <- p +\n  scale_x_continuous(\n    breaks = seq(0, 1, 0.2),\n    labels = scales::percent,\n    position = 'top'\n  ) +\n  labs(\n    title = 'Accuracy rates for highest volume warehouses',\n    y = 'WAREHOUSE ID',\n    x = '% OF TOTAL ORDERS FULFILLED',\n  ) +\n  theme(\n    axis.line.x = element_line(colour = unhighlighed_col_darker),\n    axis.ticks.x = element_line(colour = unhighlighed_col_darker),\n    axis.text = element_text(colour = unhighlighed_col_darker),\n    text = element_text(colour = unhighlighed_col_darker),\n    plot.title = element_text(colour = 'black')\n  )\np\n\n\n\n\nNotice that we have customized the theme elements via element_*() functions. Basically, each geom type like “line”, “rect”, “text”, etc. has their own element_*() function. The theme() function expects attributes to be changed using these. If you are unfamiliar with this concept, maybe the corresponding part in my YARDS lecture notes will help you."
  },
  {
    "objectID": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#align-labels",
    "href": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#align-labels",
    "title": "Recreating the Storytelling with Data look with ggplot",
    "section": "Align labels",
    "text": "Align labels\nAligning plot elements, e.g. labels, to form clean lines is another major aspect of the SWD look. Before I read about it, I did not even notice it but once you see it you cannot go back. Basically, plots feel “more harmonious” if there are clear (not necessarily drawn) lines like with the left and right edge of the stacked bars. But this concept does not stop with the bars and can be used for the labels too. Let’s demonstrate that by moving the labels with more of theme().\n\np <- p +\n  theme(\n    axis.title.x = element_text(hjust = 0),\n    axis.title.y = element_text(hjust = 1),\n    plot.title.position = 'plot'\n    # aligns the title to the whole plot and not the (inner) panel\n  )\np\n\n\n\n\nOnce again, the design enforces that important information like what’s on an axis is in the top left corner. This was done by changing hjust. In this case hjust = 0 corresponds to left-justified whereas hjust = 1 corresponds to right-justified. Of course, vjust works similarly. For more details w.r.t. hjust and vjust, check out this stackoverflow answer that gives you everything that you need in one visual. For your convenience, here is a slightly changed form of that visual.\n\n\n\n\n\nBut once you start aligning the axes titles, you notice that the 0% and 100% labels fall outside the grid. We could try to set hjust of axis.text.x in theme() but sadly this is not vectorized. Subsequently, all hjust values must be the same. That’s not bueno. Therefore, I drew the axes labels manually with annotate() but make sure that you remove the current labels in scale_x_continuous(). Also, now you know why we had to set clip = 'off' earlier. The axes labels are outside of the regular panel.\n\np <- p +\n  # Overwriting previous scale will generate a warning but that's ok\n  scale_x_continuous(\n    breaks = seq(0, 1, 0.2), # We still want the axes ticks\n    labels = rep('', 6), # Empty strings as labels\n    position = 'top'\n  ) +\n  annotate(\n    'text',\n    x = seq(0, 1, 0.2),\n    y = 20.75,\n    label = scales::percent(seq(0, 1, 0.2), accuracy = 1),\n    size = 3,\n    hjust = c(0, rep(0.5, 4), 1),\n    # individual hjust here\n    vjust = 0, \n    col = unhighlighed_col_darker\n  ) +\n  theme(\n    axis.title.x = element_text(hjust = 0, vjust = 0)\n    # change vjust to avoid overplotting\n  )\np"
  },
  {
    "objectID": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#add-text-labels",
    "href": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#add-text-labels",
    "title": "Recreating the Storytelling with Data look with ggplot",
    "section": "Add text labels",
    "text": "Add text labels\nThe same trick can be used to add the category description (accuracy, null, error) to the right top corner and label the highlighted bars. For the latter part, we simply extract the corresponding rows from our data and use that in conjunction with geom_text().\n\ntext_labels <- colored_dat %>% \n  filter(type == 'error', percent > 0.1) %>% \n  mutate(percent = scales::percent(percent, accuracy = 1))\n\np <- p +\n  geom_text(\n    data = text_labels, \n    aes(x = 1, label = percent), \n    hjust = 1.1,\n    col = 'white',\n    size = 4\n  )\np\n\n\n\n\nNotice that I used a hjust value greater than 1 here to add some white space on the right side of the labels. Otherwise, the percent sign will be too close to the bar’s edge.\nNext, we add the category descriptions. This is a bit more tricky, though, because we want to highlight a word too, So, we will add a richtext as described in my previous blog post.\n\nlibrary(ggtext)\np <- p +\n  annotate(\n    'richtext',\n    x = 1,\n    y = 21.25,\n    label = \"ACCURATE | NULL | <span style = 'color:#E69F00'>ERROR</span>\",\n    hjust = 1,\n    vjust = 0, \n    col = unhighlighed_col_darker, \n    size = 4,\n    label.colour = NA,\n    fill = NA\n  )\np"
  },
  {
    "objectID": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#add-story-text",
    "href": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#add-story-text",
    "title": "Recreating the Storytelling with Data look with ggplot",
    "section": "Add story text",
    "text": "Add story text\nNow that the bar plot is finished we can work on the story text. For that, we create another plot that contains only the text. Later on, we will combine both of our plots with the patchwork package. There are no really knew techniques here, so let’s get straight to the code.\n\n# Save text data in a tibble\ntib_summary_text <- tibble(\n  x = 0, \n  y = c(1.65, 0.5), \n  label = c(\"<span style = 'color:grey60'>OVERALL:</span> **The error rate is 10% across all<br>66 warehouses**. <span style = 'color:grey60'>The good news is that<br>the accuracy rate is 85% so we\\'re hitting<br>the mark in nearly all our centers due to<br>the quality initiatives implemented last year.</span>\",\n  \"<span style = 'color:#E69F00'>OPPORTUNITY TO IMPROVE:</span> <span style = 'color:grey60'>10 centers<br>have higher than average error rates of<br>10%-16%.</span> <span style = 'color:#E69F00'>We recommend investigating<br>specific details and **scheduling meetings<br>with operations managers to<br>determine what's driving this.**</span>\"\n  )\n)\n\n# Create text plot with geom_richtext() and theme_void()\ntext_plot <- tib_summary_text %>% \n  ggplot() +\n  geom_richtext(\n    aes(x, y, label = label),\n    size = 3,\n    hjust = 0,\n    vjust = 0,\n    label.colour = NA\n  ) +\n  coord_cartesian(xlim = c(0, 1), ylim = c(0, 2), clip = 'off') +\n  # clip = 'off' is important for putting it together later.\n  theme_void()\ntext_plot"
  },
  {
    "objectID": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#add-main-message-as-new-title-and-subtitle",
    "href": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#add-main-message-as-new-title-and-subtitle",
    "title": "Recreating the Storytelling with Data look with ggplot",
    "section": "Add main message as new title and subtitle",
    "text": "Add main message as new title and subtitle\nAs I said before, we will put the two plots together with patchwork. If you have never dealt with patchwork, feel free to check out my short intro to patchwork. Putting the plots together gives us another opportunity: We can now set additional titles and subtitles of the whole plot. Use these to add the main message of your plot.\nBut make sure that there is enough white space around them by setting the title margins in theme(). Otherwise, your plot will feel “too full”. Adding spacing is achieved through a margin() function in element_text(). Though, in this case we use element_markdown() which works exactly the same but enables Markdown syntax like using asterisks for bold texts.\n\n# Save texts as variables for better code legibility\n# Here I used Markdown syntax\n# To enable its rendering, use element_markdown() in theme\ntitle_text <- \"**Action needed:** 10 warehouses have <span style = 'color:#E69F00'>high error rates</span>\"\nsubtitle_text <- \"<span style = 'color:#E69F00'>DISCUSS:</span> what are <span style = 'color:#E69F00'>**next steps to improve errors**</span> at highest volume warehouses?<br><span style = 'font-size:10pt;color:grey60'>The subset of centers shown (19 out of 66) have the highest volume of orders fulfilled</span>\"\ncaption_text <- \"SOURCE: ProTip Dashboard as of Q4/2021. See file xxx for additional context on remaining 47 warehouses<br><span style = 'font-size:6pt;color:grey60'>Original: Storytelling with Data - improve this graph! exercise | {ggplot2} remake by Albert Rapp (@rappa753).\"\n\n# Compose plot\nlibrary(patchwork)\np +\n  text_plot +\n  # Make text plot narrower\n  plot_layout(widths = c(0.6, 0.4)) +\n  # Add main message via title and subtitle\n  plot_annotation(\n    title = title_text,\n    subtitle = subtitle_text,\n    caption = caption_text,\n    theme = theme(\n      plot.title = element_markdown(\n        margin = margin(b = 0.4, unit = 'cm'),\n        # 0.4cm margin at bottom of title\n        size = 16\n      ),\n      plot.subtitle = element_markdown(\n        margin = margin(b = 0.4, unit = 'cm'),\n        # 0.4cm margin at bottom of title\n        size = 11.5\n      ),\n      plot.caption.position = 'plot',\n      plot.caption = element_markdown(\n        hjust = 0, \n        size = 7, \n        colour = unhighlighed_col_darker, \n        lineheight = 1.25\n      ),\n      plot.background = element_rect(fill = 'white', colour = NA)\n      # This is only a trick to make sure that background really is white\n      # Otherwise, some browsers or photo apps will apply a dark mode\n    )\n  )"
  },
  {
    "objectID": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#get-the-sizes-right",
    "href": "posts/ggplot2-tips/10_recreating_swd_look/10_recreating_swd_look.html#get-the-sizes-right",
    "title": "Recreating the Storytelling with Data look with ggplot",
    "section": "Get the sizes right",
    "text": "Get the sizes right\nIn the last plot, I cheated. I gave you the correct code I used to generate the picture. But I did not execute it. Instead, I only displayed the code and then showed you the (imported) picture from the start of this blog post. Why did I do this? Because getting the sizes right sucks!\nIf you have dealt with ggplot enough, then you will know that text sizes are often set in absolute rather than in relative terms. Therefore, if you make the bar plot smaller in width (like we did), then the bars may be appropriately scaled to the new width but, more often than not, the texts are not. In this case, this led to way too large fonts as beautifully demonstrated in Christophe Nicault’s helpful blog post.\nSo, how do you avoid this? First off, choose size and fonts last (choose the font first, though). This will save you a lot of repetitive work when you change the alignment in your plot. But this tip will only get you so far, because you have to fix some sizes in between to get a feeling for the visualization you are trying to create.\nTherefore, try to get you canvas into an appropriate size first. I try to do this by using the camcorder package at the start of my visualization process. This will ensure that my plots are saved as a png-file with predetermined dimensions and the resulting file is displayed in the Viewer pane in RStudio (as opposed to the Plots pane).\nFor example, at the start of working on this visualization I have called\n\ncamcorder::gg_record(\n  dir = 'img', dpi = 300, width = 16, height = 9, units = 'cm'\n)\n\nThis made getting the sizes right for my final output somewhat easier because the canvas size remains the same throughout the process. Though be sure to call gg_record() after library(ggtext) or make sure that you call gg_record() again if you add ggtext only later. Otherwise, your plots will revert back to being displayed in the Plots pane (with relative sizing). Finally, if you want to use camcorder in conjunction with showtext, then be sure that showtext will know what dpi value you chose when calling gg_record().\n\nshowtext::showtext_opts(dpi = 300)\n\nAlright, that concludes this somewhat long blog post. I hope that you enjoyed it and learned something valuable. If you did, feel free to leave a comment. Also, you can stay in touch with my work by subscribing to my RSS feed or following me on Twitter."
  },
  {
    "objectID": "posts/ggplot2-tips/08_fonts_and_icons/08_fonts_and_icons.html",
    "href": "posts/ggplot2-tips/08_fonts_and_icons/08_fonts_and_icons.html",
    "title": "How to use Fonts and Icons in ggplot",
    "section": "",
    "text": "For some reason, using other than the default font in plots has been a major problem for me in R. Supposedly, one can use the extrafont package to manage all of that but I found it too cumbersome. Instead, I found out that the showtext package can make my life easier.\nEven though working with text in plot is not yet completely free of troubles, showtext has made many things easier. Now, I can finally choose fonts freely and even use icons. This blogposts gives you a how-to so that you can do that too."
  },
  {
    "objectID": "posts/ggplot2-tips/08_fonts_and_icons/08_fonts_and_icons.html#import-and-use-fonts-with-showtext",
    "href": "posts/ggplot2-tips/08_fonts_and_icons/08_fonts_and_icons.html#import-and-use-fonts-with-showtext",
    "title": "How to use Fonts and Icons in ggplot",
    "section": "Import and Use Fonts with showtext",
    "text": "Import and Use Fonts with showtext\nA great source for fonts is Google’s font page. What is great abut this page is that it can display texts in many different fonts.\n\n\n\n\n\nScreenshot from fonts.google.com\n\n\n\n\nOnce we found a nice font, we can use its name to make it available within R. This is done with showtext’s helpful font_add_google() function. Let’s import a couple of random fonts.\n\n# Packages that we will use in this post\n\nlibrary(tidyverse)\nlibrary(showtext)\nlibrary(ggtext)\nlibrary(gghighlight)\n\n# Import fonts\n# First argument = google name, \n# Secont name = font name in R\nfont_add_google('Lora', 'lora')\nfont_add_google('Lobster', 'lobster')\nfont_add_google('Anton', 'anton')\nfont_add_google('Fira Sans', 'firasans')\nfont_add_google('Syne Mono', 'syne')\n\n# Important step to enable showtext font rendering!\nshowtext_auto()\n\nNotice that we have also used showtext_auto(). This is necessary for showtext to take over the show. Otherwise, the new fonts would not be usable. Now, let’s take a look at our new fonts.\n\ntib <- tibble(\n  family = c('firasans', 'lora', 'lobster', 'anton', 'syne'),\n  x = 0,\n  y = seq(0.0, 1, length.out = 5),\n  label = 'Showtext shows text. Wow. What an insight.'\n)\n\ntib %>%\n  ggplot(aes(x, y, label = label)) +\n  geom_text(family = tib$family, size = 13, hjust = 0, col = 'dodgerblue4') +\n  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +\n  theme_void()\n\n\n\n\nYou may wonder why we have used coord_cartesian() here. We did this in order to ensure that the x-axis is not centered at 0 and our example texts won’t be outside of the plot. Personally, I find this somewhat tedious but this can’t be helped, I guess. With text elements we always run at the risk of writing outside of the plot area.\nNext, let’s make our use of fonts somewhat more practical. In my last blog post, I stressed the use of highlighting a few important things instead of using many colors. Combine this with direct labels instead of a legend and you get this plot I created using the Fira Sans font.\n\n\n\n\n\nNow, see what it would look like had I used the Lobster font instead.\n\n\n\n\n\nFeels different doesn’t it? And this is still different than the Anton font."
  },
  {
    "objectID": "posts/ggplot2-tips/08_fonts_and_icons/08_fonts_and_icons.html#import-and-use-icon-fonts-with-showtext",
    "href": "posts/ggplot2-tips/08_fonts_and_icons/08_fonts_and_icons.html#import-and-use-icon-fonts-with-showtext",
    "title": "How to use Fonts and Icons in ggplot",
    "section": "Import and Use Icon Fonts with showtext",
    "text": "Import and Use Icon Fonts with showtext\nWe can not only use regular text fonts but also icons with showtext. For example, we may want to use one of the free Fontawesome icons. To do so, download the newest version and extract the .otf-files into your working directory. These contain the font information that you need. Importing these (and any other font for that matter) works with font_add() and the path to the .otf-files.\n\n# First argument = name in R\n# Second argument = path to .otf-file\nfont_add('fa-reg', 'fonts/Font Awesome 6 Free-Regular-400.otf')\nfont_add('fa-brands', 'fonts/Font Awesome 6 Brands-Regular-400.otf')\nfont_add('fa-solid', 'fonts/Font Awesome 6 Free-Solid-900.otf')\n\nNow that we imported the fonts, we can use ggtext’s geom_richtext() and some HTML wizardry to add icons to our previously imported fonts from Google. But first, what we need is an icon’s unicode identifier? Uni-what?\nThe easiest way to find that is to stroll through the Fontawesome icons online. Then, find one that matches the font you want to use, e.g. free and solid. Finally, find it’s unicode character in the corresponding popup menu.\n\n\n\n\n\nScreenshot from fontawesome.com. Unicode highlighted in yellow.\n\n\n\n\nOnce you got this, you can add &#x in front of the unicode and wrap <span> tags around it. Within these tags, you will have to specify font-family so that the icon is rendered.\n\ntib <- tibble(\n  family = c('firasans', 'lora', 'lobster', 'anton', 'syne'),\n  x = 0,\n  y = seq(0.0, 1, length.out = 5),\n  label = \"Let's talk cash <span style='font-family:fa-solid'>&#xf651;</span>\"\n)\n\ntib %>%\n  ggplot(aes(x, y, label = label)) +\n  geom_richtext(family = tib$family, size = 16, hjust = 0, col = 'dodgerblue4', label.colour = NA) +\n  coord_cartesian(xlim = c(0, 1), ylim = c(-0.1, 1.1)) +\n  theme_void()\n\n\n\n\nThis way, you can also use icons in scatter plots. Though, make sure to set fill=NA if you do not want to have white boxes around the icons.\n\ntibble(x = runif(25), y = runif(25)) %>% \n  ggplot(aes(x, y, label = \"<span style='font-family:fa-solid;'>&#xf651;</span>\")) +\n  geom_richtext(size = 12, label.colour = NA, fill = NA, col = 'dodgerblue4',) +\n  theme_minimal()\n\n\n\n\nYou will notice that using the two previous code chunks will generate a lot of warnings about “native encoding”. So far, I have always been able to ignore these without any trouble. I really don’t know why they appear. And if you know, please let me know in the comments below."
  },
  {
    "objectID": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html",
    "href": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html",
    "title": "6 Lessons that I learned from teaching R to non-programmers",
    "section": "",
    "text": "Last week, I gave a short workshop teaching R to Economics students without prior programming experience. On Twitter, I shared six lessons that I wish I had learnt before. This blog post is a more detailed account of my experience. Additionally, I incorporated many excellent suggestions from others on Twitter into this blog post."
  },
  {
    "objectID": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#do-less",
    "href": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#do-less",
    "title": "6 Lessons that I learned from teaching R to non-programmers",
    "section": "Do less",
    "text": "Do less\nThe first and most important lesson is that I have to plan to do less. This is probably the trickiest part for me. Usually, I like to create an ambitious curriculum for my students. For example, in my semester-long YARDS course for mathematicians, I introduce many topics like\n\ndata viz and wrangling with ggplot2 and dplyr,\nif-conditions and for-loops (yes, I think everyone should be familiar with for-loops),\nmodel building with tidymodels,\nfunctional programming with purrr,\ncreating and sharing content with Rmarkdown and\ninteractive web apps with shiny.\n\nIn that course, students already came with at least a basic understanding of programming. Most of the time, I only had to lay out a few code examples. Then, I quickly made students work on assignments. As students already knew how to read code and debug, they knew how to get themselves unstuck. This was nice as it enabled my ambitious curriculum. Though, I never fully appreciated how much easier this made teaching.\n\nScaling down for the workshop\nOf course, I knew that I needed to scale down in a 4x3h workshop. And I thought I did that. But I was in for a surprise because there was a crucial issue:\n\nThings that are obvious to someone with programming experience are not obvious to non-programmers at all.\n\nLet’s go through an example. Imagine students already had seen the following fictitious code chunks.\n\nfilter(dat, year > 1995)\n\n\nggplot(data = dat) + \n    geom_point(mapping = aes(x = x, y = y))\n\nIn one exercise, the students were supposed to plot not the whole data set dat but a filtered version of it. So, I thought that - having seen the code chunks and heard explanations - students would “intuitively” know to combine the chunks like so\n\nggplot(data = filter(dat, year > 1994)) + \n    geom_point(mapping = aes(x = x, y = y))\n\nBut this was not the case. Students were so unfamiliar with code that they were hesitant to “stick together” the two building blocks. Unfortunately, I did not anticipate this. And without any programming experience, students will get stuck in even more unexpected places. Therefore, be sure to have time for detours. And the only way I see how that’s possible is to plan to do less."
  },
  {
    "objectID": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#teach-through-typing",
    "href": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#teach-through-typing",
    "title": "6 Lessons that I learned from teaching R to non-programmers",
    "section": "Teach through typing",
    "text": "Teach through typing\nSo, providing students with building blocks was not enough. At least in their learning stage it wasn’t. Students were just too unfamiliar with code. Fortunately, it turns out that it does not take a huge effort to familiarize students with code. Think: What’s the fastest way to learn to write code? Well, write a lot of code.\nAnd that’s exactly what I tried with them. Instead of doing demos where only I typed code, I made students type along. And I know this sounds silly. But to my surprise, it helped. It helped a lot!\nIn the end, students had the same building block as if I had done the demo alone. Yet, students were more willing to experiment with the code they have written themselves. In my book, that’s a huge step on anyone’s programming journey.\n\nMake time for typing\nOn Twitter, Trader Vix disagreed with this typing along approach. It is a fair point to say that not everyone can type “sufficiently” fast. Subsequently, some students may fall behind. In this case, I argued that even more students struggled without this typing approach. This way though, students were not falling behind. Instead, they didn’t know how to get started.\nOf course, you will have to make time for students to type. Once again: Plan to do less as students will be slower than you are. Also, make sure to ask students if they can execute the code. This can even have additional benefits. In one case, the students couldn’t execute the code we typed. From their description, I could tell that they forgot quotes as in x == 'text'. Thus, typing together can also generate teachable moments."
  },
  {
    "objectID": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#start-with-ggplot",
    "href": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#start-with-ggplot",
    "title": "6 Lessons that I learned from teaching R to non-programmers",
    "section": "Start with ggplot",
    "text": "Start with ggplot\nSome say vector manipulation should be the first thing to teach. I didn’t and this was one of the few things I wouldn’t change. My non-statistically inclined students seemed to find visual results more engaging than number/vector crunching. And once students can create graphics, the nice thing is that you can always refer back to that for motivation. Let me elaborate.\nShockingly, data wrangling does not have value in itself. I know that as mathematicians/statisticians/number crunchers it is hard to imagine that. However, incorporating a calculated value in a visualizations can generate insights. And many people find that more valuable. Take a look at two examples that use this idea. Both consider the ames data from {modeldata}.\n\nlibrary(tidyverse)\ndata(ames, package = 'modeldata')\ndat <- ames %>% \n  janitor::clean_names() %>% \n  select(lot_area, sale_price, neighborhood)\ndat\n## # A tibble: 2,930 × 3\n##    lot_area sale_price neighborhood\n##       <int>      <int> <fct>       \n##  1    31770     215000 North_Ames  \n##  2    11622     105000 North_Ames  \n##  3    14267     172000 North_Ames  \n##  4    11160     244000 North_Ames  \n##  5    13830     189900 Gilbert     \n##  6     9978     195500 Gilbert     \n##  7     4920     213500 Stone_Brook \n##  8     5005     191500 Stone_Brook \n##  9     5389     236500 Stone_Brook \n## 10     7500     189000 Gilbert     \n## # … with 2,920 more rows\n## # ℹ Use `print(n = ...)` to see more rows\n\n\nSlice for highlighting\nWhy would you want to extract specific rows from a data set? Here, we use it to highlight specific houses.\nHow do we do that? We extract the rows and throw that into another point layer.\nNow, come up with an artificial story why house 1, 5, 7 and 10 may be important. Bam! You now have a visual motivation for why slicing data is a neat technique to master.\n\nggplot(dat, aes(lot_area, sale_price)) +\n  geom_point(col = 'grey80') +\n  geom_point(data = slice(dat, 1, 5, 7, 10), col = 'red', size = 2) +\n  scale_x_log10() +\n  scale_y_log10() + \n  theme_minimal()\n\n\n\n\n\n\nSummaries for context\nThe same can be done to motivate vector calculations. Why is it good to extract a vector and compute its median? To put your other data into context. Let’s try that out for the sale prices in dat.\n\nx <- dat$sale_price\nmed <- median(x)\n\nggplot() +\n  geom_jitter(aes(x = x, y = 1), alpha = 0.25) +\n  geom_point(aes(x = med, y = 1), col = 'red', size = 5) +\n  scale_x_log10() +\n  labs(x = 'Sale price') +\n  theme_minimal()\n\n\n\n\nFor a more elaborate case, you can do the same for each neighborhood.\n\nsummaries <- dat %>% \n  group_by(neighborhood) %>% \n  summarise(med = median(sale_price))\n\nggplot(dat, aes(x = sale_price, y = neighborhood)) +\n  geom_point(alpha = 0.2) +\n  geom_point(data = summaries, aes(x = med), col = 'red', size = 3) +\n  scale_x_log10() + \n  theme_minimal()"
  },
  {
    "objectID": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#is-ggplot-too-hard",
    "href": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#is-ggplot-too-hard",
    "title": "6 Lessons that I learned from teaching R to non-programmers",
    "section": "Is ggplot too hard?",
    "text": "Is ggplot too hard?\nSo, why didn’t I use the plot() function instead of ggplot? That’s because most people (including myself) would agree that ggplot’s defaults are visually more pleasing. But the more pressing question is: Was ggplot too hard for the students? Let’s take a step back on this one.\n\nEvery new syntax is confusing\nTake a look at the following quote.\n\n\nWhen teaching, be careful not to mix up \"I learned this a long time ago\" with \"This is simple\"#rstats\n\n— David Robinson (@drob) April 20, 2016\n\n\nIn the same spirit, let me say:\n\nWhen teaching, be careful not to mix up “I learned this only fairly recently” with “This is pretty advanced stuff and better not taught early on”.\n\nPersonally, I have encountered ggplot and the tidyverse only late in my programming game. Therefore, I have refrained from teaching it to students for some time. Honestly though, this was also due to my own lack of wanting to learn new “complicated” syntax. As David Robinson states in his insightful “Teach the tidyverse to beginners” essay:\n“…all programming syntax is confusing for non-programmers.”\nIn my case, I was not a non-programmer. But ggplot has its own ecosystem with its own syntax. Thus, it is easy to say that ggplot is way too hard for beginners. But the truth is:\n\nEvery syntax is confusing to the uninitiated.\n\n\n\nSo, did students cope with ggplot or not?\nComing back to my students, this long-winded detour explains why they did surprisingly well with ggplot. Sure, the intricacies of aes() are probably not fully understood after this workshop. But having no prior experience in programming whatsoever, they were surprisingly willing to accept that aes() is just a necessary fact of life ggplot.\nAs a mathematician, I like to compare this to integral notation $ \\int_0^1 f(x)\\, \\mathrm{d}x $. In high school, I did not fully grasp the meaning of $ \\mathrm{d}x $. But I accepted its presence and treated it like a delimiter for what’s inside and outside the integral. Clearly, this is far from understanding the meaning of this notation. Yet, it enabled me to do what I needed to do.\nOverall, students would sometimes forget to put e.g. col = into the correct place. For most parts, though, the students accepted the syntax just the way it is and understood that within aes() we refer to the columns in data. And what may sound even more surprising:\nStudents fared better with ggplot() than with other functions like filter(), slice() or mutate().\nI suspect that is because ggplot is its own ecosystem and each plot follows the same template. But filter(), slice() and mutate() each work differently. One wants a conditional statement, one wants row indices and one wants a new column name plus its “formula”."
  },
  {
    "objectID": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#stress-to-save-variables-before-using-them",
    "href": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#stress-to-save-variables-before-using-them",
    "title": "6 Lessons that I learned from teaching R to non-programmers",
    "section": "Stress to save variables before using them",
    "text": "Stress to save variables before using them\nThis was another surprising moments for me. Apparently, if you have never programmed, it is not obvious to save results. Therefore, make sure that students understand that calculations can only be used later if they are saved into a variable. But this variable-saving line has to be executed for the actual saving to happen. Often, students would have a code chunk like this\n\ndat <- filter(ames, Sale_Price > 200000)\nggplot(data = dat) +\n    geom_point(mapping = aes(x = Lot_Area, Sale_Price))\n\nBut then, they would only execute the second line. More often than not, there was no previous dat variable in the environment and students would get an error. This was confusing because the dat <- line is right there. Of course, that does not mean the line was executed. Unfortunately, you will likely have to mention this multiple times before it sticks."
  },
  {
    "objectID": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#teach-named-functions",
    "href": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#teach-named-functions",
    "title": "6 Lessons that I learned from teaching R to non-programmers",
    "section": "Teach named functions",
    "text": "Teach named functions\nThe symbols$, [ and ] all have their rightful place in R. However, students unanimously found using memorable function names like pull() or select() easier to learn. But remember: Do less. I tried to show students multiple ways to get a job done using e.g. $ or pull(). This only caused confusion. So, use only one approach.\nThough, in this specific case I would likely go with $ instead of pull(). Of course, pull() is nice but chances are that students will encounter $ at some point. Thus, see this as an investment into enabling students to read more code. For the same reason, Nikita Telkar suggested to teach the full name notation, e.g. dplyr::select().\nPersonally, I would not use full names all the time though. But one particular use case comes to mind. After experiencing a few errors due to not using capital letters in column names, the students were more than willing to use clean_names() from {janitor}. So, janitor::clean_names() may be a good showcase for the full name notation."
  },
  {
    "objectID": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#use-pipes",
    "href": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#use-pipes",
    "title": "6 Lessons that I learned from teaching R to non-programmers",
    "section": "Use pipes",
    "text": "Use pipes\nAs mentioned, saving variables felt foreign to students at first. The same was true for nesting functions. Consequently, two step processes like selecting and filtering were hard. Here, pipes helped.\nThough I’ve had Math students complain that pipes feel wrong, for the Economics students it was just the right thing. In fact, the pipe often mimicked what students wanted to do anyway. Frequently, students knew that they want to e.g. filter data. So they typed filter(year > 1999). Of course, this misses the data. It seems like students were so caught up in what they wanted to do that they forgot to tell R what data to use. But chaining multiple function calls circumvented that (modulo at the beginning of the chain).\nAdditionally, let me mention that there were great contributions on Twitter by Deidre Toher and Fran Barton. Deidre Toher suggested reading pipes as ‘then’ and Fran Barton pronounced conditions like x[x > 5] as ‘such that’. This kind of reading code aloud, could go a long way to make code feel more natural for beginners."
  },
  {
    "objectID": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#other-reactions",
    "href": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#other-reactions",
    "title": "6 Lessons that I learned from teaching R to non-programmers",
    "section": "Other reactions",
    "text": "Other reactions\nThese were my six lessons. All of them were enhanced by a lot of great suggestions from Twitter. But not all suggestions fit into the previous text. So, let me mention a few more contributors.\n\nTidyverseSkeptic\nI don’t want to give the impression that I filtered out “negative” comments. So, let me point out that Prof. Norm Matloff chimed in and advocated against using pipes. For his elaborate reasons see the TidyverseSkeptic essay. That being said, I do not agree with many statements in his tweet or his essay (though I agree that for-loops should not be a reason to feel ashamed).\nHaving gotten this off my chest, I really do not want to comment more on the artificial fight between base-R and the tidyverse. To me, a lot of this dispute feels like a highly subjective back-and-forth between both sides anyway. I do like parts of both worlds and I don’t want to throw my own subjective two cents into the mix.\n\n\nMiscellaneous tips\nSo, let me close this post on a lighter note with the remaining contributions.\n\nRemington Moll suggested to use data sets that students care about. This could encompass letting students choose a data set from five prepared data sets. Of course, this could potentially take up a huge amount of preparation time. But maybe skimming a few data sets in advance could be enough for demos.\nFadel Megahed shared some of his own course material. In his slides, he uses timers and I like the idea. This way, in-class time management may become easier. I always struggle with sticking to the allotted time during a set of exercises.\nDr. Robert M Flight shared the datacarpentry lessons. I have only skimmed them but I’ve heard people praise these lessons multiple times already."
  },
  {
    "objectID": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#closing",
    "href": "posts/10_lessons_learned_teaching_nonprogrammers/10_lessons_learned_teaching_nonprogrammers.html#closing",
    "title": "6 Lessons that I learned from teaching R to non-programmers",
    "section": "Closing",
    "text": "Closing\nThis concludes my blog post. Thanks to everyone on Twitter for contributing. It pleased me to see that many people in the R community are passionate about teaching. If you liked this essay, then consider following me on Twitter and/or subscribing to my RSS feed. See you next time!"
  },
  {
    "objectID": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html",
    "href": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html",
    "title": "6 simple Shiny things I have learned from creating a somewhat small app",
    "section": "",
    "text": "A couple of weeks back, I wanted to explain to my student what I mean when I talk about the “variance of the sample variance”. In my head, this term sounds quite confusing and contains the word “variance” at least one too many times. But as I was not sure whether my subsequent explanation really came through, I decided to let my students explore the notion on their own through a Shiny app.\nHonestly, I thought this would be quite simple to code because I have already learned the basics of Shiny when I wanted to show my students what exciting web developmental things R can do. Back then, I summarized the basics in one chapter of my YARDS lecture notes.\nHowever, even though the idea of my app was simple, I soon came to realize that I would need to learn a couple more Shiny-related things to get the job done. And, as is usual with coding, I did this mostly by strolling through the web in order to find code solutions for my particular problems. Most of the time, I consulted Hadley Wickham’s Mastering Shiny but still I ended up searching for a lot of random other stuff on the web.\nConsequently, I decided that it might be nice to collect what I have learned in one place. So, here is a compilation of loosely connected troubles I solved during my Shiny learning process. May this summary serve someone well."
  },
  {
    "objectID": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#use-a-theme-for-simple-customization",
    "href": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#use-a-theme-for-simple-customization",
    "title": "6 simple Shiny things I have learned from creating a somewhat small app",
    "section": "Use a theme for simple customization",
    "text": "Use a theme for simple customization\nLet’s start with something super easy. If you wish to customize the appearance of you app, you can set the theme argument of fluidPage() to either a CSS-file that contains the necessary configuration (this is the hard way) or use a theme from bslib::bs_theme(). The latter approach comes with a lot of named preimplemented themes and is easily implemented by bootswatch = \"name\". In my app, I have simply added theme = bslib::bs_theme(bootswatch = \"superhero\"). For other themes, have a look at RStudio’s Shiny themes page.\nCheck out this super simple example that I have adapted from the default “new Shiny app” output (you will actually have to copy and run this in an R script on your own).\n\nlibrary(shiny)\nlibrary(tidyverse)\n\nui <- fluidPage(\n  # Theme added here\n  theme = bslib::bs_theme(bootswatch = \"superhero\"),\n  \n  titlePanel(\"Old Faithful Geyser Data\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"bins\",\n                  \"Number of bins:\",\n                  min = 1,\n                  max = 50,\n                  value = 30)\n    ),\n    \n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  output$distPlot <- renderPlot({\n    x    <- faithful[, 2]\n    bins <- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x, breaks = bins, col = 'darkgray', border = 'white')\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nDuring the course of this text, we will extend this small example bit by bit. But, I want to avoid copy-and-pasting code each time we change something. Thus, for the remaining examples I will only describe the changes to the previous version instead of pasting the whole code. Nevertheless, I will provide links after each example so that each script can be downloaded at will. The current example can be found here."
  },
  {
    "objectID": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#isolate-slider-from-reactivity",
    "href": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#isolate-slider-from-reactivity",
    "title": "6 simple Shiny things I have learned from creating a somewhat small app",
    "section": "Isolate slider from reactivity",
    "text": "Isolate slider from reactivity\nAs is currently intended, our app’s histogram changes whenever the slider is moved. Sometimes, though, this is not what we wish to do. Instead, we may want to delay the rendering of the plot until a button is clicked.\nThis can be achieved through a simple isolate() command which, well, isolates whatever is in between the function’s parentheses from changes on the UI. Here, let us put input$bins into the isolate() function and check what happens when we move the slider (full code here), i.e. we changed\n\nbins <- seq(min(x), max(x), length.out = isolate(input$bins) + 1)\n\nExcellent! Nothing happens when we move the slider. Dumb and useless but excellent anyway.\nObserve that we could have also put the whole renderPlot() function call into isolate(). This app would work in the sense that we created valid code but then the reactivity of the slider is still active. The isolate() documentation hints at this with “…if you assign a variable inside the isolate(), its value will be visible outside of the isolate()”."
  },
  {
    "objectID": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#create-and-observe-buttons",
    "href": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#create-and-observe-buttons",
    "title": "6 simple Shiny things I have learned from creating a somewhat small app",
    "section": "Create and observe Buttons",
    "text": "Create and observe Buttons\nLet us bring back some reactivity to our app by adding a button that reevaluates our histogram when clicked. First, we will add a button to the UI. Second, we will implement what needs to happen on the server side of things when the button is clicked.\nThe first step is pretty simple. All we have to do is add actionButton() to the UI. Same as sliderInput() we have to specify a inputId and label for the button. Here, we could add\n\nactionButton(\"draw_button\", \"Reevaluate!\", width = \"100%\")\n\nThen, on the server side we will have to catch each click on the button. Once a click is registered, the plot is supposed to be rendered again. We do this with observeEvent() which expects an event expression and a handler expression. In our case, the former is simply the id of our button, i.e. input$draw_button, and the latter is what code is to be executed when the event is observed. Therefore, we move our code for rendering the plot into this part of observeEvent(). Thus, in our server function we now have\n\nobserveEvent(\n  input$draw_button, {\n    output$distPlot <- renderPlot({\n      x    <- faithful[, 2]\n      bins <- seq(min(x), max(x), length.out = isolate(input$bins) + 1)\n      hist(x, breaks = bins, col = 'darkgray', border = 'white')\n    })\n  }\n)\n\nNotice that we have wrapped our code into {}. Strictly speaking, this is not necessary because we only “do one thing” but, of course, we can easily imagine that we want to tie multiple calculations to a button click. In this case, we will need to wrap all commands into {}. In any case, our code now does what we expect it to do and on each click a new histogram is rendered using the current value of the slider input. This new app’s complete code can be found here."
  },
  {
    "objectID": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#use-eventreactive-as-an-alternative-for-updating-values",
    "href": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#use-eventreactive-as-an-alternative-for-updating-values",
    "title": "6 simple Shiny things I have learned from creating a somewhat small app",
    "section": "Use eventReactive() as an alternative for updating values",
    "text": "Use eventReactive() as an alternative for updating values\nHonestly, this part I learned just 5 minutes ago while I was writing the last section of this blog post. When I looked into the documentation of observeEvent(), I noticed that there is also a function eventReactive() which may be better suited for our current use case as it allows us to avoid manually isolating input$bins.\nThis new function works similar to observeEvent() but it creates a reactive variable instead. This, we can use for rendering. Check this out\n\nplot <- eventReactive(\n  input$draw_button, {\n    x    <- faithful[, 2]\n    bins <- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x, breaks = bins, col = 'darkgray', border = 'white')\n  }\n)\n\noutput$distPlot <- renderPlot({plot()})\n\nNotice how we do not use isolate() anymore and use the plot variable like a reactive in renderPlot(), i.e. we have to “call” its value with ().\nHowever, be aware that eventReactive() creates a reactive variable such that you cannot change, say, multiple plots at once. Nevertheless, eventReactive() can be a great way to tie a plot to an event. So, I guess it dependes on your use case and personal preference if you want to use eventReactive() rather than observeEvent(). Anyway, this version’s code can be copied from here."
  },
  {
    "objectID": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#use-reactiveval-to-manually-change-values-on-click",
    "href": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#use-reactiveval-to-manually-change-values-on-click",
    "title": "6 simple Shiny things I have learned from creating a somewhat small app",
    "section": "Use reactiveVal() to manually change values on click",
    "text": "Use reactiveVal() to manually change values on click\nAnother neat function is reactiveVal() which helps you to construct for instance counters that increase on the click of a button. We can initialize a reactive value by writing\n\ncounter <- reactiveVal(value = 0)\n\nwithin the server function. This way, our counter is set to zero and we can update it and set it to, say, one by calling counter(value = 1). The current value of the counter can be accessed through counter().\nClearly, we can tie the updating of a reactive value to an event that we observe through observeEvent(). For instance, we count how often the draw button in our small app is clicked by changing our previous observeEvent(input$draw_button, ...). Here, we would change this particular line of code to\n\nobserveEvent(\n  input$draw_button, {\n    tmp <- counter()\n    counter(tmp + 1)\n    \n    output$distPlot <- renderPlot({\n      x    <- faithful[, 2]\n      bins <- seq(min(x), max(x), length.out = isolate(input$bins) + 1)\n      hist(x, breaks = bins, col = 'darkgray', border = 'white')\n    })\n  }\n)\n\nFinally, we can show this information on our UI for demonstration purposes by adding a textOutput(\"demonstration_text\") to our UI and setting\n\noutput$demonstration_text <- renderText(paste(\n  \"You have clicked the draw button\",\n  counter(),\n  \"times. Congrats!\"\n))\n\nThe complete app can be found here."
  },
  {
    "objectID": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#use-tabsetpanel-and-unique-plot-names",
    "href": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#use-tabsetpanel-and-unique-plot-names",
    "title": "6 simple Shiny things I have learned from creating a somewhat small app",
    "section": "Use tabsetPanel and unique plot names",
    "text": "Use tabsetPanel and unique plot names\nOften, you do not want to display all information at once. In my particular case, I wanted to show only one out of two plots based on the user’s chosen estimator (sample mean or sample variance). A great way to achieve that is to use tabsetPanel() in the UI.\nOrdinarily, you can create a UI this way by setting\n\nmainPanel(\n  tabsetPanel(\n    tabPanel(\"Plot\", plotOutput(\"plot\")),\n    tabPanel(\"Summary\", verbatimTextOutput(\"summary\")),\n    tabPanel(\"Table\", tableOutput(\"table\"))\n  )\n)\n\nThis was an example taken straight out of the documentation of tabsetPanel(). What you will get if you start an app containing a UI like this is a panel with three tabs (each one corresponding to a plot, text or table output) and the user can click on the tabs to switch between the views. This isn’t that surprising.\nHowever, if we also add an id to this and set type to hidden, like so\n\nmainPanel(\n  tabsetPanel(\n    id = \"my_tabs\",\n    type = \"hidden\",\n    tabPanel(\"Plot\", plotOutput(\"plot\")),\n    tabPanel(\"Summary\", verbatimTextOutput(\"summary\")),\n    tabPanel(\"Table\", tableOutput(\"table\"))\n  )\n)\n\nthen, by default, the user does not have the options to change between views by clicking on tabs. Now, the view will need to change based on other interactions of the user with the UI. This change will then need to be customized within the server function. This is where the id argument comes into play because it allows ourselves to address the tabs via updateTabsetPanel().\nHere, let us take our previous example and display the same information on a different panel, i.e. at the end we will have two panels with exactly the same information in each tab. I know. This is not particularly exciting or meaningful but it serves our current purpose well.\nNaively, we might implement our user-interface like so\n\nmainPanel(\n  tabsetPanel(\n    id = \"my_tabs\",\n    type = \"hidden\",\n    tabPanel(\"panel1\", {\n      # UI commands from before here\n    }),\n    tabPanel(\"panel2\", {\n      # UI commands from before here\n    }),\n  )\n)\n\nHowever, we will have to be careful! If we simply copy-and-paste our UI from before, then we won’t have unique identifiers to address e.g. the draw button or the plot output. Since this is a serious NO-NO (all caps for dramatic effect) and the app won’t work properly, let us instead write a function that draws the UI for us but creates it with different identifiers like this\n\ncreate_UI <- function(unique_part) {\n  sidebarLayout(\n    sidebarPanel(\n      # unique label here by adding unique_part to bins\n      sliderInput(paste(\"bins\", unique_part, sep = \"_\"),\n                  \"Number of bins:\",\n                  min = 1,\n                  max = 50,\n                  value = 30),\n      actionButton(paste(\"draw_button\", unique_part, sep = \"_\"), \"Reevaluate!\", width = \"100%\"),\n      actionButton(paste(\"change_view\", unique_part, sep = \"_\"), \"Change view\", width = \"100%\")\n    ),\n    \n    mainPanel(\n      textOutput(paste(\"demonstration_text\", unique_part, sep = \"_\")), # Counter text added\n      textOutput(paste(\"countEvaluations\", unique_part, sep = \"_\")),\n      plotOutput(paste(\"distPlot\", unique_part, sep = \"_\"))\n    )\n  )\n}\n\nAlso, notice that I have created another button called “Change view” within the UI. Further, this button’s name is so mind-baffling that I won’t even try to elaborate what it will do. Finally, using create_UI, we can set up the UI like so\n\nmainPanel(\n  tabsetPanel(\n    id = \"my_tabs\",\n    selected = \"panel1\",\n    type = \"hidden\",\n    tabPanel(\"panel1\", create_UI(\"panel1\")),\n    tabPanel(\"panel2\", create_UI(\"panel2\")),\n  )\n)\n\nand address everything within the UI in a unique manner. Of course, such a functional approach only works well if the two panels look sufficiently similar such that it makes sense to design them through a single function. In my particular app that deals with the variance of estimators, this was the case because the tabs for the sample mean and sample variance were quite similar in their structure.\nNow that we have covered how the UI needs to be set up, let me show you how to change the view from one panel to the next. Shockingly, let us link this to a click on the “change view” button(s) like so\n\nobserveEvent(\n  input$change_view_panel1, \n  updateTabsetPanel(inputId = \"my_tabs\", selected = \"panel2\")\n)\nobserveEvent(\n  input$change_view_panel2, \n  updateTabsetPanel(inputId = \"my_tabs\", selected = \"panel1\")\n)\n\nAlso, note that the previous code\n\nobserveEvent(\n  input$draw_button, {\n    tmp <- counter()\n    counter(tmp + 1)\n    \n    output$distPlot <- renderPlot({\n      x    <- faithful[, 2]\n      bins <- seq(min(x), max(x), length.out = isolate(input$bins) + 1)\n      hist(x, breaks = bins, col = 'darkgray', border = 'white')\n    })\n  }\n)\n\nwon’t work anymore because the old identifiers like draw_button etc. need to be updated to draw_button_panel1 or draw_button_panel2. Clearly, this could potentially require some code duplication to implement the server-side logic for both tabs. But since we feel particularly clever today1, let us write another function that avoids a lot of code duplication.\n\nrender_my_plot <- function(panel, counter, input, output) {\n  tmp <- counter() # save current value of counter\n  counter(tmp + 1) # update counter\n  \n  # Create identifier names\n  bins_name <- paste(\"bins\", panel, sep = \"_\")\n  distplot_name <- paste(\"distPlot\", panel, sep = \"_\")\n  demonstration_text <- paste(\"demonstration_text\", panel, sep = \"_\")\n  \n  # Render Plot\n  output[[distplot_name]] <- renderPlot({\n    x    <- faithful[, 2]\n    bins <- seq(min(x), max(x), length.out = isolate(pluck(input, bins_name)) + 1)\n    hist(x, breaks = bins, col = 'darkgray', border = 'white')\n  })\n  \n  # Render counter text\n  output[[demonstration_text]] <- renderText(paste(\n    \"You have clicked the draw button\",\n    counter(),\n    \"times. Congrats!\"\n  ))\n}\n\nNotice a few things here: - Our function needs to know the objects counter, input and output to work. - Also we need to switch to double-bracket notation for assigning new variables like distPlot_panel1 to output. Obviously, we couldn’t use $ for assignment anymore but single-bracket notation like output[var_name] is for some reason forbidden in Shiny. At least, that’s what an error message will kindly tell you when you dare to use only one bracket.\nSo, all in all our server-side logic looks like this now\n\nserver <- function(input, output) {\n  # Counter initialization\n  counter <- reactiveVal(value = 0)\n  counter2 <- reactiveVal(value = 0)\n  \n  # Plot Rendering\n  observeEvent(\n    input$draw_button_panel1, {\n      render_my_plot(\"panel1\", counter, input, output)\n    }\n  )\n  observeEvent(\n    input$draw_button_panel2, {\n      render_my_plot(\"panel2\", counter2, input, output)\n    }\n  )\n  \n  # Panel Switching\n  observeEvent(\n    input$change_view_panel1, \n    updateTabsetPanel(inputId = \"my_tabs\", selected = \"panel2\")\n  )\n  observeEvent(\n    input$change_view_panel2, \n    updateTabsetPanel(inputId = \"my_tabs\", selected = \"panel1\")\n  )\n}\n\nThe complete app that we have just build can be found here."
  },
  {
    "objectID": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#closing",
    "href": "posts/06_shiny_app_learnings/06_shiny_app_learnings.html#closing",
    "title": "6 simple Shiny things I have learned from creating a somewhat small app",
    "section": "Closing",
    "text": "Closing\nAlright, I hope this helps you to build your own small Shiny app. In my particular case, I had to use another cool function from the shinyjs package to update the text on the UI such that it appears in red for a second (in order for the user to notice what changes). And because I have the feeling that shinyjs has way more in store for us, I will end this already quite long blog post here and save that (exciting) story for another time. Hope you will be there when I talk about shinyjs."
  },
  {
    "objectID": "posts/08_drawing_ggplot_interactively/08_drawing_ggplot_interactively.html",
    "href": "posts/08_drawing_ggplot_interactively/08_drawing_ggplot_interactively.html",
    "title": "Interactive ggplots, user feedback, and a little bit of Javascript magic with Shiny",
    "section": "",
    "text": "I’ve been reading Mastering Shiny by Hadley Wickham lately and one of the things that intrigued me is that you can make ggplots interactive. Though I believe that there are limitation to the level of interactiveness compared to using, say, plotly, I really wanted to practice interactive ggplots with Shiny. Naturally, I build a Shiny app to figure things out. Here’s a demonstration of what the app can do. The rest of this chapter teaches you how some parts of the app were implemented."
  },
  {
    "objectID": "posts/08_drawing_ggplot_interactively/08_drawing_ggplot_interactively.html#working-with-clicks",
    "href": "posts/08_drawing_ggplot_interactively/08_drawing_ggplot_interactively.html#working-with-clicks",
    "title": "Interactive ggplots, user feedback, and a little bit of Javascript magic with Shiny",
    "section": "Working with clicks",
    "text": "Working with clicks\nIf you have build at least one Shiny app, then you are probably aware that you can include plots on the UI with plotOutput(). (If, in fact, you have never touched Shiny, then feel free to check out how I thought my students the basics of Shiny.) But what you may not know, is that apart from its outputId, width and height arguments, this output function also uses arguments like click and dblclick. These are the secrets to unlocking interactiveness.\nImagine that you have a user interface that includes a plot output via\n\nplotOutput('awesome_plot', click = 'awesome_click')\n\nNow, what this small additional argument gives you is a way to access the coordinates of something the user clicks on. What you will have to do is to observe input$awesome_click. Here’s a minimal example of how that works.\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nui <- fluidPage(\n  plotOutput('awesome_plot', click = 'awesome_click'),\n)\n\nserver <- function(input, output, session) {\n  # Create dummy data as reactive value\n  my_dat <- reactiveVal(tibble(x = 3, y = 4, msg = 'Click me'))\n  # Render plot with fixed coordinate system\n  output$awesome_plot <- renderPlot({\n    ggplot(data = my_dat()) +\n      geom_text(aes(x, y, label = msg), size = 15, vjust = 0, hjust = 0) +\n      coord_cartesian(xlim = c(0, 7), ylim = c(0, 8))\n  })\n  # Update dummy data on click \n  observeEvent(input$awesome_click, {\n    my_dat(\n      my_dat() %>% \n        mutate(\n          x = input$awesome_click$x,\n          y = input$awesome_click$y,\n          msg = if (runif(1) < 0.5) 'I like that. Do it again.' else 'Stop that!'\n        )\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nThis will give us the following app.\n\n\n\n\n\nLike a cat, this app is a master of mixed signals and wants to be touched but only a random amount of times. Unlike a cat, the app will show you a plot displaying its latest message at the most recently clicked spot.\nAll of this is powered by observing changes in input$awesome_click and then using this list’s new x- and y-values to update the reactive value my_dat that underlies the plot. Notice that I have fixed the axes of the plot because otherwise the message will always be displayed in the middle of the plot. After all, the plot will be entirely rebuilt using new underlying data. Fundamentally, this is how I build the ‘color my voronoi’ from above.\nBut, of course, I have tried out more stuff like user feedback and even some javascript magic. Stick around if you want to learn these ancient skills as well. Destiny is calling."
  },
  {
    "objectID": "posts/08_drawing_ggplot_interactively/08_drawing_ggplot_interactively.html#let-your-user-know-that-he-messed-up-and-stop-him-before-its-too-late",
    "href": "posts/08_drawing_ggplot_interactively/08_drawing_ggplot_interactively.html#let-your-user-know-that-he-messed-up-and-stop-him-before-its-too-late",
    "title": "Interactive ggplots, user feedback, and a little bit of Javascript magic with Shiny",
    "section": "Let your user know that he messed up and stop him before it’s too late",
    "text": "Let your user know that he messed up and stop him before it’s too late\nTo my surprise, UI elements like numericInput() do not actually check that an input is valid even though there are arguments like min and max. Of course, a user may end up giving wrong inputs that your app can’t handle. We can’t have that now, can we?\nWe will need to stop that insubordinate and churlish behavior immediately. In case you recognized that combination of ‘insubordinate’ and ‘churlish’, then I will have you now, yes, this is a reference to Mr. Garvey and the rest of this section is a homage to a skit that makes me giggle every time.\nSo, let’s build an app that works as follows:\n\n\n\n\n\nThe notifications in this app are all powered through the shinyFeedback package. In order to activate its powers, drop a shinyFeedback::useShinyFeedback() in the UI like so.\n\nlibrary(shiny)\nlibrary(dplyr)\n\nui <- fluidPage(\n  shinyFeedback::useShinyFeedback(),\n  h3('A Day with Mr. Garvey'),\n  textInput(\n    'name',\n    'What\\'s your name?',\n  )\n)\n\nThen, you are all set up to activate warnings and notifications by your server function. Here is a simplified version of the app’s remaining code.\n\nnames <- c('Jay Quellin','Jacqueline', 'Balakay', 'Blake', 'Dee-nice', 'Denise',\n           'Ay-Ay-Ron', 'Aaron')\nreturn_msg <- function(name) {\n  case_when(\n    name == 'Balakay' ~ 'My name is Blake.',\n    name == 'Blake' ~ 'Do you wanna go to war, Balakay? You better check yourself!',\n    name == 'Jay Quellin' ~ 'Do you mean Jacqueline?',\n    name == 'Jacqueline' ~ 'So that\\'s how it\\'s going to be. I got my eye on you Jay Quellin!',\n    name == 'Dee-nice' ~ 'Do you mean Denise?',\n    name == 'Denise' ~ 'You say your name right!',\n    name == 'Ay-Ay-Ron' ~ 'It is pronounced Aaron.',\n    name == 'Aaron' ~ 'You done messed up Ay-Ay-Ron!'\n  )\n}\n\nserver <- function(input, output, session) {\n  name_input <- reactive(input$name) \n  observeEvent(name_input(), {\n    shinyFeedback::feedbackDanger(\n      'name',\n      show = (name_input() %in% names),\n      text = return_msg(name_input())\n    )\n    \n    shinyFeedback::feedbackSuccess(\n      'name',\n      show = !(name_input() %in% names),\n      text = 'Thank you!'\n    )\n  })\n}\n\nshinyApp(ui, server)\n\nAs you can see, the feedback functions work with\n\nthe name of an input,\na rule when to show up and\na text to display.\n\nThis code is pretty straightforward but, unfortunately, this app does not work like the one you have seen above. There are a couple of problems:\n\nFor starters, if you start the app, then the text input is empty, so !(name_input() %in% names) is true and the app will immediately display “Thank you!”\nAfter you write a name on the list of defined names, then no warning will be displayed. That is because the later feedbackSuccess() will always command that there is nothing to display.\n\nTherefore, we need something that stops the feedbackSuccess() from executing when we don’t need it. This can be achieved through the little but powerful req() function. It checks that all given conditions are met or stops the execution where it is. In this case, you will need to drop req(name_input(), !(name_input() %in% names)) in front of feedbackSuccess().\nSmall technical detail: Notice that name_input() will be '' in the beginning. Technically, this is not a boolean but that doesn’t matter to Shiny. What matters is that '' is not “truthy”. See ?isTruthy for more details.\nNow, even with this small change. Our app won’t run smoothly because sometimes the notifaction will not change from “success” to “danger”. This is is because sometimes the notifaction needs to be reseted to work with new notifications. Therefore, a hideFeedback() is in order.\nAlso, if you are not fast at typing, then a notification might already show up, when you are still typing. It is rude to interrupt our kind user like this. Therefore, let’s make sure our app waits a little before giving out notifications. We can let out app wait for a defined amount of milliseconds by sending our reactive name_input() to debounce(). In total, our server function now looks like this.\n\nserver <- function(input, output, session) {\n  name_input <- reactive(input$name) %>% debounce(250)\n  observeEvent(name_input(), {\n    shinyFeedback::hideFeedback('name')\n    \n    shinyFeedback::feedbackDanger(\n      'name',\n      show = (name_input() %in% names),\n      text = return_msg(name_input())\n    )\n    \n    req(name_input(), !(name_input() %in% names))\n    shinyFeedback::feedbackSuccess(\n      'name',\n      show = !(name_input() %in% names),\n      text = 'Thank you!'\n    )\n  })\n}\n\nFinally, let me mention that, within the function req(), it is also possible to set cancelOutput = TRUE. This stops the code execution as usual but avoids destroying previously displayed outputs."
  },
  {
    "objectID": "posts/08_drawing_ggplot_interactively/08_drawing_ggplot_interactively.html#sprinkle-some-javascript-magic-on-top-of-your-app",
    "href": "posts/08_drawing_ggplot_interactively/08_drawing_ggplot_interactively.html#sprinkle-some-javascript-magic-on-top-of-your-app",
    "title": "Interactive ggplots, user feedback, and a little bit of Javascript magic with Shiny",
    "section": "Sprinkle some javascript magic on top of your app",
    "text": "Sprinkle some javascript magic on top of your app\nFor my final trick before I disappear into the ether, let me show you a little bit of javascript. As I, myself, do not know much about JS, I am particularly proud that I included some of that web magic into my voronoi coloring app. ‘What did you do?’, you ask? Well, did you notice that the colour dropdown menu in the voronoi app contains the actual colors next to the color names? That is some JS magic right there! Impressive, I know.\nTo make that work, I had to use the options argument of selectizeInput() together with the render() function and some actual JS code. The whole thing is adapted from this SO post and looks like this:\n\ncol2hex <- gplots::col2hex\ncolorValues <- colors()\ncolorNames <- glue::glue(\"{colorValues} <span style='background-color:{col2hex(colorValues)}'>{rep('&nbsp;', 15) %>% stringr::str_c(collapse = '')}</span>\")\ncolors <- setNames(colorValues, colorNames)\njs_render_string <- I(\"\n  {\n    item: function(item, escape) { return '<div>' + item.label + '</div>'; },\n    option: function(item, escape) { return '<div>' + item.label + '</div>'; }\n  }\")\nselectizeInput(\n  \"color\",\n  \"Colour\",\n  selected = 'grey80',\n  choices = colors,\n  options = list(render = js_render_string)\n)\n\nLet’s untangle this step by step. The first part of this code gives us a vector colors containing the color names like “white” and “aliceblue” as values. The same vector also uses names for the vector elements that will be displayed to the user. In principal, this colors vector looks like this:\n\nx <- c('white', 'aliceblue') # no names\nx\n\n[1] \"white\"     \"aliceblue\"\n\nx <- setNames(x, c('name1', 'name2')) # with names\nx\n\n      name1       name2 \n    \"white\" \"aliceblue\" \n\nx['name1'] # named vectors can be used like dictionaries\n\n  name1 \n\"white\" \n\n\nIn our color example, instead of using arbitrary names, I converted the color names to their hexvalues like #FFFFFF and wrapped those in some HTML code that could potentially look like \"<span style='background-color#FFFFFF'>white</span>\". This corresponds to the word white with background color #FFFFFF (also white - unspectacular).\nBut in the actual app I wanted to have colored bars next to the color names. Thus, I have used the HTML code for white space &nbsp; and made this into \"white <span style='background-color#FFFFFF'>&nbsp;</span>\". Now, to make that color bar longer, I repeated whited space with rep() and glued those into a single string using stringr::str_c(). This is what the vector looks like if I only use two white space repeats.\n\nlibrary(dplyr, warn.conflicts = F)\ncol2hex <- gplots::col2hex\ncolorValues <- colors()\ncolorNames <- glue::glue(\"{colorValues} <span style='background-color:{col2hex(colorValues)}'>{rep('&nbsp;', 2) %>% stringr::str_c(collapse = '')}</span>\")\ncolors <- setNames(colorValues, colorNames)\ncolors[1]\n\nwhite <span style='background-color:#FFFFFF'>&nbsp;&nbsp;</span> \n                                                         \"white\" \n\ncolors[2]\n\naliceblue <span style='background-color:#F0F8FF'>&nbsp;&nbsp;</span> \n                                                         \"aliceblue\" \n\n\nIn the dropdown menu of the app the user will see the names of the color vector, i.e. the HTML code and within the server function of our app the selection will then correspond to the actual value of the vector, i.e. the color name without the html stuff.\nIn our dummy example from above, the user would see name1 and name2 in the dropdown menu but within the server function a user’s selection would correspond to input$color which would evaluate to white or aliceblue.\nClearly, we don’t want the user to see the raw HTML code. This is where JS comes into play. The code that is stored in js_render_string evaluates the HTML code in order to display the actual colors instead of the raw code. Finally, to execute the JS code we need to pass it to the options of selectizeInput via options = list(render = js_render_string).\nThere you go, this is how I created the color bars in my app using a JS snippet I found on Stackoverflow. You can find the complete codes of the apps we’ve build here (click app), here (notification names app) and here (voronoi coloring app). If you liked this post and want to see more Shiny posts, let me know in the comments or simply hit the applause button below. Of course, you can also always follow my work via Twitter."
  },
  {
    "objectID": "ggplot-series.html",
    "href": "ggplot-series.html",
    "title": "Series: ggplot2-tips",
    "section": "",
    "text": "We build rebuild a ‘Storytelling with Data’ plot which uses rounded rectangles. I’ll show you an easy and a hard way to make rectangles round.\n\n\n\n\n\n\nMay 4, 2022\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nWe try to imitate the Storytelling with Data look with ggplot\n\n\n\n\n\n\nMar 29, 2022\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nFunctional programming is a mighty sword. Today, we use it to avoid tedious repetitions when things go wrong in ggplot.\n\n\n\n\n\n\nMar 25, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a short tutorial on how to import fonts and icons in R using the showtext package.\n\n\n\n\n\n\nMar 4, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nInspired by a datawrapper blogpost, we explore how to work with fewer colors in ggplot.\n\n\n\n\n\n\nFeb 19, 2022\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nI advocate to take part in the TidyTuesday events to learn with and from others.\n\n\n\n\n\n\nJan 10, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe ggplot2-tips series is continued with a few example plots from the ggforce package\n\n\n\n\n\n\nDec 31, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe patchwork and ggforce packages can be used to compose plots from multiple subplots. Let’s have a look at how that works.\n\n\n\n\n\n\nOct 28, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nWe take a look at the differences between position = ‘stack’ and position = position_stack().\n\n\n\n\n\n\nSep 11, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nWe talk about how to easily create labels for an aesthetic.\n\n\n\n\n\n\nAug 19, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nThis is the beginning of a series about a few ggplot2 tricks I picked up along the way. In this first installment we talk about how logarithmizing scales can be beneficial.\n\n\n\n\n\n\nAug 7, 2021\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  }
]