{
  "hash": "66d61548da5a427136c1b12fee6c8e8a",
  "result": {
    "markdown": "---\ntitle: \"The ultimate beginner's guide to generalized linear models (GLMs)\"\ndate: '2022-08-12'\ncategories: [\"Statistics\", \"ML\"]\ndescription: \"This is an beginner's guide on GLMs. We cover the mathematical foundations as well as how to implement GLMs with R. The implementations are done with and without `{tidymodels}`.\"\nexecute: \n  message: false\n  warning: false\n  collapse: false\n  fig-width: 7\n  fig-align: center\neditor_options: \n  chunk_output_type: console\neditor: \n  markdown: \n    wrap: sentence\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\nYou may have never heard about generalized linear models (GLMs).\nBut you've probably heard about logistic regression or Poisson regression.\nBoth of them are special cases of GLMs.\n\nThere are even more special cases of GLMs.\nThat's because GLMs are versatile statistical models.\nAnd in this blog post we're going to explore the mathematical foundations of these models.\nActually, this post is based on one of my recent [Twitter threads](https://twitter.com/rappa753/status/1538156165535760384).\nYou can think of this post as the long form version of that thread.\n\nHere, I'll add a few more details on the mathematical foundations of GLMs.[^1]\nMore importantly, though, I will show you how to implement GLMs with R.\nWe'll learn both the `{tidymodels}` and the `{stats}` way of doing GLMs.\nSo without further ado, let's go.\n\n[^1]:  Of course I can't cover everything.\n    This is a beginner's guide after all.\n    For more details let me refer to the excellent book [@fahrmeir2013].\n\n## Logistic regression\n\nLet's start with logistic regression.\nAssume that you have the following data about penguins.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 333 Ã— 4\n   sex    body_mass_g species bill_length_mm\n   <fct>        <int> <fct>            <dbl>\n 1 male          3750 Adelie            39.1\n 2 female        3800 Adelie            39.5\n 3 female        3250 Adelie            40.3\n 4 female        3450 Adelie            36.7\n 5 male          3650 Adelie            39.3\n 6 female        3625 Adelie            38.9\n 7 male          4675 Adelie            39.2\n 8 female        3200 Adelie            41.1\n 9 male          3800 Adelie            38.6\n10 male          4400 Adelie            34.6\n# â€¦ with 323 more rows\n# â„¹ Use `print(n = ...)` to see more rows\n```\n:::\n:::\n\n\nImagine that your goal is to classify penguins as male or female based on the other variables `body_mass_g`, `species` and `bill_length_mm`.\nBetter yet, let's make this specific.\nHere's a dataviz for this exact scenario.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14_GLMS_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nAs you can see, the male and female penguins form clusters that do not overlap too much.\nHowever, regular linear regression won't help us to distinguish them.\nThink about it.\nIts output is something numerical.\nHere, we want to find classes.\n\nSo, how about trying to predict a related numerical quantity then?\nLike a probability that a penguin is male.\nCould we convert the classes to 0 and 1 and then run a linear regression?\nWell, we could.\nBut this won't give us probabilities either.\nWhy?\nBecause predictions are not restricted to $[0, 1]$.\n\nBut I suspect you're REALLY determined to use linear regression.\nAfter all, what have you learned ordinary least squares (OLS) for if not for using it everywhere?\nSo, what saves you from huge predictions?\nThat's the glorious logistic function (applied to linear regression's predictions).\nIt looks like this.\n\n\n::: {.cell fig.showtext='true'}\n\n```{.r .cell-code}\ntibble(x = seq(-10, 10, 0.1), y = plogis(x)) %>% \n  ggplot(aes(x, y)) +\n  geom_line(color = thematic::okabe_ito(3)[3], size = 2) +\n  theme_minimal(base_size = 14) +\n  theme(panel.grid.minor = element_blank())\n```\n\n::: {.cell-output-display}\n![](14_GLMS_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nI've applied this strategy to our data to \"predict\" probabilities.\nThen, I used a 50% threshold for classification.\n\nNote that I have chosen this threshold here only for demo purposes.\nIn general, 50% is not a good threshold.\nThat's because there are many situations where you want your model to be **really** sure before it makes a classification.\nFor example, with malignant tumor detection we want to use a threshold that is different from a coin flip.\nClearly, we want to be sure that a tumor is dangerous before we undergo surgery.\n\nSo, back to our prediction strategy.\nAgainst all odds we've run a linear regression to \"predict\" probabilities and classified on this 50% threshold.\nDoes this give us good results?\nHave a look for yourself.\n\n::: panel-tabset\n### Predicted probabilities\n\n\n::: {.cell fig.showtext='true'}\n::: {.cell-output-display}\n![](14_GLMS_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### Correct classifications?\n\n\n::: {.cell fig.showtext='true'}\n::: {.cell-output-display}\n![](14_GLMS_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n:::\n\nThe predictions for male and female penguins overlap quite a lot.\nThis leads to many incorrect classifications.\nNot bueno.\nAt this point, you may as well have trained a model that answers \"Is this a male penguin?\" with [\"Nope, just Chuck Testa\"](https://www.youtube.com/watch?v=LJP1DphOWPs).\n\nSo, our classification is bad.\nAnd hopefully I've convinced you that OLS isn't the way to go here.\nBut what now?\n\nWell, it wasn't all bad.\nThe idea of linking a desired quantity (like a probability) to a linear predictor is actually what GLMs do.\nTo make it work, let's take a step back.\n\nUsually, we model our response variable $Y$ by decomposing it into\n\n-  a deterministic function $f(X_1,..., X_n)$ dependent on predictors $X_1, \\ldots, X_p$, $p \\in \\mathbb{N}$, plus\n-  a random error term\n\nThus, regression is nothing but finding a function describing the **average** outcome.\nWith a little change in notation this becomes clearer:\n\n\n$$\n\\begin{align*}\nY_i &= f(X_1, \\ldots, X_p) + \\varepsilon_i \\\\[2mm]\n&= \\mathbb{E}[Y_i | X_1, \\ldots, X_p] + \\varepsilon_i, \\quad i = 1, \\ldots, n\n\\end{align*}\n$$\n\n\nIn linear regression, this deterministic function is given by a linear predictor.\nWe will denote this linear predictor by $\\eta_i(\\beta)$ Note that it depends on a parameter $\\beta \\in \\mathbb{R}^{p + 1}$.\n\n\n$$\n\\eta_i(\\beta) = \\beta_0 + x_{i, 1}\\beta_1 + \\cdots + x_{i, p} \\beta_p\n$$\n\n\nAlright, we've emphasized that we're really trying to model an expectation.\nNow, think about what we're trying to predict.\nWe're looking for probabilities, are we not?\n\nAnd do we know a distribution whose expectation is a probability?\nBingo!\nWe're thinking about Bernoulli.\nTherefore, let us assume that our response variable $Y$ is Bernoulli-distributed (given our predictors), i.e. $Y_i | X_1, \\ldots, X_p \\sim \\text{Ber}(\\pi_i)$.\n\nAnd now we're back with our idea to link the average outcome to a linear predictor via a suitable transformation (logistic function).\nThis sets up our model.\nIn formulas, this is written as\n\n\n$$\n\\mathbb{E}[Y_i | X_1, \\ldots, X_p] = \\pi_i = h\\big(\\eta_i(\\beta)\\big)\n$$\n\n\nwhere\n\n\n$$\nh(x) = \\frac{e^{\\text{x}}}{1 + e^x}.\n$$\n\n\nYou're thinking we've tried this already, aren't you?\nHow will we get different results?\nIsn't this new setup just semantics?\nTheoretic background is useless in practice, right?\n(I've actually heard someone say that to a speaker at a scientific workshop. A shitshow ensued.)\n\nPreviously, we used the OLS estimator to find the linear predictor's parameter $\\beta$.\nBut with our new model setup comes a new way of estimating $\\beta$.\nTake a look.\nCompare the results of using the OLS estimator with what we get when we maximize the so-called likelihood.\n\n::: panel-tabset\n### Transforming OLS estimates\n\n\n::: {.cell fig.showtext='true'}\n::: {.cell-output-display}\n![](14_GLMS_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n### Maximizing likelihood\n\n\n::: {.cell fig.showtext='true'}\n::: {.cell-output-display}\n![](14_GLMS_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n:::\n\nMuch fewer incorrect results, right?\nAnd that's despite having used the same 50% threshold once I predicted probabilites.\nThis means that maximizing the likelihood delivers a way better estimator.\nLet's see how that works.\n\nThe **likelihood function** $L$ is the product of the densities of the assumed distribution of Y given the predictors (here Bernoulli).\nThis makes it the joint probability of the observed data.\nIn formulas, this is\n\n\n$$\nL(\\beta) := \\prod_{i = 1}^n f(y_i | \\beta) = \\prod_{i = 1}^n \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i}.\n$$\n\n\nWe estimate $\\beta$ by maximizing this function or equivalently (but easier) its logarithm\n\n\n$$\nl(\\beta) \n:= \n\\log L(\\beta) \n= \n\\sum_{i = 1}^n \\bigg[ \n  \\underbrace{y_i \\log \\bigg( \n    \\frac{\\pi_i}{1 - \\pi_i}\n  \\bigg) \n  + \n  \\log (1 - \\pi_i)}_{=: l_i(\\beta)}\n\\bigg].\n$$\n\n\nHow do we find this maximum?\nBy using the same strategy as for any other function that we want to maximize: Compute the first derivative and find its root.\nThat's why it's easier to maximize the log-likelihood (sums are easier to differentiate than products).\n\nIn this context, the first derivative[^2] is also known as **score fct**\n\n[^2]: For detailed calculations of the score function and any other formula, I recommend that you refer to [@fahrmeir2013].\n\n\n$$\ns(\\beta)\n:=\n\\frac{\\partial l(\\beta)}{\\partial \\beta}\n=\n\\sum_{i = 1}^n \\frac{\\partial l_i(\\beta)}{\\partial \\beta}\n=\n\\sum_{i = 1}^n s_i(\\beta),\n$$\n\n\nwhere\n\n\n$$\n\\begin{align*} \n  s_i(\\beta) &= x_i(y_i - \\pi_i) = x_i\\big(y_i - h(x_i^T \\beta)\\big)\\quad \\text{and} \\\\[2mm]\n  h(x) &= \\frac{e^{\\text{x}}}{1 + e^x}\n\\end{align*}\n$$\n\n\nHere, finding a root is hard because no analytical solutions exist.\nThus, we'll have to rely on numerical methods.\n\n### Newton's method\n\nA well-known procedure is Newton's method.\nIn each iteration it tries to get closer to a function's root by moving along its gradient.\nHave a look at this GIF from [Wikipedia](https://en.wikipedia.org/wiki/Newton's_method#/media/File:NewtonIteration_Ani.gif).\nIt shows how Newton's method works for a univariate function.\nBut the same ideas work in higher dimensions, i.e. in $\\mathbb{R}^n$ where $n \\in \\mathbb{N}$.\n\n![](NewtonIteration_Ani.gif)\n\nAlright, let me unwrap what you see here.\nIn each iteration, Newton's method computes $f(x_k)$, i.e. the function's value at $x_k$ (the current value).\nThen, it uses the function's derivative $f^{\\prime}$ to find the tangent line of $f$ at the current position $x_k$.\nOnce the tangent line is found, our new position $x_{k + 1}$ is determined by the intersection of the tangent line and the $x$-axis.\nAll of this can be summarized via\n\n\n$$\nx_{k + 1} = f(x_k) - (f^{\\prime}(x_k))^{-1} f(x_k).\n$$\n\n\nAs I said, this can be generalized for vector-valued functions $f$.\nIn that case, $f^{\\prime}$ is a matrix (the so called [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix)) and $(f^{\\prime})^{-1}$ is the inverse of that matrix.\nLike the one-dimensional derivative the Hessian matrix is dependent of the current position $x_k$.\n\nIn the context of GLMs, the current position $x_k$ is usually denoted with $\\beta_{k}$ and the Hessian\n\n\n$$\n\\mathcal{J}(\\beta) = - \\frac{\\partial^2 l(\\beta)}{\\partial \\beta \\partial \\beta^T}\n$$\n\n\nis called **observed information Matrix**.\nOnce again we can summarize Newton's algorithm in one single line by\n\n\n$$\n\\beta_{t + 1} = \\beta_{t} + \\mathcal{J}^{-1}(\\beta_{t}) s(\\beta_{t}).\n$$\n\n\n## Poisson regression\n\nCongrats!\nYou've brushed up on one example of GLMs, namely logistic regression.\nBut GLMs wouldn't be general if that were all.\n\nDepending on the assumed distribution and the function that links the linear predictor to the expectation, GLMs have many names.\n**Poisson regression** is another one.\n\nPoisson regression is a GLM which assumes that Y follows a Poisson distribution (who would have seen that one coming), i.e. $Y_i | X_1, X_2, X_3 \\sim \\text{Poi}(\\lambda_i)$, $\\lambda_i > 0$.\nA suitable link function is given by the exponential function\n\n\n$$\n\\mathbb{E}[Y_i | X_1, X_2, X_3] = \\lambda_i = \\exp\\big(\\eta_i(\\beta)\\big).\n$$\n\n\nThis model is used when you try to estimate count data like this.\n\n\n::: {.cell fig.showtext='true'}\n::: {.cell-output-display}\n```{=html}\n<div id=\"bgemewavzm\" style=\"overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>html {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#bgemewavzm .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 12pt;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: 80%;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#bgemewavzm .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#bgemewavzm .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#bgemewavzm .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#bgemewavzm .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#bgemewavzm .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#bgemewavzm .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#bgemewavzm .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#bgemewavzm .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#bgemewavzm .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#bgemewavzm .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#bgemewavzm .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#bgemewavzm .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#bgemewavzm .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#bgemewavzm .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#bgemewavzm .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#bgemewavzm .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#bgemewavzm .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#bgemewavzm .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#bgemewavzm .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#bgemewavzm .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#bgemewavzm .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#bgemewavzm .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#bgemewavzm .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#bgemewavzm .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#bgemewavzm .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#bgemewavzm .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#bgemewavzm .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#bgemewavzm .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-left: 4px;\n  padding-right: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#bgemewavzm .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#bgemewavzm .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#bgemewavzm .gt_left {\n  text-align: left;\n}\n\n#bgemewavzm .gt_center {\n  text-align: center;\n}\n\n#bgemewavzm .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#bgemewavzm .gt_font_normal {\n  font-weight: normal;\n}\n\n#bgemewavzm .gt_font_bold {\n  font-weight: bold;\n}\n\n#bgemewavzm .gt_font_italic {\n  font-style: italic;\n}\n\n#bgemewavzm .gt_super {\n  font-size: 65%;\n}\n\n#bgemewavzm .gt_two_val_uncert {\n  display: inline-block;\n  line-height: 1em;\n  text-align: right;\n  font-size: 60%;\n  vertical-align: -0.25em;\n  margin-left: 0.1em;\n}\n\n#bgemewavzm .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 75%;\n  vertical-align: 0.4em;\n}\n\n#bgemewavzm .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#bgemewavzm .gt_slash_mark {\n  font-size: 0.7em;\n  line-height: 0.7em;\n  vertical-align: 0.15em;\n}\n\n#bgemewavzm .gt_fraction_numerator {\n  font-size: 0.6em;\n  line-height: 0.6em;\n  vertical-align: 0.45em;\n}\n\n#bgemewavzm .gt_fraction_denominator {\n  font-size: 0.6em;\n  line-height: 0.6em;\n  vertical-align: -0.05em;\n}\n</style>\n<table class=\"gt_table\">\n  \n  <thead class=\"gt_col_headings\">\n    <tr>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\">Marijuana</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\">Cigarette</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\">Alcohol</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\">Count</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td class=\"gt_row gt_left\">yes</td>\n<td class=\"gt_row gt_left\">yes</td>\n<td class=\"gt_row gt_left\">yes</td>\n<td class=\"gt_row gt_right\">911</td></tr>\n    <tr><td class=\"gt_row gt_left\">no</td>\n<td class=\"gt_row gt_left\">yes</td>\n<td class=\"gt_row gt_left\">yes</td>\n<td class=\"gt_row gt_right\">538</td></tr>\n    <tr><td class=\"gt_row gt_left\">yes</td>\n<td class=\"gt_row gt_left\">no</td>\n<td class=\"gt_row gt_left\">yes</td>\n<td class=\"gt_row gt_right\">44</td></tr>\n    <tr><td class=\"gt_row gt_left\">no</td>\n<td class=\"gt_row gt_left\">no</td>\n<td class=\"gt_row gt_left\">yes</td>\n<td class=\"gt_row gt_right\">456</td></tr>\n    <tr><td class=\"gt_row gt_left\">yes</td>\n<td class=\"gt_row gt_left\">yes</td>\n<td class=\"gt_row gt_left\">no</td>\n<td class=\"gt_row gt_right\">3</td></tr>\n    <tr><td class=\"gt_row gt_left\">no</td>\n<td class=\"gt_row gt_left\">yes</td>\n<td class=\"gt_row gt_left\">no</td>\n<td class=\"gt_row gt_right\">43</td></tr>\n    <tr><td class=\"gt_row gt_left\">yes</td>\n<td class=\"gt_row gt_left\">no</td>\n<td class=\"gt_row gt_left\">no</td>\n<td class=\"gt_row gt_right\">2</td></tr>\n    <tr><td class=\"gt_row gt_left\">no</td>\n<td class=\"gt_row gt_left\">no</td>\n<td class=\"gt_row gt_left\">no</td>\n<td class=\"gt_row gt_right\">279</td></tr>\n  </tbody>\n  \n  <tfoot class=\"gt_footnotes\">\n    <tr>\n      <td class=\"gt_footnote\" colspan=\"4\"> Alcohol, Cigarette, and Marijuana Use for High School Seniors; Table 7.3 of Agresti, A (2007). An Introduction to Categorical Data Analysis.</td>\n    </tr>\n  </tfoot>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet's find the maximum-likelihood estimate $\\hat{\\beta}$ for the parameter $\\beta$ of our linear predictor $\\eta_i(\\beta)$.\nThankfully, the formulas look very similar to logistic regression, i.e.\n\n\n$$\n\\begin{align*}\n  l(\\beta) &= \\sum_{i = 1}^n \\big[y_i x_i^T\\beta - \\exp(x_i^T\\beta )\\big] \\\\\ns(\\beta) &= \\sum_{i = 1}^n x_i\\big(y_i - \\exp(x_i^T \\beta)\\big)\n\\end{align*}\n$$\n\n\nAgain, applying Newton's method to the score function $s$ will give us its root and therefore the ML-estimate of $\\beta$.\n\n## Generalized linear models\n\nWe've now seen two examples of GLMs.\nIn each case, we have assumed two different distributions and link functions.\nThis begs two questions ðŸ¤”\n\n1.  Does this work with any distribution?\n2.  How in the world do we choose the link function?\n\nThe secret ingredient that has been missing is a concept known as **exponential families**.\nIt can answer both questions.\nIsn't that just peachy?\n\nExponential families (not to be confused with the [exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution)) are distributions whose density can be rewritten in a **very** special form\n\n\n$$\nf(y | \\theta) = \\exp\\bigg\\{\n  \\frac{y\\theta - b(\\theta)}{\\phi}w + c(y, \\phi, w)\n\\bigg\\}, \n$$\n\n\nwhere\n\n-   $\\theta$ is the **natural/canonical parameter**\n-   $b(\\theta)$ is a twice differentiable function\n-   $\\phi$ is a dispersion parameter\n-   $w$ is a known weight\n-   $c$ is a normalization constant independent of $\\theta$\n\nHonestly, this curious form is anything but intuitive.\nYet, it is surprisingly versatile and the math just works.\nIf you ask me, that's quite mathemagical[^3].\n\n[^3]: An interesting perspective on what makes exponential familes so magical can be found on [SE](https://stats.stackexchange.com/questions/284260/why-do-we-assume-the-exponential-family-in-the-glm-context/284285#284285).\n\n| Distribution                 | $\\theta$                 | $b(\\theta)$          | $b^\\prime(\\theta)$              | $\\phi$     |\n|---------------|---------------|---------------|---------------|---------------|\n| $\\mathcal{N}(\\mu, \\sigma^2)$ | $\\mu$                    | $\\theta^2/2$         | $\\theta$                        | $\\sigma^2$ |\n| $\\text{Ber}(\\pi)$            | $\\log (\\pi / (1 - \\pi))$ | $\\log(1 + e^\\theta)$ | $\\frac{e^\\theta}{1 + e^\\theta}$ | 1          |\n| $\\text{Poi}(\\lambda)$        | $\\log(\\lambda)$          | $\\exp(\\theta)$       | $\\exp(\\theta)$                  | 1          |\n\nIt probably doesn't come as a surprise that Bernoulli and Poisson distributions are exponential families (see table or [Wikipedia](https://en.wikipedia.org/wiki/Exponential_family) for even more exponential distributions).\nBut what may surprise you is this:\n\nThe function $b$ plays an extraordinary role: Its derivative can be used as link function!\nThat's why we have chosen the link function the way we did.\nIn fact, that's the **canoncial choice**.\n\nSo, now you know GLMs' ingredients: Exponential families and link functions.\n\nBam!\nWe've made it through the math part.\nNow begins the programming part.\n\n## Implementing GLMs\n\nAs promised, we will implement GLMs in two different ways.\nFirst, we'll do it the `{stats}` way.\n\n### GLMs with `glm()`.\n\nWith `{stats}`, the `glm()` function is the main player to implement any GLM.\nAmong other arguments, this function accepts\n\n-   a `formula` argument: This is how we tell `glm()` what variable we want to predict based on which predictors.\n-   a `family` argument: This is the exponential family that we want to use (for logistic regression this will be Bernoulli or, more generally, binomial)\n-   a `data` argument: This is the data.frame/tibble that contains the variables that you've stated in the `formula`.\n\nFor our penguins example, this may look like this.\n\n\n::: {.cell fig.showtext='true'}\n\n```{.r .cell-code}\nglm.mod <- glm(\n  sex ~ body_mass_g + bill_length_mm + species, \n  family = binomial, \n  data = penguins_data\n)\n```\n:::\n\n\nHere, I've saved our fitted model into a variable `glm.mod`.\nFor our purposes, we can treat this variable like a list\nthat is equipped with a so-called class attribute (which influences the behavior of some functions later).\nOur list's column `fitted.values` contains the probabilities the GLM predicted, e.g.\n\n\n::: {.cell fig.showtext='true'}\n\n```{.r .cell-code}\nglm.mod$fitted.values[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         1          2          3          4          5          6          7 \n0.69330991 0.80620808 0.12555009 0.05499928 0.55937374 0.45142835 0.99939311 \n         8          9         10 \n0.14437378 0.69994208 0.92478757 \n```\n:::\n:::\n\n\nNow we can manually turn the predicted probabilities into `sex` predictions.\nThat requires us to know whether the predicted probability refers to male or female penguins.\n\nStrolling through the docs of `glm()` reveals that the second level in our response factor `sex` is considered a \"success\".\nSo that means the probability refers to male penguins (because that's the second level of `sex`).\nTricky, I know.\n\n\n::: {.cell fig.showtext='true'}\n\n```{.r .cell-code}\nthreshold <- 0.5\npreds <- penguins_data %>% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit > threshold, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\npreds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 333 Ã— 7\n   sex    body_mass_g species bill_length_mm prob.fit prediction correct  \n   <fct>        <int> <fct>            <dbl>    <dbl> <chr>      <chr>    \n 1 male          3750 Adelie            39.1   0.693  male       correct  \n 2 female        3800 Adelie            39.5   0.806  male       incorrect\n 3 female        3250 Adelie            40.3   0.126  female     correct  \n 4 female        3450 Adelie            36.7   0.0550 female     correct  \n 5 male          3650 Adelie            39.3   0.559  male       correct  \n 6 female        3625 Adelie            38.9   0.451  female     correct  \n 7 male          4675 Adelie            39.2   0.999  male       correct  \n 8 female        3200 Adelie            41.1   0.144  female     correct  \n 9 male          3800 Adelie            38.6   0.700  male       correct  \n10 male          4400 Adelie            34.6   0.925  male       correct  \n# â€¦ with 323 more rows\n# â„¹ Use `print(n = ...)` to see more rows\n```\n:::\n:::\n\n\nAnd we can also predict other probabilities for observations that have not been present in the original dataset.\n\n\n::: {.cell fig.showtext='true'}\n\n```{.r .cell-code}\nnew_observations <- tibble(\n  body_mass_g = c(4532, 5392),\n  bill_length_mm = c(40, 49),\n  species = c('Adelie', 'Gentoo')\n)\npredict(\n  glm.mod, \n  newdata = new_observations\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2 \n6.911824 3.661797 \n```\n:::\n:::\n\n\nNote that this gave us the value of the linear predictors.\nBut that's not we're interested, right?\nSo let's tell `predict()` that we care about the response variable.\n\n\n::: {.cell fig.showtext='true'}\n\n```{.r .cell-code}\npredict(\n  glm.mod, \n  newdata = new_observations,\n  type = 'response'\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1         2 \n0.9990051 0.9749570 \n```\n:::\n:::\n\n\nNow, these look more like probabilities.\nAnd they are.\nThese are our predicted probabilities.\nBut we can also save the probabilities into our tibble.\n\n\n::: {.cell fig.showtext='true'}\n\n```{.r .cell-code}\nnew_observations |> \n  mutate(\n    pred_prob = predict(\n      glm.mod,\n      newdata = new_observations,\n      type = 'response'\n    )\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 4\n  body_mass_g bill_length_mm species pred_prob\n        <dbl>          <dbl> <chr>       <dbl>\n1        4532             40 Adelie      0.999\n2        5392             49 Gentoo      0.975\n```\n:::\n:::\n\n\nOne last thing before we move on to `{tidymodels}`.\nAt first, it may be hard to work with `predict()` because the documentation does not feel super helpful at first.\nTake a look.\nThe docs do not tell you much about arguments like `newdata` or `type`.\n\n![](images/paste-1010B979.png){fig-align=\"center\" width=\"70%\"}\n\nThat's because `predict()` is actually a really versatile function.\nIt works differently depending on what kind of model object you feed it with, e.g. output from `lm()` or `glm()`.\nThat's the part that is determined by `glm.mod`'s class attribute.\n\nNow, at the end of `predict()`'s docs you will find a list that shows you all kinds of other `predict()` functions like `predict.glm()`. \nThis is the function that works with objects of class `glm`.\nIn the docs of the latter function you can now look up all arguments that you need.\n\n### GLMs with `{tidymodels}`\n\nLike the tidyverse, `{tidymodels}` is actually more than a single package.\nIt is a whole ecosystem of packages.\nAll of these packages share a design philosophy and are tailored to modelling/machine learning.\nCheck out how many packages get attached when we call `{tidymodels}`.\n\n\n::: {.cell fig.showtext='true'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.0.0 â”€â”€\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nâœ” broom        1.0.0     âœ” rsample      1.1.0\nâœ” dials        1.0.0     âœ” tune         1.0.0\nâœ” infer        1.0.2     âœ” workflows    1.0.0\nâœ” modeldata    1.0.0     âœ” workflowsets 1.0.0\nâœ” parsnip      1.0.0     âœ” yardstick    1.0.0\nâœ” recipes      1.0.1     \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\nâœ– scales::discard() masks purrr::discard()\nâœ– dplyr::filter()   masks stats::filter()\nâœ– recipes::fixed()  masks stringr::fixed()\nâœ– dplyr::lag()      masks stats::lag()\nâœ– yardstick::spec() masks readr::spec()\nâœ– recipes::step()   masks stats::step()\nâ€¢ Use suppressPackageStartupMessages() to eliminate package startup messages\n```\n:::\n:::\n\n\nObviously, we can't dig into everything here.\nSo, let's just cover the minimal amount we need for our use case.\nFor a more detailed intro to `{tidymodels}` you can check out my [YARDS lecture notes](https://yards.albert-rapp.de/how-to-build-a-model.html).\nAnd for a really thorough deep dive I recommend the [Tidy Modeling with R book](https://www.tmwr.org/).\n\nTo run a logistic regression in the `{tidymodels}` framework we need to first define a **model specification**.\nThese are handled by `{parsnip}`.\nHere, this looks like this.\n\n\n::: {.cell fig.showtext='true'}\n\n```{.r .cell-code}\nlogistic_spec <- logistic_reg() %>% \n  set_engine(\"glm\") %>% \n  set_mode(\"classification\")\nlogistic_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n\nNotice that the output is really nothing more than a description saying\n\n- we're doing a logistic regression (`logistic_reg()`).\n- we want to use the `stats::glm()` engine/function to fit the logistic regression (`set_engine(\"glm\")`)\n- we want to do classification (and not regression) (`set_mode(\"classification\")`)\n\nNotice that there are other engines that we could use, e.g. `\"glmnet\"` or `\"keras\"`.\nAlso, specifying the mode is kind of superfluous here.\nBut for models that can do both, classification and regression, e.g. [random forests](https://yards.albert-rapp.de/tree-based-models.html), this is necessary. \nThat's why I've shown that step here. \n\nAlright, so we've set up our model.\nTime to fit it.\nThat happens with `fit()`.\n\n\n::: {.cell fig.showtext='true'}\n\n```{.r .cell-code}\nglm.tidymod <- logistic_spec |> \n  fit(\n    formula = sex ~ body_mass_g + bill_length_mm + species, \n    data = penguins_data\n  )\n```\n:::\n\n\nLike `predict()`, the `fit()` function works a little bit differently depending on its input object.\nWhat's more is that can be a bit tricky to find the correct page in the docs to look up the correct arguments.\nWhat you're actually looking for is `fit.model_spec()`.\nThat's because `logistic_spec` is not a `glm` object but a `model_spec` object.\n\nDon't worry about the technicalities if you've found that confusing.\nThe point is that `fit()` expects a\n\n- model specification\n- a formula (exactly like `glm()`)\n- data\n\nNow we can throw `glm.tidymod` into `predict()`.\nBeware of your objects though!\nIn the `{stats}` way, we've passed a `glm` object to `predict` so really it behaves like `predict.glm()`.\nNow, we're using a `model_fit()` object (that's what `fit()` returns).\nSo, we'll need to look at the docs of `predict.model_fit()`.\n\nEt voilÃ !\nThe documentation reveals that there are arguments called `new_data` (mind the `_`) and `type`.\nThe latter argument can deal with a lot of different inputs.\nBut here we're just going with `\"prob\"`.\n\n\n::: {.cell fig.showtext='true'}\n\n```{.r .cell-code}\nnew_observations |> \n  mutate(\n    pred_prob = predict(\n      glm.tidymod,\n      new_data = new_observations,\n      type = 'prob'\n    )\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 4\n  body_mass_g bill_length_mm species pred_prob$.pred_female $.pred_male\n        <dbl>          <dbl> <chr>                    <dbl>       <dbl>\n1        4532             40 Adelie                0.000995       0.999\n2        5392             49 Gentoo                0.0250         0.975\n```\n:::\n:::\n\n\n\nNotice that this actually returns **two** probabilities.\nOne for each `sex`.\nThat's great because we don't have to wonder anymore what class our predicted probability refers to.\n\nThis is just one of many advantages working with `{tidymodels}`.\nYou will still need some knowledge of the engine you're using (`glm()` in this case). \nBut `{tidymodels}` will give you a nicer interface to the engine.\nAnd you can easily switch between engines and other models because it's all constructed from a unified interface.\n\nFinally, let me mention that switching to a Poisson regression is just a matter of exchanging `logistic_reg()` for `poisson_reg()` (and changing the mode to `\"regression\"`).\nIn fact, that's how you switch to any other model, e.g. to random forests via `rand_forest()`.\n\n## Conclusion\n\nThat's a wrap!\nThis guide should give you a solid understanding of both, the theoretical and practical, aspects of GLMs.\nIf you've got any questions, feel free to use the comment section or send me a mail.\n\nIf you don't want to stay in touch or don't want to miss new blog posts, then don't forget to subscribe to my newsletter below.\nEnjoy the rest of your day and see you next time!\n\n\n",
    "supporting": [
      "14_GLMS_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}