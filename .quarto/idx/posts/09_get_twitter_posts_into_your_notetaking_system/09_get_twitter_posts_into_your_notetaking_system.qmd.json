{"title":"How to collect dataviz from Twitter into your note-taking system","markdown":{"yaml":{"title":"How to collect dataviz from Twitter into your note-taking system","date":"2022-04-14","categories":["Automation","API"],"description":"The goal is to send a tweet's URL to a dummy mail account and let R extract it, call the Twitter API and then put everything as a Markdown file into your note-taking system.","execute":{"message":false,"warning":false,"collapse":false},"editor_options":{"chunk_output_type":"console"}},"headingText":"| echo: false","containsRefs":false,"markdown":"\n\n```{r}\n#| results: 'hide'\nrenv::use(lockfile = \"renv.lock\")\n```\n\n## Intro\n\nIt is mid-April and the [#30daychartchallenge](https://twitter.com/30DayChartChall) is well on its way.\nOne glace at the hashtag's Twitter feed suffices to realize that there are great contributions.\nThat's a perfect opportunity to collect data viz examples for future inspirations.\n\nIdeally, I can scroll through Twitter and with a few clicks incorporate these contributions straight into my [Obsidian](https://obsidian.md/) or any other Markdown-based note-taking system.\nUnfortunately, `rtweet`'s snapshot function does not seem to work anymore.\nSo, let's build something on our own that gets the job done.\nThe full script can be found on [GitHub gist](https://gist.github.com/AlbertRapp/37a2e0993acea9b4e36400037b797391).\nHere's what we will need:\n\n* Twitter app bearer token (to access Twitter's API) - I'll show you how to get that\n* Elevated API access (just a few clicks once you have a bearer token)\n* Dummy mail account to send tweets to\n\n\n## Overview\n\nBefore we begin, let me summarize what kind of note-taking process I have in mind: \n\n1. Stroll through Twitter and see great data viz on twitter.\n\n2. Send tweet link and a few comments via mail to a dummy mail account\n\n3. A scheduled process accesses the dummy mail account and scans for new mails from\nauthorized senders.\n\n4. If there is a new mail, R extracts tweet URL and uses Twitter's API to download the tweet's pictures and texts.\n\n5. A template Markdown file is used to create a new note that contains\nthe images and texts.\n\n6. Markdown file is copied to your note-taking system within your file system.\n\n7. Ideally, your Markdown template contains tags like #dataviz and #twitter so that\nyour new note can be easily searched for.\n\n8. Next time you look for inspiration, stroll through your collections or search for\ncomments.\n\n## Preparations\n\nOk, we know what we want to accomplish.\nTime to get the prelims done.\nFirst, we will need a Twitter developer account.\nThen, we have to mask sensitive information in our code.\nIf you already have a twitter app resp. a bearer token and know the `keyring` package, feel free to skip this section.\n\n### Get Twitter developer account \n\nLet's create a developer account for Twitter.\nUnfortunately, there is no way to get such an account without providing Twitter\nwith your phone number.\nSadly, if this burden on your privacy is a problem for you, then you cannot proceed.\nOtherwise, create an account at [developer.twitter.com](https://developer.twitter.com/en).\n\nIn your developer portal, create a project. \nWithin this project create an app.\nAlong the way, you will get a bunch of keys, secrets, IDs and tokens.\nYou will see them only once, so you will have to save them somewhere.\nI suggest saving them into a password manager like [bitwarden](https://bitwarden.com/).\n\nWhen you create your app or shortly after, you will need to set the authentication settings.\nI use `OAuth 2.0`.\nThis requires \n\n- type of app: `Automated bot or app`\n- Callback URI / Redirect URI: `http://127.0.0.1:1410` (DISCLAIMER: This is magic to me \nbut the `rtweet` docs - or possibly some other doc (not entirely sure anymore)- taught me to set up \nan app that way)\n- Website URL: Your Twitter link (in my case `https://twitter.com/rappa753`)\n\nNext, you will likely need to upgrade your project to 'elevated' status.\nThis can be done for free on your project's dashboard.\nFrom what I recall, you will have to fill out a form and tell Twitter what you want to do with your app.\nJust be honest and chances are that your request will immediately be granted.\nJust be yourself! \nWhat could possibly go wrong?\nGo get the ~~girl~~ elevated status (ahhh, what a perfect opportunity for a [Taylor song](https://www.youtube.com/watch?v=6FQ11gCO64o)).\n\n```{r}\n#| echo: false\n#| fig-cap: 'Click on detailed features to apply for higher access'\n\nknitr::include_graphics('project-elevated.PNG')\n```\n\n\n### How to embed your bearer token and other sensitive material in your code\n\nUse the `keyring` package to first save secrets via `key_set` and then extract them\nin your session via `key_get()`.\nThis way, you won't share your sensitive information by accident when you share your code (like I do).\nIn this post, I do this for my bearer token, my dummy mail, my dummy mail's password\nand for the allowed senders (that will be the mail where the tweets come from).\n\n```{r}\n#| echo: false\nkeyring::keyring_unlock('blogpost', 'blog-dummy-password')\n```\n\n\n```{r}\nbearer_token <- keyring::key_get('twitter-bearer-token', keyring = 'blogpost')\nuser_mail <- keyring::key_get('dataviz-mail', keyring = 'blogpost')\npassword_mail <- keyring::key_get('dataviz-mail-password', keyring = 'blogpost')\nallowed_senders <- keyring::key_get('allowed_senders', keyring = 'blogpost')\n```\n\nThe `allowed_senders` limitation is a precaution so that we do not accidentally download some malicious spam mail from God knows who onto our computer.\nI am no security expert but this feels like a prudent thing to do.\nIf one of you fellow readers knows more about this security business, feel kindly invited to reach out to me with better security strategies.\n\n\n## What to do once we have a URL\n\nLet's assume for the sake of this section that we already extracted a tweet URL\nfrom a mail.\nHere's the URL that we will use.\nIn fact, it's [Christian Gebhard](https://twitter.com/c_gebhard)'s tweet that inspired me to start this project.\nFrom the URL we can extract the tweet's ID (the bunch of numbers after `/status/`).\nAlso, we will need the URL of Twitter's API.\n\n```{r}\n#| message: false\n#| warning: false\nlibrary(stringr) # for regex matching\nlibrary(dplyr) # for binding rows and pipe\ntweet_url <- 'https://twitter.com/c_gebhard/status/1510533315262042112'\ntweet_id <- tweet_url %>% str_match(\"status/([0-9]+)\") %>% .[, 2]\nAPI_url <- 'https://api.twitter.com/2/tweets'\n```\n\n\n### Use GET() to access Twitter API\n\nNext, we use the `GET()` function from the `httr` package to interact with Twitter's API.\n\n```{r}\nlibrary(httr) # for API communication\n\nauth <- paste(\"Bearer\", bearer_token) # API needs format \"Bearer <my_token>\"\n\n# Make request to API\nrequest <- GET(\n  API_url, \n  add_headers(Authorization = auth), \n  query = list(\n    ids = tweet_id, \n    tweet.fields = 'created_at', # time stamp\n    expansions = 'attachments.media_keys,author_id', \n    # necessary expansion fields for img_url\n    media.fields = 'url' # img_url\n  )\n) \nrequest\n```\n\nSo, how do we know how to use the `GET()` function?\nWell, I am no expert on APIs but let me try to explain how I came up with the arguments I used here.\n\nRemember those toys you would play with as a toddler where you try\nto get a square through a square-shaped hole, a triangle through a triangle-shaped\nhole and so on?\nYou don't?\nWell, neither do I.\nWho remembers that stuff from very early childhood?\n\nBut I hear that starting a sentence with \"Remember those...\" is good for building\na rapport with your audience.\nSo, great!\nNow that we feel all cozy and connected, I can tell you how I managed to get the API\nrequest to work.\n\nAnd the truth is actually not that far from the toddler [\"intelligence test\"](https://www.youtube.com/watch?v=NNl7GQFTULU).\nFirst, I took a look at a [help page](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/media) from Twitter's developer page.\nThen, I hammered at the `GET()` function until its output contained a URL that looks \nsimilar to the example I found.\nHere's the example code I was aiming at.\n\n```\ncurl --request GET 'https://api.twitter.com/2/tweets?ids=1263145271946551300&\nexpansions=attachments.media_keys&\nmedia.fields=duration_ms,height,media_key,preview_image_url,public_metrics,type,url,width,alt_text' \n--header 'Authorization: Bearer $BEARER_TOKEN'\n```\n\nThis is not really R code but it looks like usually you have to feed a GET request\nwith a really long URL.\nIn fact, it looks like the URL needs to contain everything you want to extract from the API.\nSpecifically, the structure of said URL looks like\n\n- the API's base URL (in this case `r API_url`)\n- a question mark `?`\n- pairs of `keywords` (like `ids`) and a specific value, e.g. `ids=1263145271946551300`, \nthat are connected via `&`\n\nTherefore, it is only a matter of figuring out how to make the output of `GET()` \ndeliver this result.\nHints on that came from `GET()` examples in the docs.\n\n```{r}\nGET(\"http://google.com/\", path = \"search\", query = list(q = \"ham\"))\nGET(\"http://httpbin.org/get\", add_headers(a = 1, b = 2))\n```\n\nSo, the first example shows how an argument `query` can be filled with a list\nthat creates the URL we need.\nThe second examples shows us that there is something called `add_headers()`.\nDo I know exactly what that is?\nI mean, from a technical perspective of what is going on behind the scenes?\nDefinitely not.\nBut Twitter's example request had something called header.\nTherefore, `add_headers()` is \nprobably something that does what the Twitter API expects.\n\nAnd where do the key-value pairs come from? \nI found these strolling through Twitter's [data dictionary](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/introduction).\nThus, a `GET()` request was born and I could feel like a true [Hackerman](https://knowyourmeme.com/memes/hackerman).\n\n```{r}\n#| eval: false\nauth <- paste(\"Bearer\", bearer_token) # API needs format \"Bearer <my_token>\"\n\n# Make request to API and parse to list\nrequest <- GET(\n  API_url, \n  add_headers(Authorization = auth), \n  query = list(\n    ids = tweet_id, \n    tweet.fields = 'created_at', # time stamp\n    expansions = 'attachments.media_keys,author_id', \n    # necessary expansion fields for img_url\n    media.fields = 'url' # img_url\n  )\n) \n```\n\nAlright, we successfully requested data.\nNow, it becomes time to parse it to something useful.\nThe `content()` function will to that.\n\n```{r}\nparsed_request <- request %>% content('parsed')\nparsed_request\n```\n\n\n### Extract tweet data from what the API gives us and download images\n\nWe have seen that `parsed_request` is basically a large list that contains \neverything we requested from the API.\nUnfortunately, it is a highly nested list, so we have to do some work to extract\nthe parts we actually want.\n`pluck()` from the `purrr` package is our best friend on this one.\nHere's all the information we extract from the `parsed_request`.\n\n```{r}\nlibrary(purrr) # for pluck and map functions\n# Extract necessary information from list-like structure\ntweet_text <- parsed_request %>% \n  pluck(\"data\", 1, 'text') \ntweet_text\n\ntweet_user <-  parsed_request %>% \n  pluck(\"includes\", 'users', 1, 'username')\ntweet_user\n\n# We will use the tweet date and time as part of unique file names\n# Replace white spaces and colons (:) for proper file names\ntweet_date <- parsed_request %>% \n  pluck(\"data\", 1, 'created_at') %>% \n  lubridate::as_datetime() %>% \n  str_replace(' ', '_') %>% \n  str_replace_all(':', '')\ntweet_date\n\nimg_urls <- parsed_request %>% \n  pluck(\"includes\", 'media') %>% \n  bind_rows() %>% # bind_rows for multiple pictures, i.e. multiple URLS\n  filter(type == 'photo') %>% \n  pull(url)\nimg_urls\n```\n\nNext, download all the images via the `img_urls` and `download.file()`.\nWe will use `walk2()` to download all files (in case there are multiple images/URLs)\nand save the files into PNGs that are named using the unique `tweet_date` IDs.\nRemember to set `mode = 'wb'` in `download.file()`.\nI am not really sure why but without it you will save poor quality images.\n\n```{r}\n# Download image - set mode otherwise download is blurry\nimg_names <- paste('tweet', tweet_user, tweet_date, seq_along(img_urls), sep = \"_\")\nwalk2(img_urls, img_names, ~download.file(.x, paste0(.y, '.png'), mode = 'wb'))\n```\n\nSo let's do a quick recap of what we have done so far.\nWe \n\n* Assembled an API request\n* Parsed the return of the request\n* Cherrypicked the information that we want from the resulting list\n* Used the image URLs to download and save the files to our working directory.\n\nLet's cherish this mile stone with a dedicated function.\n\n```{r}\nrequest_twitter_data <- function(tweet_url, bearer_token) {\n  # Extract tweet id by regex\n  tweet_id <- tweet_url %>% str_match(\"status/([0-9]+)\") %>% .[, 2]\n  auth <- paste(\"Bearer\", bearer_token) # API needs format \"Bearer <my_token>\"\n  API_url <- 'https://api.twitter.com/2/tweets'\n  \n  # Make request to API and parse to list\n  parsed_request <- GET(\n    API_url, \n    add_headers(Authorization = auth), \n    query = list(\n      ids = tweet_id, \n      tweet.fields = 'created_at', # time stamp\n      expansions='attachments.media_keys,author_id', \n      # necessary expansion fields for img_url\n      media.fields = 'url' # img_url\n    )\n  ) %>% content('parsed')\n  \n  # Extract necassary information from list-like structure\n  tweet_text <- parsed_request %>% \n    pluck(\"data\", 1, 'text') \n  \n  tweet_user <-  parsed_request %>% \n    pluck(\"includes\", 'users', 1, 'username')\n  \n  # Make file name unique through time-date combination\n  # Replace white spaces and colons (:) for proper file names\n  tweet_date <- parsed_request %>% \n    pluck(\"data\", 1, 'created_at') %>% \n    lubridate::as_datetime() %>% \n    str_replace(' ', '_') %>% \n    str_replace_all(':', '')\n  \n  img_urls <- parsed_request %>% \n    pluck(\"includes\", 'media') %>% \n    bind_rows() %>% \n    filter(type == 'photo') %>% \n    pull(url)\n  \n  # Download image - set mode otherwise download is blurry\n  img_names <- paste('tweet', tweet_user, tweet_date, seq_along(img_urls), sep = \"_\")\n  walk2(img_urls, img_names, ~download.file(.x, paste0(.y, '.png'), mode = 'wb'))\n  \n  # Return list with information\n  list(\n    url = tweet_url,\n    text = tweet_text,\n    user = tweet_user,\n    file_names = paste0(img_names, '.png'),\n    file_paths = paste0(getwd(), '/', img_names, '.png')\n  )\n}\n```\n\n\n### Fill out Markdown template using extracted information and images\n\nWe have our images and the original tweet now.\nThanks to our previous function, we can save all of the information in a list.\n\n```{r}\nrequest <- request_twitter_data(tweet_url, bearer_token)\n```\n\nSo, let's bring all that information into a Markdown file.\nHere is the `template.md` file that I have created for this joyous occasion.\n\n```{r}\nlibrary(readr) # for reading and writing files from/to disk\ncat(read_file('template.md'))\n```\n\nAs you can see, I started the Markdown template with two tags `#dataviz` and `#twitter`.\nThis helps me to search for a specific dataviz faster.\nAlso, I have already written out the Markdown syntax for image imports `![[...]]`\nand added a placeholder `insert_img_name_here`.\nThis one will be replaced by the file path to the image.\nSimilarly, other placeholders like `insert_text_here` and `insert_mail_here` allow \nme to save the tweet and the mail content into my note taking system too.\n\nTo do so, I will need a function that replaces all the placeholders.\nFirst, I created a helper function that changes the image import placeholder\nproperly, when there are multiple images.\n\n```{r}\nmd_import_strings <- function(file_names) {\n  paste0('![[', file_names, ']]', collapse = '\\n') \n}\n```\n\nThen, I created a function that takes the `request` list that we got from calling \nour own `request_twitter_data()` function and iteratively uses `str_replace_all()`.\nThis iteration is done with `reduce2()` which will replace all placeholders in `template.md` .\n\n```{r}\n\nlibrary(tibble) # for easier readable tribble creation\n# Replace the placeholders in the template\n# We change original mail place holder later on\nreplace_template_placeholder <- function(template_name, request) {\n  # Create a dictionary for what to replace in template\n  replace_dict <- tribble(\n    ~template, ~replacement,\n    '\\\\!\\\\[\\\\[insert_img_name_here\\\\]\\\\]', md_import_strings(request$file_names),\n    'insert_text_here', request$text %>% str_replace_all('#', '(#)'),\n    'insert_URL_here', request$url\n  )\n  \n  # Iteratively apply str_replace_all and keep only final result\n  reduce2(\n    .x = replace_dict$template, \n    .y = replace_dict$replacement,\n    .f = str_replace_all,\n    .init =  read_lines(template_name) \n  ) %>% \n    paste0(collapse = '\\n') # Collaps lines into a single string\n}\n\nreplace_template_placeholder('template.md', request) %>% cat()\n```\n\nAs you can see, my `replace_template_placeholder()` function also replaces the \ntypical `#` from Twitter with `(#)`.\nThis is just a precaution to avoid wrong interpretation of these lines as headings\nin Markdown.\nAlso, the original mail has not been inserted yet because we have no mail yet.\nBut soooon.\nFinally, we need to write the replaced strings to a file.\nI got some helpers for that right here.\n\n```{r}\nwrite_replaced_text <- function(replaced_text, request) {\n  file_name <- request$file_name[1] %>% str_replace('_1.png', '.md')\n  write_lines(replaced_text, file_name)\n  paste0(getwd(), '/', file_name) \n}\nreplaced_template <- replace_template_placeholder('template.md', request) %>%\n  write_replaced_text(request)\n```\n\n\n### Shuffle files around on your file system\n\nAwesome!\nWe created new image files and a new Markdown note in our working directory.\nNow, we have to move them to our Obsidian vault. \nThis is the place where I collect all my Markdown notes for use in Obsidian.\nIn my case, I will need to move the Markdown note to the vault directory and\nthe images to a subdirectory within this vault.\nThis is because I changed settings in Obsidian that makes sure that all attachments,\ne.g. images, are saved in a separate subdirectory.\n\nHere's the function I created to get that job done.\nThe function uses the `request` list again because it contains the file paths of the images.\nHere, `vault_location` and `attachments_dir` are the file paths to my Obsidian vault.\n\n```{r}\nlibrary(tidyr) # for unnesting\nmove_files <- function(request, replaced_template, vault_location, attachments_dir) {\n  # Create from-to dictionary with file paths in each column\n  move_dict <- tribble(\n    ~from, ~to,\n    request$file_path, paste0(vault_location, '/', attachments_dir),\n    replaced_template, vault_location\n  ) %>% \n    unnest(cols = 'from')\n  # Copy files from current working directory to destination\n  move_dict %>% pwalk(file.copy, overwrite = T)\n  # Delete files in current working directory\n  walk(move_dict$from, file.remove)\n}\n```\n\n\n## How to extract URL and other stuff from mail\n\nLet's take a quick breather and recap.\nWe have written functions that \n\n- take a tweet URL\n- hussle the Twitter API to give us all its data\n- download the images and tweet text\n- save everything to a new Markdown note based on a template\n- can move the note plus images to the location of our note-taking hub\n\nNot to brag but that is kind of cool.\nBut let's not rest here.\nWe still have to get some work done.\nAfter all, we want our workflow to be email-based.\nSo, let's access our mails using R.\nThen, we can extract a Twitter URL and apply our previous functions.\nAlso, this lets us finally replace the `insert_mail_here` placeholder in our Markdown note.\n\n### Postman gives you access\n\nI have created a dummy mail account at gmail.\nUsing the `mRpostman` package, we can establish a connection to our mail inbox.\nAfter the connection is established, we can filter for all new emails that are sent\nfrom our list of `allowed_senders`.\n\n```{r}\n#| message: false\n#| eval: false\nlibrary(mRpostman) # for email communication\nimap_mail <- 'imaps://imap.gmail.com' # mail client\n# Establish connection to imap server\ncon <- configure_imap(\n  url = imap_mail,\n  user = user_mail,\n  password = password_mail\n)\n\n# Switch to Inbox\ncon$select_folder('Inbox') \n\n# Extract mails that are from the list of allowed senders\nmails <- allowed_senders %>% \n  map(~con$search_string(expr = ., where = 'FROM')) %>% \n  unlist() %>% \n  na.omit() %>% # Remove NAs if no mail from a sender\n  as.numeric() # avoids attributes\n```\n\n\n### Grab URLs from mail\n\nIf `mails` is not empty, i.e. if there are new mails, then we need to extract \nthe tweet URLs from them.\nUnfortunately, depending on where you sent your email from, the mail text can be \nencoded.\n\nFor example, I send most of the tweets via the share button on Twitter using my\nAndroid smartphone.\nAnd for some reason, my Android mail client encodes the mails in something called `base64`.\nBut sending a tweet URL from Thunderbird on my computer works without any encoding.\nHere are two example mails I have sent to my dummy mail account.\n\n```{r}\n#| eval: false\nif (!is_empty(mails)) mail_bodys <- mails %>% con$fetch_text()\ncat(mail_bodys[[1]])\ncat(mail_bodys[[2]])\n```\n\nAs you can see, the mail sent from my computer is legible but the other one is gibberish.\nThankfully, Allan Cameron helped me out on [Stackoverflow](https://stackoverflow.com/questions/71772972/translate-encoding-of-android-mail-in-r) to decode the\nmail.\nTo decode the mail, the trick was to extract the parts between `base64` and\n`----`.\n\nThere are two such texts in the encoded mail. \nSurprisingly, the first one decoded to a text without line breaks.\nThis is why we take the second encoded part and decode it.\nHowever, this will give us an HTML text with all kinds of tags like `<div>` and what not.\nTherefore, we use `html_read()` and `html_text2()` from the `rvest` package to handle that.\nAll of this is summarized in this helper function.\n\n```{r}\n#| eval: false\ndecode_encoded_mails <- function(encoded_mails) {\n  # Ressource: https://stackoverflow.com/questions/71772972/translate-encoding-of-android-mail-in-r\n  # Find location in each encoded string where actual text starts\n  start_encoded <- encoded_mails %>% \n    str_locate_all('base64\\r\\n\\r\\n') %>% \n    map(~pluck(., 4) + 1) %>% \n    unlist()\n  \n  # Find location in each encoded string where actual text starts\n  end_encoded <- encoded_mails %>% \n    str_locate_all('----') %>% \n    map(~pluck(., 3) - 1)%>% \n    unlist()\n  \n  # Use str_sub() to extract encoded text\n  encoded_text <- tibble(\n    string = unlist(encoded_mails), \n    start = start_encoded, \n    end = end_encoded\n  ) %>% \n    pmap(str_sub) \n  \n  # Decode: base64 -> raw -> char -> html -> text\n  encoded_text %>% \n    map(base64enc::base64decode) %>% \n    map(rawToChar) %>% \n    map(rvest::read_html) %>% \n    map(rvest::html_text2)\n}\n```\n\nI feel like this is the most hacky part of this blog post.\nUnfortunately, your milage may vary here.\nIf your phone or whatever you use encodes the mails differently,\nthen you may have to adjust the function.\nBut I hope that I have explained enough details and concepts for you to manage that\nif it comes to this.\n\nRecall that I send both plain mails from Thunderbird and encoded mails from Android.\nTherefore, here is another helper that decoded mails if neccessary from both types in one swoop.\n\n```{r}\n#| eval: false\ndecode_all_mails <- function(mail_bodys) {\n  # Decode in case mail is base64 decoded\n  is_encoded <- str_detect(mail_bodys, 'Content-Transfer-Encoding')\n  encoded_mails <- mail_bodys[is_encoded]\n  plain_mails <- mail_bodys[!is_encoded]\n  decoded_mails <- encoded_mails %>% decode_encoded_mails()\n  c(decoded_mails, plain_mails)\n}\n```\n\nThe remaining part of the code should be familiar:\n\n- Use `decode_all_mails()` for decoding\n- Grab URLs with `str_extract()`\n- Use `request_twitter_data()` with our URLs\n- Replace placeholders with `replace_template_placeholder()`\n- This time, replace mail placeholders too with another `str_replace()` iteration\n- Move files with `move_files()`\n\nThe only new thing is that we use our postman connection to move the processed mails into a new directory (which I called \"Processed\") on the email server.\nThis way, the inbox is empty again or filled only with mails from unauthorized \nsenders.\n\n```{r}\n#| eval: false\nif (!is_empty(mails)) {\n  # Grab mail texts and URLs\n  mail_bodys <- mails %>% con$fetch_text() %>% decode_all_mails\n  urls <- mail_bodys %>% str_extract('https.*')\n  \n  # Remove mails from vector in case s.th. goes wrong \n  # and urls cannot be detected\n  mail_bodys <- mail_bodys[!is.na(urls)]\n  mails <- mails[!is.na(urls)]\n  urls <- urls[!is.na(urls)]\n  \n  # For each url request twitter data\n  requests <- map(urls, request_twitter_data, bearer_token = bearer_token)\n  \n  # Use requested twitter data to insert texts into Markdown template \n  # and write to current working directory\n  replaced_templates_wo_mails <- \n    map(requests, replace_template_placeholder, template = 'template.md') \n  \n  # Now that we have mails, replace that placeholder too\n  replaced_templates <- replaced_templates_wo_mails %>% \n    map2(mail_bodys, ~str_replace(.x, 'insert_mail_here' ,.y)) %>% \n    map2(requests, ~write_replaced_text(.x, .y))\n  \n  # Move markdown files and extracted pngs to correct place on HDD\n  walk2(\n    requests, \n    replaced_templates, \n    move_files, \n    vault_location = vault_location, \n    attachments_dir = attachments_dir\n  )\n  \n  # Move emails on imap server to Processed directory\n  con$move_msg(mails, to_folder = 'Processed')\n}\n```\n\n## Last Step: Execute R script automatically\n\nAlright, alright, alright.\nWe made it.\nWe have successfully \n\n- extracted URLs from mails,\n- created new notes and\n- moved them to their designated place\n\nThe only thing that is left to do is execute this script automatically.\nAgain, if you don't want to assemble the R script yourself using the code\nchunks in this blog post, check out this [GitHub gist](https://gist.github.com/AlbertRapp/37a2e0993acea9b4e36400037b797391).\n\nOn Windows, you can write a VBS script that will execute the R script.\nWindow's task scheduler is [easily set up](https://www.windowscentral.com/how-create-automated-task-using-task-scheduler-windows-10) to run that VBS script regularly, say every hour.\nFor completeness' sake let me give you an example VBS script.\nBut beware that I have no frikkin clue how VBS scripts work beyond this simple call.\n\n```\nSet wshshell = WScript.CreateObject (\"wscript.shell\")\nwshshell.run \"\"\"C:\\Program Files\\R\\R-4.0.5\\bin\\Rscript.exe\"\" \"\"D:\\Local R Projects\\Playground\\TwitterTracking\\my_twitter_script.R\"\"\", 6, True\nset wshshell = nothing\n```\n\nThe idea of this script is to call `Rscript.exe` and give it the location of the\nR script that we want to execute.\nOf course, you will need to adjust the paths to your file system.\nNotice that there are super many double quotes in this script. \nThis is somewhat dumb but it's the only way I could find to make file paths with white spaces work (see [StackOverflow](https://stackoverflow.com/questions/14360599/vbs-with-space-in-file-path)).\n\nOn Ubuntu (and probably other Unix-based systems), I am sure that every Unix user knows that there is [CronTab](https://stackoverflow.com/questions/38778732/schedule-a-rscript-crontab-everyminute) to schedule regular tasks.\nOn Mac, I am sure there is something.\nBut instead of wandering even further from my expertise, I will refer to\nyour internet search skills.\n\n## Mind the possibilities\n\nWe made it!\nWe connected to Twitter's API and our dummy email to get data viz (what's the plural here? viz, vizz, vizzes, vizzeses?) into our note-taking system.\nHonestly, I think that was quite an endeavor.\nBut now we can use the same ideas for all kind of other applications!\nFrom the top of my head I can think of more scenarios where similar solutions should be manageable.\nHere are two ideas.\n\n- Take notes on the fly using emails and automatically incorporate the emails into your note-taking system.\n\n- Take a photo from a book/text you're reading and send it to another dummy mail. \nRun a script that puts the photo and the mail directly into your vault.\n\nSo, enjoy the possibilities!\nIf you liked this blog post, then consider following me on [Twitter](https://twitter.com/rappa753) and/or subscribing to my [RSS feed](https://albert-rapp.de/blog.xml).\nUntil next time!\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"message":false,"collapse":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["code-filename"],"toc":true,"toc-depth":3,"include-after-body":["../../footer.html"],"output-file":"09_get_twitter_posts_into_your_notetaking_system.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.35","editor":"source","theme":{"light":"../../theme.scss"},"title-block-banner":false,"author":"Albert Rapp","page-layout":"article","comments":{"utterances":{"repo":"AlbertRapp/blogComments"}},"title":"How to collect dataviz from Twitter into your note-taking system","date":"2022-04-14","categories":["Automation","API"],"description":"The goal is to send a tweet's URL to a dummy mail account and let R extract it, call the Twitter API and then put everything as a Markdown file into your note-taking system.","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}}}}